{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jP5uwXtZay_",
        "outputId": "cb39cd39-ecc1-41ab-9f44-b3de6923857c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRPF7T0b-QZ",
        "outputId": "091db2b9-e211-416f-e58e-b7a5e0c83318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spafe\n",
            "  Downloading spafe-0.3.2-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.11.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from spafe) (4.5.0)\n",
            "Installing collected packages: spafe\n",
            "Successfully installed spafe-0.3.2\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "!pip install -U spafe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s2K5aSHYb-Sm"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-xNIFhPtb-U5"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QgbvNKYAb-YM"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet50,ResNet101\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.model_selection import StratifiedKFold , KFold ,RepeatedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LXsQV7ivcIcc"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32) # scaler value\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  return tf.matmul(attention_weights, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WB2_09_e-qt"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# This allows to the transformer to know where there is real data and where it is padded\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # check if a values of the sequence are 0 or not\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PjEQIR8TcIe3"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8hF4xYdLcSQA"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j7ewC3iBcSSo"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  # outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) # extra layer added\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_8IFloblcSWJ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder(time_steps,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            projection,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  if projection=='linear':\n",
        "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
        "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
        "    print('linear')\n",
        "\n",
        "  else:\n",
        "    projection=tf.identity(inputs)\n",
        "    print('none')\n",
        "\n",
        "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NXgJh2PpcIiK"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def transformer(time_steps,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                output_size,\n",
        "                projection,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(tf.dtypes.cast(\n",
        "\n",
        "      #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
        "      # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked\n",
        "      tf.math.reduce_sum(\n",
        "      inputs,\n",
        "      axis=2,\n",
        "      keepdims=False,\n",
        "      name=None\n",
        "  ), tf.int32))\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      time_steps=time_steps,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      projection=projection,\n",
        "      name='encoder'\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  #We reshape for feeding our FC in the next step\n",
        "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
        "\n",
        "  #We predict our class\n",
        "  outputs = tf.keras.layers.Dense(units=d_model/2,use_bias=True, name=\"outputs_1\")(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model/4,use_bias=True, name=\"outputs_2\")(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True, name=\"outputs_3\")(outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u4Iw3KzecbRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40bc61e-ca84-4d42-eba5-d41e1a34e944"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2075, 181)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#ajit\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate5_spectral2.csv')\n",
        "# dm\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/audio-interspeech/2075_concatenate_dm_4featu.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm_4featu.csv')\n",
        "\n",
        "\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/melspectogram_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/chroma_cqt_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/mfcc_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/spectral_centroid_meandm_2075_200.csv')\n",
        "\n",
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2HrbtbLXcbTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030e346d-171f-4bfa-9d31-acc73863658e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "siz=415\n",
        "df_read = df.copy()\n",
        "df1 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df1.index)\n",
        "df2 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df2.index)\n",
        "df3 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df3.index)\n",
        "df4 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df4.index)\n",
        "df5 = df_read.copy()\n",
        "\n",
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)\n",
        "print(df4.shape)\n",
        "print(df5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_j19bgqpcbWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4ff2b3-737c-4e8e-ac03-7e08c75ef90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int64Index([ 881,  453, 2004, 1353,  281,  941, 1185, 1159, 1138,  599,\n",
            "            ...\n",
            "             834, 1730,  353, 1345, 1190, 1375,  185,  701, 1671, 1982],\n",
            "           dtype='int64', length=415)\n"
          ]
        }
      ],
      "source": [
        "q = list(df1.index)+list(df2.index)+list(df3.index)+list(df4.index)+list(df5.index)\n",
        "print(df1.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uT0Jrfa5cbZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72294bbc-0759-4299-adcd-ccb62ec3d645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear\n",
            "Epoch 1/1000\n",
            "52/52 [==============================] - 35s 55ms/step - loss: 4.4306 - val_loss: 2.6761\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.2188 - val_loss: 1.9972\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.0757 - val_loss: 2.0609\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.8691 - val_loss: 1.8883\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.7969 - val_loss: 1.7275\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.5952 - val_loss: 1.7346\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.5719 - val_loss: 1.5910\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.3640 - val_loss: 1.5498\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 2.3262 - val_loss: 1.4360\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 2.2754 - val_loss: 1.5049\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 2.1879 - val_loss: 1.4412\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.1242 - val_loss: 1.4797\n",
            "Epoch 13/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 2.1943 - val_loss: 1.5167\n",
            "Epoch 14/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.1304 - val_loss: 1.5585\n",
            "Epoch 15/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.1367 - val_loss: 1.5440\n",
            "Epoch 16/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.0124 - val_loss: 1.4816\n",
            "Epoch 17/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.0891 - val_loss: 1.3854\n",
            "Epoch 18/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.0093 - val_loss: 1.3949\n",
            "Epoch 19/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.9461 - val_loss: 1.4221\n",
            "Epoch 20/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.9478 - val_loss: 1.4132\n",
            "Epoch 21/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.8944 - val_loss: 1.3832\n",
            "Epoch 22/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.8765 - val_loss: 1.4120\n",
            "Epoch 23/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.9429 - val_loss: 1.3585\n",
            "Epoch 24/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.8989 - val_loss: 1.3573\n",
            "Epoch 25/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.7331 - val_loss: 1.4170\n",
            "Epoch 26/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.7918 - val_loss: 1.3664\n",
            "Epoch 27/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.7889 - val_loss: 1.4471\n",
            "Epoch 28/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.7880 - val_loss: 1.3614\n",
            "Epoch 29/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.7583 - val_loss: 1.3123\n",
            "Epoch 30/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.7155 - val_loss: 1.3918\n",
            "Epoch 31/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.8438 - val_loss: 1.3680\n",
            "Epoch 32/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.7039 - val_loss: 1.3794\n",
            "Epoch 33/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.7592 - val_loss: 1.3594\n",
            "Epoch 34/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.7963 - val_loss: 1.3993\n",
            "Epoch 35/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.6804 - val_loss: 1.3958\n",
            "Epoch 36/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.6433 - val_loss: 1.3477\n",
            "Epoch 37/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5831 - val_loss: 1.4055\n",
            "Epoch 38/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.6503 - val_loss: 1.3730\n",
            "Epoch 39/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.6829 - val_loss: 1.3237\n",
            "Epoch 40/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.6755 - val_loss: 1.3307\n",
            "Epoch 41/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.6057 - val_loss: 1.4194\n",
            "Epoch 42/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.6817 - val_loss: 1.3540\n",
            "Epoch 43/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.7104 - val_loss: 1.3991\n",
            "Epoch 44/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.6614 - val_loss: 1.3549\n",
            "Epoch 45/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.5804 - val_loss: 1.2576\n",
            "Epoch 46/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.6549 - val_loss: 1.2331\n",
            "Epoch 47/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5579 - val_loss: 1.3768\n",
            "Epoch 48/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.6125 - val_loss: 1.2760\n",
            "Epoch 49/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.5335 - val_loss: 1.3640\n",
            "Epoch 50/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5835 - val_loss: 1.2876\n",
            "Epoch 51/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.5824 - val_loss: 1.4222\n",
            "Epoch 52/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.5332 - val_loss: 1.2949\n",
            "Epoch 53/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.5274 - val_loss: 1.3250\n",
            "Epoch 54/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.5892 - val_loss: 1.2911\n",
            "Epoch 55/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5564 - val_loss: 1.2844\n",
            "Epoch 56/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.5430 - val_loss: 1.2950\n",
            "Epoch 57/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5851 - val_loss: 1.3083\n",
            "Epoch 58/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 1.5542 - val_loss: 1.3601\n",
            "Epoch 59/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 1.4832 - val_loss: 1.3203\n",
            "Epoch 60/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.5019 - val_loss: 1.4211\n",
            "Epoch 61/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.5593 - val_loss: 1.4372\n",
            "Epoch 62/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5307 - val_loss: 1.3714\n",
            "Epoch 63/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5641 - val_loss: 1.2980\n",
            "Epoch 64/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5327 - val_loss: 1.3245\n",
            "Epoch 65/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5209 - val_loss: 1.2776\n",
            "Epoch 66/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4949 - val_loss: 1.2910\n",
            "Epoch 67/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.4917 - val_loss: 1.3190\n",
            "Epoch 68/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.3709 - val_loss: 1.2260\n",
            "Epoch 69/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.5529 - val_loss: 1.2751\n",
            "Epoch 70/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 1.4630 - val_loss: 1.2726\n",
            "Epoch 71/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.4471 - val_loss: 1.3096\n",
            "Epoch 72/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.4678 - val_loss: 1.3536\n",
            "Epoch 73/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4627 - val_loss: 1.1734\n",
            "Epoch 74/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4270 - val_loss: 1.2402\n",
            "Epoch 75/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4254 - val_loss: 1.2567\n",
            "Epoch 76/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4002 - val_loss: 1.2715\n",
            "Epoch 77/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.4480 - val_loss: 1.2907\n",
            "Epoch 78/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4324 - val_loss: 1.3192\n",
            "Epoch 79/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.3015 - val_loss: 1.3185\n",
            "Epoch 80/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4214 - val_loss: 1.2458\n",
            "Epoch 81/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.4363 - val_loss: 1.2841\n",
            "Epoch 82/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.4509 - val_loss: 1.2945\n",
            "Epoch 83/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3865 - val_loss: 1.3559\n",
            "Epoch 84/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4235 - val_loss: 1.2408\n",
            "Epoch 85/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4225 - val_loss: 1.2545\n",
            "Epoch 86/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4354 - val_loss: 1.2336\n",
            "Epoch 87/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4253 - val_loss: 1.3556\n",
            "Epoch 88/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3775 - val_loss: 1.2791\n",
            "Epoch 89/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.4115 - val_loss: 1.2730\n",
            "Epoch 90/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3902 - val_loss: 1.2454\n",
            "Epoch 91/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.3647 - val_loss: 1.2054\n",
            "Epoch 92/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.3909 - val_loss: 1.2236\n",
            "Epoch 93/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.4250 - val_loss: 1.2680\n",
            "Epoch 94/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3608 - val_loss: 1.1437\n",
            "Epoch 95/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.3277 - val_loss: 1.2471\n",
            "Epoch 96/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3368 - val_loss: 1.3023\n",
            "Epoch 97/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3880 - val_loss: 1.2055\n",
            "Epoch 98/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3267 - val_loss: 1.2399\n",
            "Epoch 99/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3997 - val_loss: 1.2304\n",
            "Epoch 100/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3635 - val_loss: 1.1983\n",
            "Epoch 101/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3903 - val_loss: 1.2242\n",
            "Epoch 102/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.3562 - val_loss: 1.2545\n",
            "Epoch 103/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.3056 - val_loss: 1.2066\n",
            "Epoch 104/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3658 - val_loss: 1.2151\n",
            "Epoch 105/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.3006 - val_loss: 1.2188\n",
            "Epoch 106/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2755 - val_loss: 1.1736\n",
            "Epoch 107/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2972 - val_loss: 1.2353\n",
            "Epoch 108/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.3138 - val_loss: 1.2163\n",
            "Epoch 109/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2932 - val_loss: 1.1619\n",
            "Epoch 110/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3740 - val_loss: 1.1427\n",
            "Epoch 111/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2872 - val_loss: 1.2849\n",
            "Epoch 112/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3288 - val_loss: 1.2895\n",
            "Epoch 113/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.3024 - val_loss: 1.1628\n",
            "Epoch 114/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.2789 - val_loss: 1.2321\n",
            "Epoch 115/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3506 - val_loss: 1.2393\n",
            "Epoch 116/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2661 - val_loss: 1.2483\n",
            "Epoch 117/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2511 - val_loss: 1.2093\n",
            "Epoch 118/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2762 - val_loss: 1.2291\n",
            "Epoch 119/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3219 - val_loss: 1.1593\n",
            "Epoch 120/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2867 - val_loss: 1.1323\n",
            "Epoch 121/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.3060 - val_loss: 1.1592\n",
            "Epoch 122/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2684 - val_loss: 1.1487\n",
            "Epoch 123/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.2873 - val_loss: 1.2198\n",
            "Epoch 124/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.2543 - val_loss: 1.2285\n",
            "Epoch 125/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3225 - val_loss: 1.3235\n",
            "Epoch 126/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3010 - val_loss: 1.1142\n",
            "Epoch 127/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2413 - val_loss: 1.1959\n",
            "Epoch 128/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2388 - val_loss: 1.1879\n",
            "Epoch 129/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3100 - val_loss: 1.1822\n",
            "Epoch 130/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2564 - val_loss: 1.2281\n",
            "Epoch 131/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2293 - val_loss: 1.1935\n",
            "Epoch 132/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2554 - val_loss: 1.2784\n",
            "Epoch 133/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2938 - val_loss: 1.2198\n",
            "Epoch 134/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.2123 - val_loss: 1.1834\n",
            "Epoch 135/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.2257 - val_loss: 1.1986\n",
            "Epoch 136/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2475 - val_loss: 1.1498\n",
            "Epoch 137/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2040 - val_loss: 1.2161\n",
            "Epoch 138/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2589 - val_loss: 1.0638\n",
            "Epoch 139/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2086 - val_loss: 1.2156\n",
            "Epoch 140/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2178 - val_loss: 1.1675\n",
            "Epoch 141/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2544 - val_loss: 1.0735\n",
            "Epoch 142/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2218 - val_loss: 1.1166\n",
            "Epoch 143/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2605 - val_loss: 1.1060\n",
            "Epoch 144/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.2204 - val_loss: 1.2251\n",
            "Epoch 145/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.2157 - val_loss: 1.1753\n",
            "Epoch 146/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2254 - val_loss: 1.2708\n",
            "Epoch 147/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2190 - val_loss: 1.1224\n",
            "Epoch 148/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2416 - val_loss: 1.0730\n",
            "Epoch 149/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1481 - val_loss: 1.1352\n",
            "Epoch 150/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2308 - val_loss: 1.0596\n",
            "Epoch 151/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1670 - val_loss: 1.1305\n",
            "Epoch 152/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1969 - val_loss: 1.1907\n",
            "Epoch 153/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2475 - val_loss: 1.1291\n",
            "Epoch 154/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.1590 - val_loss: 1.1334\n",
            "Epoch 155/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.1960 - val_loss: 1.1080\n",
            "Epoch 156/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 1.1614 - val_loss: 1.1016\n",
            "Epoch 157/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1969 - val_loss: 1.0696\n",
            "Epoch 158/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1520 - val_loss: 1.1139\n",
            "Epoch 159/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2096 - val_loss: 1.0793\n",
            "Epoch 160/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1777 - val_loss: 1.1109\n",
            "Epoch 161/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1837 - val_loss: 1.0405\n",
            "Epoch 162/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.1719 - val_loss: 1.0613\n",
            "Epoch 163/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.2083 - val_loss: 1.0487\n",
            "Epoch 164/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.1567 - val_loss: 1.1542\n",
            "Epoch 165/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.1589 - val_loss: 1.0921\n",
            "Epoch 166/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.1821 - val_loss: 1.0659\n",
            "Epoch 167/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1731 - val_loss: 1.1129\n",
            "Epoch 168/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1227 - val_loss: 1.0391\n",
            "Epoch 169/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1552 - val_loss: 1.0844\n",
            "Epoch 170/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1410 - val_loss: 1.0876\n",
            "Epoch 171/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1173 - val_loss: 1.0801\n",
            "Epoch 172/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 1.1302 - val_loss: 1.1457\n",
            "Epoch 173/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1808 - val_loss: 1.0914\n",
            "Epoch 174/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1096 - val_loss: 1.0743\n",
            "Epoch 175/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1450 - val_loss: 1.0634\n",
            "Epoch 176/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1430 - val_loss: 1.0951\n",
            "Epoch 177/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.1422 - val_loss: 1.1223\n",
            "Epoch 178/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1466 - val_loss: 1.0926\n",
            "Epoch 179/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1287 - val_loss: 1.0536\n",
            "Epoch 180/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1376 - val_loss: 1.0847\n",
            "Epoch 181/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1357 - val_loss: 1.0266\n",
            "Epoch 182/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1639 - val_loss: 1.0540\n",
            "Epoch 183/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0999 - val_loss: 1.0025\n",
            "Epoch 184/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0834 - val_loss: 1.0213\n",
            "Epoch 185/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.1662 - val_loss: 1.0941\n",
            "Epoch 186/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1197 - val_loss: 1.0406\n",
            "Epoch 187/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.0483 - val_loss: 1.0351\n",
            "Epoch 188/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1103 - val_loss: 1.0716\n",
            "Epoch 189/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0748 - val_loss: 1.1081\n",
            "Epoch 190/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0714 - val_loss: 1.0506\n",
            "Epoch 191/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0608 - val_loss: 0.9754\n",
            "Epoch 192/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1144 - val_loss: 1.0544\n",
            "Epoch 193/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0891 - val_loss: 1.0612\n",
            "Epoch 194/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0888 - val_loss: 0.9890\n",
            "Epoch 195/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1341 - val_loss: 1.0328\n",
            "Epoch 196/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.1157 - val_loss: 1.1048\n",
            "Epoch 197/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.0565 - val_loss: 0.9953\n",
            "Epoch 198/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.0702 - val_loss: 0.9723\n",
            "Epoch 199/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1156 - val_loss: 1.0953\n",
            "Epoch 200/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0854 - val_loss: 0.9694\n",
            "Epoch 201/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1003 - val_loss: 1.0502\n",
            "Epoch 202/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0475 - val_loss: 1.0572\n",
            "Epoch 203/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0641 - val_loss: 0.9758\n",
            "Epoch 204/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1021 - val_loss: 0.9430\n",
            "Epoch 205/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0684 - val_loss: 0.9880\n",
            "Epoch 206/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0594 - val_loss: 0.9916\n",
            "Epoch 207/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0739 - val_loss: 1.0199\n",
            "Epoch 208/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.1013 - val_loss: 0.9687\n",
            "Epoch 209/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0493 - val_loss: 0.9786\n",
            "Epoch 210/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0641 - val_loss: 1.0408\n",
            "Epoch 211/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0344 - val_loss: 0.9916\n",
            "Epoch 212/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0648 - val_loss: 1.0275\n",
            "Epoch 213/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0607 - val_loss: 1.0263\n",
            "Epoch 214/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0869 - val_loss: 0.9030\n",
            "Epoch 215/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0895 - val_loss: 1.0029\n",
            "Epoch 216/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0428 - val_loss: 1.1058\n",
            "Epoch 217/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0618 - val_loss: 1.0537\n",
            "Epoch 218/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.0648 - val_loss: 0.9623\n",
            "Epoch 219/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0663 - val_loss: 0.9224\n",
            "Epoch 220/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0341 - val_loss: 0.9238\n",
            "Epoch 221/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0224 - val_loss: 0.9839\n",
            "Epoch 222/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0620 - val_loss: 0.9460\n",
            "Epoch 223/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0683 - val_loss: 1.0112\n",
            "Epoch 224/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0229 - val_loss: 0.9398\n",
            "Epoch 225/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0285 - val_loss: 0.9885\n",
            "Epoch 226/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0526 - val_loss: 0.8930\n",
            "Epoch 227/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.0914 - val_loss: 0.8741\n",
            "Epoch 228/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0361 - val_loss: 0.9813\n",
            "Epoch 229/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.0426 - val_loss: 0.9565\n",
            "Epoch 230/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0604 - val_loss: 0.9505\n",
            "Epoch 231/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0917 - val_loss: 0.9850\n",
            "Epoch 232/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0418 - val_loss: 0.9569\n",
            "Epoch 233/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0830 - val_loss: 0.9652\n",
            "Epoch 234/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0219 - val_loss: 0.9251\n",
            "Epoch 235/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0441 - val_loss: 0.8854\n",
            "Epoch 236/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0462 - val_loss: 0.9208\n",
            "Epoch 237/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0014 - val_loss: 0.9212\n",
            "Epoch 238/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0645 - val_loss: 0.8736\n",
            "Epoch 239/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0229 - val_loss: 0.9328\n",
            "Epoch 240/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0318 - val_loss: 0.9119\n",
            "Epoch 241/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0414 - val_loss: 0.9350\n",
            "Epoch 242/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0216 - val_loss: 0.9963\n",
            "Epoch 243/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0541 - val_loss: 0.9002\n",
            "Epoch 244/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0122 - val_loss: 0.8817\n",
            "Epoch 245/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0227 - val_loss: 0.8958\n",
            "Epoch 246/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0333 - val_loss: 0.9331\n",
            "Epoch 247/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0236 - val_loss: 0.8734\n",
            "Epoch 248/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.0341 - val_loss: 0.9130\n",
            "Epoch 249/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9974 - val_loss: 0.9326\n",
            "Epoch 250/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.0445 - val_loss: 0.8732\n",
            "Epoch 251/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9873 - val_loss: 0.9217\n",
            "Epoch 252/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9736 - val_loss: 0.8497\n",
            "Epoch 253/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0392 - val_loss: 0.9271\n",
            "Epoch 254/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0826 - val_loss: 0.9563\n",
            "Epoch 255/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0155 - val_loss: 0.9047\n",
            "Epoch 256/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9903 - val_loss: 0.9136\n",
            "Epoch 257/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9867 - val_loss: 0.9004\n",
            "Epoch 258/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.0210 - val_loss: 1.0020\n",
            "Epoch 259/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9750 - val_loss: 0.9077\n",
            "Epoch 260/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9717 - val_loss: 0.8952\n",
            "Epoch 261/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0063 - val_loss: 0.9231\n",
            "Epoch 262/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9627 - val_loss: 0.8584\n",
            "Epoch 263/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0193 - val_loss: 0.9773\n",
            "Epoch 264/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9463 - val_loss: 0.9124\n",
            "Epoch 265/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0246 - val_loss: 0.9192\n",
            "Epoch 266/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9890 - val_loss: 0.8964\n",
            "Epoch 267/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9995 - val_loss: 0.8994\n",
            "Epoch 268/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9675 - val_loss: 0.8808\n",
            "Epoch 269/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9938 - val_loss: 0.8859\n",
            "Epoch 270/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.9874 - val_loss: 0.9092\n",
            "Epoch 271/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9739 - val_loss: 0.9044\n",
            "Epoch 272/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0206 - val_loss: 0.9030\n",
            "Epoch 273/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9904 - val_loss: 0.8768\n",
            "Epoch 274/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9536 - val_loss: 0.8267\n",
            "Epoch 275/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9742 - val_loss: 0.8905\n",
            "Epoch 276/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9482 - val_loss: 0.9078\n",
            "Epoch 277/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9923 - val_loss: 0.9110\n",
            "Epoch 278/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0025 - val_loss: 0.8956\n",
            "Epoch 279/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.0203 - val_loss: 0.8252\n",
            "Epoch 280/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9511 - val_loss: 0.9014\n",
            "Epoch 281/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9760 - val_loss: 0.8724\n",
            "Epoch 282/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9551 - val_loss: 0.8971\n",
            "Epoch 283/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9914 - val_loss: 0.8542\n",
            "Epoch 284/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9635 - val_loss: 0.8863\n",
            "Epoch 285/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9774 - val_loss: 0.8322\n",
            "Epoch 286/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9608 - val_loss: 0.8475\n",
            "Epoch 287/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9779 - val_loss: 0.9355\n",
            "Epoch 288/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9674 - val_loss: 0.8840\n",
            "Epoch 289/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0118 - val_loss: 0.8468\n",
            "Epoch 290/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9262 - val_loss: 0.8146\n",
            "Epoch 291/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.9888 - val_loss: 0.8609\n",
            "Epoch 292/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9576 - val_loss: 0.8591\n",
            "Epoch 293/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9607 - val_loss: 0.8518\n",
            "Epoch 294/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9683 - val_loss: 0.8763\n",
            "Epoch 295/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9338 - val_loss: 0.8466\n",
            "Epoch 296/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9525 - val_loss: 0.8807\n",
            "Epoch 297/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9275 - val_loss: 0.8618\n",
            "Epoch 298/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9742 - val_loss: 0.8490\n",
            "Epoch 299/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9622 - val_loss: 0.8843\n",
            "Epoch 300/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9668 - val_loss: 0.8306\n",
            "Epoch 301/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.9322 - val_loss: 0.8252\n",
            "Epoch 302/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9494 - val_loss: 0.8231\n",
            "Epoch 303/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9496 - val_loss: 0.8845\n",
            "Epoch 304/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9665 - val_loss: 0.8338\n",
            "Epoch 305/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9448 - val_loss: 0.8319\n",
            "Epoch 306/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9216 - val_loss: 0.8125\n",
            "Epoch 307/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9355 - val_loss: 0.8393\n",
            "Epoch 308/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9663 - val_loss: 0.8746\n",
            "Epoch 309/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9602 - val_loss: 0.8497\n",
            "Epoch 310/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9487 - val_loss: 0.8564\n",
            "Epoch 311/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9350 - val_loss: 0.8352\n",
            "Epoch 312/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9227 - val_loss: 0.8212\n",
            "Epoch 313/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9148 - val_loss: 0.8880\n",
            "Epoch 314/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9090 - val_loss: 0.8331\n",
            "Epoch 315/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9384 - val_loss: 0.8560\n",
            "Epoch 316/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9376 - val_loss: 0.8605\n",
            "Epoch 317/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9410 - val_loss: 0.8316\n",
            "Epoch 318/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9144 - val_loss: 0.8189\n",
            "Epoch 319/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9610 - val_loss: 0.8756\n",
            "Epoch 320/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9216 - val_loss: 0.8799\n",
            "Epoch 321/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9442 - val_loss: 0.8778\n",
            "Epoch 322/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9364 - val_loss: 0.8999\n",
            "Epoch 323/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9510 - val_loss: 0.8583\n",
            "Epoch 324/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9388 - val_loss: 0.8781\n",
            "Epoch 325/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9460 - val_loss: 0.8724\n",
            "Epoch 326/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9239 - val_loss: 0.8244\n",
            "Epoch 327/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9149 - val_loss: 0.8407\n",
            "Epoch 328/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9336 - val_loss: 0.8239\n",
            "Epoch 329/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9230 - val_loss: 0.8786\n",
            "Epoch 330/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9060 - val_loss: 0.9307\n",
            "Epoch 331/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.9262 - val_loss: 0.8680\n",
            "Epoch 332/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9441 - val_loss: 0.8924\n",
            "Epoch 333/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9020 - val_loss: 0.8045\n",
            "Epoch 334/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8989 - val_loss: 0.8436\n",
            "Epoch 335/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9117 - val_loss: 0.8549\n",
            "Epoch 336/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9120 - val_loss: 0.8173\n",
            "Epoch 337/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8950 - val_loss: 0.8425\n",
            "Epoch 338/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9247 - val_loss: 0.8342\n",
            "Epoch 339/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9218 - val_loss: 0.8881\n",
            "Epoch 340/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9437 - val_loss: 0.8032\n",
            "Epoch 341/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9145 - val_loss: 0.8481\n",
            "Epoch 342/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.9165 - val_loss: 0.8799\n",
            "Epoch 343/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9299 - val_loss: 0.8258\n",
            "Epoch 344/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9352 - val_loss: 0.8807\n",
            "Epoch 345/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8958 - val_loss: 0.8420\n",
            "Epoch 346/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8753 - val_loss: 0.7753\n",
            "Epoch 347/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9060 - val_loss: 0.8281\n",
            "Epoch 348/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9150 - val_loss: 0.8557\n",
            "Epoch 349/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8982 - val_loss: 0.8609\n",
            "Epoch 350/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9086 - val_loss: 0.8563\n",
            "Epoch 351/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8887 - val_loss: 0.8520\n",
            "Epoch 352/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9142 - val_loss: 0.8319\n",
            "Epoch 353/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8640 - val_loss: 0.8390\n",
            "Epoch 354/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8468 - val_loss: 0.8295\n",
            "Epoch 355/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9121 - val_loss: 0.7979\n",
            "Epoch 356/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8850 - val_loss: 0.7900\n",
            "Epoch 357/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9073 - val_loss: 0.7946\n",
            "Epoch 358/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9165 - val_loss: 0.7809\n",
            "Epoch 359/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.9013 - val_loss: 0.8093\n",
            "Epoch 360/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8615 - val_loss: 0.7965\n",
            "Epoch 361/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9019 - val_loss: 0.7711\n",
            "Epoch 362/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8906 - val_loss: 0.7966\n",
            "Epoch 363/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8875 - val_loss: 0.7689\n",
            "Epoch 364/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8956 - val_loss: 0.8270\n",
            "Epoch 365/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8729 - val_loss: 0.8271\n",
            "Epoch 366/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8865 - val_loss: 0.7631\n",
            "Epoch 367/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8995 - val_loss: 0.7820\n",
            "Epoch 368/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8869 - val_loss: 0.8334\n",
            "Epoch 369/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8641 - val_loss: 0.8160\n",
            "Epoch 370/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8576 - val_loss: 0.8109\n",
            "Epoch 371/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8665 - val_loss: 0.8810\n",
            "Epoch 372/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8715 - val_loss: 0.9073\n",
            "Epoch 373/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8864 - val_loss: 0.7872\n",
            "Epoch 374/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8856 - val_loss: 0.8601\n",
            "Epoch 375/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8851 - val_loss: 0.8259\n",
            "Epoch 376/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8477 - val_loss: 0.7810\n",
            "Epoch 377/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8645 - val_loss: 0.7898\n",
            "Epoch 378/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8783 - val_loss: 0.7876\n",
            "Epoch 379/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8250 - val_loss: 0.8226\n",
            "Epoch 380/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8552 - val_loss: 0.7887\n",
            "Epoch 381/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9072 - val_loss: 0.8213\n",
            "Epoch 382/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8546 - val_loss: 0.7509\n",
            "Epoch 383/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.8689 - val_loss: 0.8419\n",
            "Epoch 384/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8835 - val_loss: 0.8095\n",
            "Epoch 385/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8527 - val_loss: 0.8283\n",
            "Epoch 386/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8830 - val_loss: 0.7703\n",
            "Epoch 387/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8642 - val_loss: 0.8012\n",
            "Epoch 388/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8525 - val_loss: 0.8176\n",
            "Epoch 389/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8579 - val_loss: 0.8102\n",
            "Epoch 390/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8855 - val_loss: 0.8545\n",
            "Epoch 391/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8606 - val_loss: 0.7580\n",
            "Epoch 392/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8825 - val_loss: 0.7811\n",
            "Epoch 393/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8943 - val_loss: 0.7982\n",
            "Epoch 394/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8324 - val_loss: 0.7935\n",
            "Epoch 395/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8503 - val_loss: 0.8532\n",
            "Epoch 396/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8599 - val_loss: 0.8378\n",
            "Epoch 397/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8555 - val_loss: 0.7774\n",
            "Epoch 398/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8332 - val_loss: 0.8412\n",
            "Epoch 399/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8849 - val_loss: 0.7904\n",
            "Epoch 400/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8569 - val_loss: 0.7992\n",
            "Epoch 401/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8515 - val_loss: 0.8056\n",
            "Epoch 402/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8738 - val_loss: 0.8135\n",
            "Epoch 403/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8664 - val_loss: 0.7594\n",
            "Epoch 404/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8389 - val_loss: 0.7936\n",
            "Epoch 405/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8778 - val_loss: 0.7951\n",
            "Epoch 406/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8685 - val_loss: 0.8016\n",
            "Epoch 407/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8710 - val_loss: 0.7605\n",
            "Epoch 408/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8672 - val_loss: 0.8389\n",
            "Epoch 409/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8871 - val_loss: 0.7645\n",
            "Epoch 410/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8656 - val_loss: 0.7847\n",
            "Epoch 411/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8420 - val_loss: 0.8412\n",
            "Epoch 412/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8610 - val_loss: 0.7626\n",
            "Epoch 413/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8554 - val_loss: 0.7758\n",
            "Epoch 414/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8726 - val_loss: 0.8132\n",
            "Epoch 415/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8689 - val_loss: 0.7703\n",
            "Epoch 416/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8320 - val_loss: 0.8129\n",
            "Epoch 417/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8649 - val_loss: 0.7616\n",
            "Epoch 418/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8486 - val_loss: 0.8103\n",
            "Epoch 419/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8401 - val_loss: 0.8061\n",
            "Epoch 420/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8713 - val_loss: 0.7870\n",
            "Epoch 421/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8417 - val_loss: 0.7850\n",
            "Epoch 422/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8509 - val_loss: 0.8176\n",
            "Epoch 423/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8693 - val_loss: 0.7750\n",
            "Epoch 424/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8483 - val_loss: 0.7747\n",
            "Epoch 425/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8518 - val_loss: 0.8053\n",
            "Epoch 426/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8411 - val_loss: 0.7639\n",
            "Epoch 427/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8167 - val_loss: 0.7398\n",
            "Epoch 428/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8276 - val_loss: 0.7551\n",
            "Epoch 429/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8431 - val_loss: 0.7874\n",
            "Epoch 430/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8402 - val_loss: 0.7820\n",
            "Epoch 431/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8366 - val_loss: 0.7641\n",
            "Epoch 432/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8400 - val_loss: 0.8174\n",
            "Epoch 433/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8131 - val_loss: 0.7423\n",
            "Epoch 434/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8120 - val_loss: 0.7255\n",
            "Epoch 435/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8277 - val_loss: 0.7817\n",
            "Epoch 436/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8261 - val_loss: 0.8148\n",
            "Epoch 437/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8482 - val_loss: 0.7495\n",
            "Epoch 438/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8768 - val_loss: 0.7639\n",
            "Epoch 439/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8299 - val_loss: 0.7791\n",
            "Epoch 440/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8190 - val_loss: 0.7880\n",
            "Epoch 441/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8101 - val_loss: 0.7453\n",
            "Epoch 442/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8570 - val_loss: 0.7470\n",
            "Epoch 443/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8238 - val_loss: 0.7967\n",
            "Epoch 444/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8149 - val_loss: 0.8121\n",
            "Epoch 445/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8181 - val_loss: 0.7742\n",
            "Epoch 446/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8808 - val_loss: 0.7704\n",
            "Epoch 447/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8204 - val_loss: 0.8012\n",
            "Epoch 448/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8271 - val_loss: 0.7526\n",
            "Epoch 449/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8293 - val_loss: 0.8044\n",
            "Epoch 450/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8264 - val_loss: 0.8022\n",
            "Epoch 451/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8254 - val_loss: 0.7677\n",
            "Epoch 452/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8393 - val_loss: 0.7572\n",
            "Epoch 453/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8388 - val_loss: 0.8320\n",
            "Epoch 454/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8342 - val_loss: 0.7931\n",
            "Epoch 455/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8214 - val_loss: 0.7894\n",
            "Epoch 456/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8245 - val_loss: 0.7672\n",
            "Epoch 457/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8513 - val_loss: 0.7487\n",
            "Epoch 458/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8157 - val_loss: 0.7481\n",
            "Epoch 459/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8357 - val_loss: 0.7627\n",
            "Epoch 460/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8191 - val_loss: 0.8076\n",
            "Epoch 461/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8065 - val_loss: 0.7830\n",
            "Epoch 462/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8241 - val_loss: 0.8027\n",
            "Epoch 463/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8198 - val_loss: 0.7652\n",
            "Epoch 464/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8226 - val_loss: 0.7913\n",
            "Epoch 465/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8017 - val_loss: 0.7913\n",
            "Epoch 466/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8056 - val_loss: 0.7609\n",
            "Epoch 467/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8095 - val_loss: 0.7374\n",
            "Epoch 468/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8036 - val_loss: 0.7390\n",
            "Epoch 469/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7927 - val_loss: 0.7965\n",
            "Epoch 470/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7959 - val_loss: 0.7645\n",
            "Epoch 471/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8328 - val_loss: 0.7675\n",
            "Epoch 472/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7947 - val_loss: 0.7595\n",
            "Epoch 473/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8208 - val_loss: 0.8107\n",
            "Epoch 474/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8083 - val_loss: 0.8212\n",
            "Epoch 475/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7862 - val_loss: 0.7600\n",
            "Epoch 476/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8312 - val_loss: 0.7594\n",
            "Epoch 477/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8227 - val_loss: 0.7821\n",
            "Epoch 478/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8159 - val_loss: 0.7986\n",
            "Epoch 479/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8302 - val_loss: 0.7401\n",
            "Epoch 480/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8103 - val_loss: 0.7414\n",
            "Epoch 481/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8179 - val_loss: 0.7962\n",
            "Epoch 482/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8463 - val_loss: 0.7833\n",
            "Epoch 483/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8459 - val_loss: 0.7250\n",
            "Epoch 484/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8343 - val_loss: 0.7482\n",
            "Epoch 485/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7882 - val_loss: 0.7875\n",
            "Epoch 486/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7967 - val_loss: 0.7705\n",
            "Epoch 487/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8045 - val_loss: 0.7916\n",
            "Epoch 488/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7945 - val_loss: 0.7336\n",
            "Epoch 489/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8083 - val_loss: 0.7338\n",
            "Epoch 490/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7942 - val_loss: 0.7275\n",
            "Epoch 491/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8006 - val_loss: 0.7722\n",
            "Epoch 492/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7958 - val_loss: 0.7602\n",
            "Epoch 493/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8196 - val_loss: 0.7393\n",
            "Epoch 494/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7653 - val_loss: 0.7864\n",
            "Epoch 495/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7566 - val_loss: 0.7728\n",
            "Epoch 496/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7916 - val_loss: 0.7495\n",
            "Epoch 497/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7743 - val_loss: 0.8318\n",
            "Epoch 498/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7993 - val_loss: 0.7627\n",
            "Epoch 499/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8101 - val_loss: 0.7649\n",
            "Epoch 500/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7849 - val_loss: 0.7553\n",
            "Epoch 501/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8146 - val_loss: 0.7904\n",
            "Epoch 502/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7961 - val_loss: 0.7592\n",
            "Epoch 503/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7807 - val_loss: 0.7307\n",
            "Epoch 504/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7985 - val_loss: 0.7344\n",
            "Epoch 505/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8024 - val_loss: 0.7865\n",
            "Epoch 506/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8171 - val_loss: 0.7395\n",
            "Epoch 507/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8244 - val_loss: 0.7978\n",
            "Epoch 508/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7964 - val_loss: 0.7610\n",
            "Epoch 509/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8050 - val_loss: 0.7258\n",
            "Epoch 510/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7820 - val_loss: 0.7649\n",
            "Epoch 511/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8334 - val_loss: 0.7477\n",
            "Epoch 512/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7970 - val_loss: 0.7531\n",
            "Epoch 513/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7800 - val_loss: 0.7117\n",
            "Epoch 514/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8105 - val_loss: 0.7192\n",
            "Epoch 515/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7890 - val_loss: 0.7427\n",
            "Epoch 516/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7959 - val_loss: 0.7746\n",
            "Epoch 517/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7760 - val_loss: 0.7605\n",
            "Epoch 518/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7740 - val_loss: 0.7068\n",
            "Epoch 519/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7909 - val_loss: 0.7462\n",
            "Epoch 520/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7816 - val_loss: 0.7610\n",
            "Epoch 521/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7992 - val_loss: 0.7230\n",
            "Epoch 522/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7667 - val_loss: 0.7293\n",
            "Epoch 523/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8062 - val_loss: 0.7374\n",
            "Epoch 524/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7638 - val_loss: 0.7798\n",
            "Epoch 525/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7711 - val_loss: 0.8155\n",
            "Epoch 526/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7775 - val_loss: 0.7277\n",
            "Epoch 527/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7822 - val_loss: 0.7472\n",
            "Epoch 528/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7943 - val_loss: 0.7574\n",
            "Epoch 529/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7597 - val_loss: 0.7817\n",
            "Epoch 530/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7869 - val_loss: 0.7566\n",
            "Epoch 531/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7691 - val_loss: 0.7545\n",
            "Epoch 532/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7820 - val_loss: 0.7266\n",
            "Epoch 533/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7983 - val_loss: 0.7106\n",
            "Epoch 534/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7674 - val_loss: 0.7165\n",
            "Epoch 535/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8021 - val_loss: 0.7687\n",
            "Epoch 536/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7684 - val_loss: 0.7161\n",
            "Epoch 537/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7957 - val_loss: 0.7452\n",
            "Epoch 538/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7341 - val_loss: 0.7389\n",
            "Epoch 539/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7846 - val_loss: 0.7770\n",
            "Epoch 540/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7697 - val_loss: 0.7315\n",
            "Epoch 541/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7692 - val_loss: 0.7669\n",
            "Epoch 542/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7517 - val_loss: 0.7528\n",
            "Epoch 543/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7392 - val_loss: 0.7342\n",
            "Epoch 544/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7737 - val_loss: 0.7847\n",
            "Epoch 545/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8180 - val_loss: 0.8373\n",
            "Epoch 546/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7487 - val_loss: 0.7619\n",
            "Epoch 547/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7814 - val_loss: 0.7224\n",
            "Epoch 548/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7660 - val_loss: 0.7731\n",
            "Epoch 549/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7691 - val_loss: 0.7345\n",
            "Epoch 550/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8052 - val_loss: 0.7833\n",
            "Epoch 551/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7665 - val_loss: 0.7291\n",
            "Epoch 552/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7818 - val_loss: 0.7823\n",
            "Epoch 553/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7752 - val_loss: 0.7157\n",
            "Epoch 554/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7493 - val_loss: 0.7354\n",
            "Epoch 555/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8033 - val_loss: 0.7299\n",
            "Epoch 556/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7578 - val_loss: 0.7451\n",
            "Epoch 557/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7592 - val_loss: 0.7633\n",
            "Epoch 558/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7613 - val_loss: 0.7596\n",
            "Epoch 559/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7449 - val_loss: 0.7426\n",
            "Epoch 560/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7376 - val_loss: 0.7265\n",
            "Epoch 561/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7653 - val_loss: 0.7522\n",
            "Epoch 562/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7386 - val_loss: 0.6960\n",
            "Epoch 563/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7721 - val_loss: 0.7788\n",
            "Epoch 564/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7488 - val_loss: 0.8017\n",
            "Epoch 565/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7554 - val_loss: 0.7362\n",
            "Epoch 566/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7501 - val_loss: 0.7618\n",
            "Epoch 567/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7417 - val_loss: 0.7020\n",
            "Epoch 568/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7752 - val_loss: 0.7108\n",
            "Epoch 569/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7511 - val_loss: 0.7125\n",
            "Epoch 570/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7715 - val_loss: 0.7475\n",
            "Epoch 571/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7494 - val_loss: 0.7379\n",
            "Epoch 572/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7662 - val_loss: 0.7884\n",
            "Epoch 573/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7299 - val_loss: 0.7272\n",
            "Epoch 574/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7876 - val_loss: 0.6920\n",
            "Epoch 575/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7811 - val_loss: 0.7524\n",
            "Epoch 576/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7764 - val_loss: 0.6981\n",
            "Epoch 577/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7627 - val_loss: 0.7019\n",
            "Epoch 578/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7402 - val_loss: 0.6961\n",
            "Epoch 579/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7728 - val_loss: 0.7262\n",
            "Epoch 580/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7282 - val_loss: 0.7464\n",
            "Epoch 581/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7428 - val_loss: 0.7069\n",
            "Epoch 582/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7248 - val_loss: 0.7640\n",
            "Epoch 583/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7641 - val_loss: 0.7272\n",
            "Epoch 584/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7488 - val_loss: 0.7335\n",
            "Epoch 585/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8166 - val_loss: 0.6920\n",
            "Epoch 586/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7441 - val_loss: 0.6971\n",
            "Epoch 587/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7465 - val_loss: 0.7257\n",
            "Epoch 588/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7347 - val_loss: 0.7101\n",
            "Epoch 589/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7666 - val_loss: 0.7461\n",
            "Epoch 590/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7351 - val_loss: 0.7605\n",
            "Epoch 591/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7855 - val_loss: 0.7512\n",
            "Epoch 592/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.7458 - val_loss: 0.7648\n",
            "Epoch 593/1000\n",
            "52/52 [==============================] - 3s 57ms/step - loss: 0.7554 - val_loss: 0.7635\n",
            "Epoch 594/1000\n",
            "52/52 [==============================] - 3s 61ms/step - loss: 0.7479 - val_loss: 0.7755\n",
            "Epoch 595/1000\n",
            "52/52 [==============================] - 4s 68ms/step - loss: 0.7596 - val_loss: 0.7015\n",
            "Epoch 596/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.7499 - val_loss: 0.7200\n",
            "Epoch 597/1000\n",
            "52/52 [==============================] - 3s 61ms/step - loss: 0.7263 - val_loss: 0.7354\n",
            "Epoch 598/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7792 - val_loss: 0.7148\n",
            "Epoch 599/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7481 - val_loss: 0.6959\n",
            "Epoch 600/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7556 - val_loss: 0.7143\n",
            "Epoch 601/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7400 - val_loss: 0.7507\n",
            "Epoch 602/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7099 - val_loss: 0.7444\n",
            "Epoch 603/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7564 - val_loss: 0.7289\n",
            "Epoch 604/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7327 - val_loss: 0.7411\n",
            "Epoch 605/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7311 - val_loss: 0.7049\n",
            "Epoch 606/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7366 - val_loss: 0.7337\n",
            "Epoch 607/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7421 - val_loss: 0.7377\n",
            "Epoch 608/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7349 - val_loss: 0.7214\n",
            "Epoch 609/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7483 - val_loss: 0.7706\n",
            "Epoch 610/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7434 - val_loss: 0.7301\n",
            "Epoch 611/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7203 - val_loss: 0.7176\n",
            "Epoch 612/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7266 - val_loss: 0.7378\n",
            "Epoch 613/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7427 - val_loss: 0.6823\n",
            "Epoch 614/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7551 - val_loss: 0.7346\n",
            "Epoch 615/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7269 - val_loss: 0.7008\n",
            "Epoch 616/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7251 - val_loss: 0.7141\n",
            "Epoch 617/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7336 - val_loss: 0.7008\n",
            "Epoch 618/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7204 - val_loss: 0.7759\n",
            "Epoch 619/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7469 - val_loss: 0.7332\n",
            "Epoch 620/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7261 - val_loss: 0.7089\n",
            "Epoch 621/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7156 - val_loss: 0.7268\n",
            "Epoch 622/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7358 - val_loss: 0.7231\n",
            "Epoch 623/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7634 - val_loss: 0.7167\n",
            "Epoch 624/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7474 - val_loss: 0.7370\n",
            "Epoch 625/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7318 - val_loss: 0.6982\n",
            "Epoch 626/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7438 - val_loss: 0.7129\n",
            "Epoch 627/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7271 - val_loss: 0.7156\n",
            "Epoch 628/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7389 - val_loss: 0.7249\n",
            "Epoch 629/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7452 - val_loss: 0.7387\n",
            "Epoch 630/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7340 - val_loss: 0.6912\n",
            "Epoch 631/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7458 - val_loss: 0.7037\n",
            "Epoch 632/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7267 - val_loss: 0.7149\n",
            "Epoch 633/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7057 - val_loss: 0.7214\n",
            "Epoch 634/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7101 - val_loss: 0.7119\n",
            "Epoch 635/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6965 - val_loss: 0.6864\n",
            "Epoch 636/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7492 - val_loss: 0.7074\n",
            "Epoch 637/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7142 - val_loss: 0.7137\n",
            "Epoch 638/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7206 - val_loss: 0.7097\n",
            "Epoch 639/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7255 - val_loss: 0.6899\n",
            "Epoch 640/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7540 - val_loss: 0.7027\n",
            "Epoch 641/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7386 - val_loss: 0.6998\n",
            "Epoch 642/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7188 - val_loss: 0.7662\n",
            "Epoch 643/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7109 - val_loss: 0.7306\n",
            "Epoch 644/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7360 - val_loss: 0.7805\n",
            "Epoch 645/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7384 - val_loss: 0.7690\n",
            "Epoch 646/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7294 - val_loss: 0.7223\n",
            "Epoch 647/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7114 - val_loss: 0.7463\n",
            "Epoch 648/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7368 - val_loss: 0.7256\n",
            "Epoch 649/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7323 - val_loss: 0.7101\n",
            "Epoch 650/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7308 - val_loss: 0.7545\n",
            "Epoch 651/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6996 - val_loss: 0.6932\n",
            "Epoch 652/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7078 - val_loss: 0.7315\n",
            "Epoch 653/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7055 - val_loss: 0.6952\n",
            "Epoch 654/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7220 - val_loss: 0.7180\n",
            "Epoch 655/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7402 - val_loss: 0.7206\n",
            "Epoch 656/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7335 - val_loss: 0.7046\n",
            "Epoch 657/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7021 - val_loss: 0.7047\n",
            "Epoch 658/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7035 - val_loss: 0.7356\n",
            "Epoch 659/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7032 - val_loss: 0.7079\n",
            "Epoch 660/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7192 - val_loss: 0.6727\n",
            "Epoch 661/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7336 - val_loss: 0.6811\n",
            "Epoch 662/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.7290 - val_loss: 0.7422\n",
            "Epoch 663/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6952 - val_loss: 0.6870\n",
            "Epoch 664/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7287 - val_loss: 0.7123\n",
            "Epoch 665/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7227 - val_loss: 0.7238\n",
            "Epoch 666/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7267 - val_loss: 0.7187\n",
            "Epoch 667/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7299 - val_loss: 0.7375\n",
            "Epoch 668/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7347 - val_loss: 0.7303\n",
            "Epoch 669/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7302 - val_loss: 0.6778\n",
            "Epoch 670/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6970 - val_loss: 0.6878\n",
            "Epoch 671/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7355 - val_loss: 0.7320\n",
            "Epoch 672/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7076 - val_loss: 0.6997\n",
            "Epoch 673/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7205 - val_loss: 0.6948\n",
            "Epoch 674/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7087 - val_loss: 0.7255\n",
            "Epoch 675/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7097 - val_loss: 0.7230\n",
            "Epoch 676/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7348 - val_loss: 0.7462\n",
            "Epoch 677/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6987 - val_loss: 0.7245\n",
            "Epoch 678/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7576 - val_loss: 0.6970\n",
            "Epoch 679/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6857 - val_loss: 0.6700\n",
            "Epoch 680/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7136 - val_loss: 0.7107\n",
            "Epoch 681/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6972 - val_loss: 0.7081\n",
            "Epoch 682/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7028 - val_loss: 0.7107\n",
            "Epoch 683/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7016 - val_loss: 0.7218\n",
            "Epoch 684/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7217 - val_loss: 0.6902\n",
            "Epoch 685/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7139 - val_loss: 0.6669\n",
            "Epoch 686/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7249 - val_loss: 0.7063\n",
            "Epoch 687/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7139 - val_loss: 0.6679\n",
            "Epoch 688/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6912 - val_loss: 0.7021\n",
            "Epoch 689/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7042 - val_loss: 0.7164\n",
            "Epoch 690/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7072 - val_loss: 0.6888\n",
            "Epoch 691/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.7149 - val_loss: 0.7037\n",
            "Epoch 692/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7065 - val_loss: 0.6973\n",
            "Epoch 693/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7208 - val_loss: 0.6733\n",
            "Epoch 694/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6912 - val_loss: 0.6972\n",
            "Epoch 695/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6807 - val_loss: 0.7054\n",
            "Epoch 696/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6936 - val_loss: 0.6854\n",
            "Epoch 697/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6635 - val_loss: 0.7213\n",
            "Epoch 698/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7190 - val_loss: 0.6962\n",
            "Epoch 699/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7215 - val_loss: 0.7143\n",
            "Epoch 700/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7047 - val_loss: 0.6907\n",
            "Epoch 701/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7149 - val_loss: 0.7035\n",
            "Epoch 702/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6746 - val_loss: 0.6635\n",
            "Epoch 703/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7206 - val_loss: 0.6759\n",
            "Epoch 704/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7062 - val_loss: 0.6892\n",
            "Epoch 705/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7323 - val_loss: 0.7305\n",
            "Epoch 706/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6976 - val_loss: 0.7217\n",
            "Epoch 707/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6867 - val_loss: 0.6735\n",
            "Epoch 708/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6715 - val_loss: 0.7173\n",
            "Epoch 709/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7056 - val_loss: 0.7254\n",
            "Epoch 710/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6816 - val_loss: 0.6991\n",
            "Epoch 711/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7070 - val_loss: 0.7130\n",
            "Epoch 712/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6743 - val_loss: 0.6904\n",
            "Epoch 713/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6870 - val_loss: 0.7168\n",
            "Epoch 714/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6838 - val_loss: 0.6837\n",
            "Epoch 715/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7337 - val_loss: 0.7164\n",
            "Epoch 716/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6860 - val_loss: 0.6664\n",
            "Epoch 717/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7045 - val_loss: 0.6624\n",
            "Epoch 718/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7117 - val_loss: 0.7137\n",
            "Epoch 719/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6745 - val_loss: 0.7112\n",
            "Epoch 720/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7076 - val_loss: 0.7006\n",
            "Epoch 721/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7024 - val_loss: 0.7062\n",
            "Epoch 722/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6744 - val_loss: 0.7036\n",
            "Epoch 723/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6827 - val_loss: 0.7089\n",
            "Epoch 724/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6891 - val_loss: 0.6873\n",
            "Epoch 725/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6699 - val_loss: 0.7019\n",
            "Epoch 726/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6877 - val_loss: 0.6796\n",
            "Epoch 727/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6961 - val_loss: 0.7116\n",
            "Epoch 728/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6826 - val_loss: 0.6619\n",
            "Epoch 729/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6595 - val_loss: 0.7293\n",
            "Epoch 730/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6719 - val_loss: 0.7034\n",
            "Epoch 731/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6804 - val_loss: 0.6854\n",
            "Epoch 732/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6765 - val_loss: 0.7013\n",
            "Epoch 733/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6841 - val_loss: 0.6787\n",
            "Epoch 734/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6738 - val_loss: 0.6756\n",
            "Epoch 735/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6637 - val_loss: 0.6722\n",
            "Epoch 736/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6890 - val_loss: 0.6631\n",
            "Epoch 737/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6785 - val_loss: 0.6841\n",
            "Epoch 738/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7218 - val_loss: 0.6696\n",
            "Epoch 739/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6871 - val_loss: 0.6848\n",
            "Epoch 740/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6815 - val_loss: 0.6866\n",
            "Epoch 741/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.7017 - val_loss: 0.6912\n",
            "Epoch 742/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6643 - val_loss: 0.6783\n",
            "Epoch 743/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6726 - val_loss: 0.7059\n",
            "Epoch 744/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6591 - val_loss: 0.6831\n",
            "Epoch 745/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6761 - val_loss: 0.6653\n",
            "Epoch 746/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6969 - val_loss: 0.7066\n",
            "Epoch 747/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6504 - val_loss: 0.6452\n",
            "Epoch 748/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6725 - val_loss: 0.6558\n",
            "Epoch 749/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6487 - val_loss: 0.6845\n",
            "Epoch 750/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6739 - val_loss: 0.6919\n",
            "Epoch 751/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6845 - val_loss: 0.6829\n",
            "Epoch 752/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6626 - val_loss: 0.6893\n",
            "Epoch 753/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6564 - val_loss: 0.6708\n",
            "Epoch 754/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6950 - val_loss: 0.6830\n",
            "Epoch 755/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6758 - val_loss: 0.6817\n",
            "Epoch 756/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6973 - val_loss: 0.6857\n",
            "Epoch 757/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6733 - val_loss: 0.6875\n",
            "Epoch 758/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6723 - val_loss: 0.6909\n",
            "Epoch 759/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7031 - val_loss: 0.6878\n",
            "Epoch 760/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6638 - val_loss: 0.7049\n",
            "Epoch 761/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6901 - val_loss: 0.6915\n",
            "Epoch 762/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6972 - val_loss: 0.6966\n",
            "Epoch 763/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6540 - val_loss: 0.6748\n",
            "Epoch 764/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6688 - val_loss: 0.6970\n",
            "Epoch 765/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6522 - val_loss: 0.6847\n",
            "Epoch 766/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6447 - val_loss: 0.6848\n",
            "Epoch 767/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6681 - val_loss: 0.6943\n",
            "Epoch 768/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6940 - val_loss: 0.6902\n",
            "Epoch 769/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6583 - val_loss: 0.7177\n",
            "Epoch 770/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6815 - val_loss: 0.7153\n",
            "Epoch 771/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7157 - val_loss: 0.6907\n",
            "Epoch 772/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6967 - val_loss: 0.6711\n",
            "Epoch 773/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6795 - val_loss: 0.6949\n",
            "Epoch 774/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6861 - val_loss: 0.6632\n",
            "Epoch 775/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6713 - val_loss: 0.7116\n",
            "Epoch 776/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6880 - val_loss: 0.6903\n",
            "Epoch 777/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6820 - val_loss: 0.6927\n",
            "Epoch 778/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6695 - val_loss: 0.6544\n",
            "Epoch 779/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6630 - val_loss: 0.7089\n",
            "Epoch 780/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7010 - val_loss: 0.6913\n",
            "Epoch 781/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6724 - val_loss: 0.7100\n",
            "Epoch 782/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6653 - val_loss: 0.6970\n",
            "Epoch 783/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6669 - val_loss: 0.6669\n",
            "Epoch 784/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6913 - val_loss: 0.6816\n",
            "Epoch 785/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6408 - val_loss: 0.6910\n",
            "Epoch 786/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6803 - val_loss: 0.7439\n",
            "Epoch 787/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6623 - val_loss: 0.6450\n",
            "Epoch 788/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6967 - val_loss: 0.7046\n",
            "Epoch 789/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6541 - val_loss: 0.6952\n",
            "Epoch 790/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6614 - val_loss: 0.6702\n",
            "Epoch 791/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6857 - val_loss: 0.6654\n",
            "Epoch 792/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6661 - val_loss: 0.7011\n",
            "Epoch 793/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6651 - val_loss: 0.6704\n",
            "Epoch 794/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6246 - val_loss: 0.6775\n",
            "Epoch 795/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6703 - val_loss: 0.6985\n",
            "Epoch 796/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6455 - val_loss: 0.6859\n",
            "Epoch 797/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6469 - val_loss: 0.7056\n",
            "Epoch 798/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6793 - val_loss: 0.6751\n",
            "Epoch 799/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6722 - val_loss: 0.6715\n",
            "Epoch 800/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6875 - val_loss: 0.6552\n",
            "Epoch 801/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6581 - val_loss: 0.6540\n",
            "Epoch 802/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6790 - val_loss: 0.6731\n",
            "Epoch 803/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6614 - val_loss: 0.6603\n",
            "Epoch 804/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6414 - val_loss: 0.6845\n",
            "Epoch 805/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6691 - val_loss: 0.6728\n",
            "Epoch 806/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6755 - val_loss: 0.6740\n",
            "Epoch 807/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6627 - val_loss: 0.7345\n",
            "Epoch 808/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6188 - val_loss: 0.6576\n",
            "Epoch 809/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6526 - val_loss: 0.6803\n",
            "Epoch 810/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6389 - val_loss: 0.6762\n",
            "Epoch 811/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6577 - val_loss: 0.6481\n",
            "Epoch 812/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6564 - val_loss: 0.6669\n",
            "Epoch 813/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6501 - val_loss: 0.6762\n",
            "Epoch 814/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6935 - val_loss: 0.6657\n",
            "Epoch 815/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6597 - val_loss: 0.6608\n",
            "Epoch 816/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6526 - val_loss: 0.6810\n",
            "Epoch 817/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6342 - val_loss: 0.6611\n",
            "Epoch 818/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6520 - val_loss: 0.6818\n",
            "Epoch 819/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6639 - val_loss: 0.6595\n",
            "Epoch 820/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6293 - val_loss: 0.6928\n",
            "Epoch 821/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6479 - val_loss: 0.6749\n",
            "Epoch 822/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6437 - val_loss: 0.7119\n",
            "Epoch 823/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6542 - val_loss: 0.6970\n",
            "Epoch 824/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6639 - val_loss: 0.6524\n",
            "Epoch 825/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6817 - val_loss: 0.6685\n",
            "Epoch 826/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6354 - val_loss: 0.6601\n",
            "Epoch 827/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6330 - val_loss: 0.6620\n",
            "Epoch 828/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6852 - val_loss: 0.6588\n",
            "Epoch 829/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6656 - val_loss: 0.6708\n",
            "Epoch 830/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6643 - val_loss: 0.6858\n",
            "Epoch 831/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6127 - val_loss: 0.6626\n",
            "Epoch 832/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6698 - val_loss: 0.6673\n",
            "Epoch 833/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6851 - val_loss: 0.6751\n",
            "Epoch 834/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6233 - val_loss: 0.7196\n",
            "Epoch 835/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6377 - val_loss: 0.6589\n",
            "Epoch 836/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6510 - val_loss: 0.6921\n",
            "Epoch 837/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6374 - val_loss: 0.6946\n",
            "Epoch 838/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6717 - val_loss: 0.6923\n",
            "Epoch 839/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6226 - val_loss: 0.6703\n",
            "Epoch 840/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6358 - val_loss: 0.6813\n",
            "Epoch 841/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6480 - val_loss: 0.6646\n",
            "Epoch 842/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6432 - val_loss: 0.6687\n",
            "Epoch 843/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6435 - val_loss: 0.6473\n",
            "Epoch 844/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6614 - val_loss: 0.6574\n",
            "Epoch 845/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6499 - val_loss: 0.6897\n",
            "Epoch 846/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6529 - val_loss: 0.6838\n",
            "Epoch 847/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6586 - val_loss: 0.6356\n",
            "Epoch 848/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6712 - val_loss: 0.6447\n",
            "Epoch 849/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6462 - val_loss: 0.6768\n",
            "Epoch 850/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6303 - val_loss: 0.6461\n",
            "Epoch 851/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6514 - val_loss: 0.6454\n",
            "Epoch 852/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6568 - val_loss: 0.6444\n",
            "Epoch 853/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6497 - val_loss: 0.6573\n",
            "Epoch 854/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6415 - val_loss: 0.6567\n",
            "Epoch 855/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6606 - val_loss: 0.6571\n",
            "Epoch 856/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6584 - val_loss: 0.7035\n",
            "Epoch 857/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6318 - val_loss: 0.6691\n",
            "Epoch 858/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6814 - val_loss: 0.6577\n",
            "Epoch 859/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6231 - val_loss: 0.6509\n",
            "Epoch 860/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6351 - val_loss: 0.6380\n",
            "Epoch 861/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6340 - val_loss: 0.6643\n",
            "Epoch 862/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6933 - val_loss: 0.6606\n",
            "Epoch 863/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6698 - val_loss: 0.6501\n",
            "Epoch 864/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6438 - val_loss: 0.6643\n",
            "Epoch 865/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6542 - val_loss: 0.6874\n",
            "Epoch 866/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6192 - val_loss: 0.6601\n",
            "Epoch 867/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6738 - val_loss: 0.6766\n",
            "Epoch 868/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6370 - val_loss: 0.7109\n",
            "Epoch 869/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6526 - val_loss: 0.6938\n",
            "Epoch 870/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6250 - val_loss: 0.6771\n",
            "Epoch 871/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6444 - val_loss: 0.6615\n",
            "Epoch 872/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6360 - val_loss: 0.6753\n",
            "Epoch 873/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6629 - val_loss: 0.6778\n",
            "Epoch 874/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6275 - val_loss: 0.6576\n",
            "Epoch 875/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6505 - val_loss: 0.6505\n",
            "Epoch 876/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6374 - val_loss: 0.7058\n",
            "Epoch 877/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6165 - val_loss: 0.6485\n",
            "Epoch 878/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6532 - val_loss: 0.6899\n",
            "Epoch 879/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6352 - val_loss: 0.6811\n",
            "Epoch 880/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6544 - val_loss: 0.6497\n",
            "Epoch 881/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6485 - val_loss: 0.6705\n",
            "Epoch 882/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6547 - val_loss: 0.6492\n",
            "Epoch 883/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6348 - val_loss: 0.6560\n",
            "Epoch 884/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6495 - val_loss: 0.6606\n",
            "Epoch 885/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6565 - val_loss: 0.6567\n",
            "Epoch 886/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6633 - val_loss: 0.6492\n",
            "Epoch 887/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6436 - val_loss: 0.6658\n",
            "Epoch 888/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6410 - val_loss: 0.6503\n",
            "Epoch 889/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6374 - val_loss: 0.6797\n",
            "Epoch 890/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6391 - val_loss: 0.6758\n",
            "Epoch 891/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6514 - val_loss: 0.6490\n",
            "Epoch 892/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6290 - val_loss: 0.6800\n",
            "Epoch 893/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6602 - val_loss: 0.6719\n",
            "Epoch 894/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6282 - val_loss: 0.6604\n",
            "Epoch 895/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6426 - val_loss: 0.6518\n",
            "Epoch 896/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6430 - val_loss: 0.6358\n",
            "Epoch 897/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6319 - val_loss: 0.6660\n",
            "Epoch 898/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6149 - val_loss: 0.6457\n",
            "Epoch 899/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6249 - val_loss: 0.6528\n",
            "Epoch 900/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6276 - val_loss: 0.6375\n",
            "Epoch 901/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6436 - val_loss: 0.6670\n",
            "Epoch 902/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6103 - val_loss: 0.6598\n",
            "Epoch 903/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6277 - val_loss: 0.6722\n",
            "Epoch 904/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6263 - val_loss: 0.6563\n",
            "Epoch 905/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6338 - val_loss: 0.6451\n",
            "Epoch 906/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6534 - val_loss: 0.6878\n",
            "Epoch 907/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6249 - val_loss: 0.6569\n",
            "Epoch 908/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6498 - val_loss: 0.6558\n",
            "Epoch 909/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6127 - val_loss: 0.6882\n",
            "Epoch 910/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6405 - val_loss: 0.6526\n",
            "Epoch 911/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6403 - val_loss: 0.6498\n",
            "Epoch 912/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6444 - val_loss: 0.6623\n",
            "Epoch 913/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6213 - val_loss: 0.6664\n",
            "Epoch 914/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5980 - val_loss: 0.6600\n",
            "Epoch 915/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6413 - val_loss: 0.6764\n",
            "Epoch 916/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6261 - val_loss: 0.6502\n",
            "Epoch 917/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6267 - val_loss: 0.6459\n",
            "Epoch 918/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6216 - val_loss: 0.6711\n",
            "Epoch 919/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6278 - val_loss: 0.6644\n",
            "Epoch 920/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6301 - val_loss: 0.6495\n",
            "Epoch 921/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6200 - val_loss: 0.6487\n",
            "Epoch 922/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6285 - val_loss: 0.6441\n",
            "Epoch 923/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6488 - val_loss: 0.6464\n",
            "Epoch 924/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6401 - val_loss: 0.6647\n",
            "Epoch 925/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6188 - val_loss: 0.6400\n",
            "Epoch 926/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6128 - val_loss: 0.6449\n",
            "Epoch 927/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6529 - val_loss: 0.6427\n",
            "Epoch 928/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6415 - val_loss: 0.6494\n",
            "Epoch 929/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5982 - val_loss: 0.6504\n",
            "Epoch 930/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6278 - val_loss: 0.6380\n",
            "Epoch 931/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6215 - val_loss: 0.6298\n",
            "Epoch 932/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5996 - val_loss: 0.6599\n",
            "Epoch 933/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6425 - val_loss: 0.6553\n",
            "Epoch 934/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6173 - val_loss: 0.6726\n",
            "Epoch 935/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6048 - val_loss: 0.6524\n",
            "Epoch 936/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6284 - val_loss: 0.6534\n",
            "Epoch 937/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6019 - val_loss: 0.6327\n",
            "Epoch 938/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6327 - val_loss: 0.6538\n",
            "Epoch 939/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6211 - val_loss: 0.6486\n",
            "Epoch 940/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6060 - val_loss: 0.6236\n",
            "Epoch 941/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6221 - val_loss: 0.6463\n",
            "Epoch 942/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6127 - val_loss: 0.6335\n",
            "Epoch 943/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6608 - val_loss: 0.6503\n",
            "Epoch 944/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6333 - val_loss: 0.6512\n",
            "Epoch 945/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6335 - val_loss: 0.6635\n",
            "Epoch 946/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6182 - val_loss: 0.6606\n",
            "Epoch 947/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6197 - val_loss: 0.6530\n",
            "Epoch 948/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6039 - val_loss: 0.6680\n",
            "Epoch 949/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6266 - val_loss: 0.6461\n",
            "Epoch 950/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6336 - val_loss: 0.6541\n",
            "Epoch 951/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6013 - val_loss: 0.6402\n",
            "Epoch 952/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6353 - val_loss: 0.6480\n",
            "Epoch 953/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6393 - val_loss: 0.6575\n",
            "Epoch 954/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6239 - val_loss: 0.6368\n",
            "Epoch 955/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6365 - val_loss: 0.6460\n",
            "Epoch 956/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5917 - val_loss: 0.6567\n",
            "Epoch 957/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6460 - val_loss: 0.6551\n",
            "Epoch 958/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6116 - val_loss: 0.6611\n",
            "Epoch 959/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6111 - val_loss: 0.6385\n",
            "Epoch 960/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5923 - val_loss: 0.6441\n",
            "Epoch 961/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6194 - val_loss: 0.6388\n",
            "Epoch 962/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6067 - val_loss: 0.6522\n",
            "Epoch 963/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5728 - val_loss: 0.6321\n",
            "Epoch 964/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6444 - val_loss: 0.6427\n",
            "Epoch 965/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6280 - val_loss: 0.6385\n",
            "Epoch 966/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6197 - val_loss: 0.6378\n",
            "Epoch 967/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6008 - val_loss: 0.6488\n",
            "Epoch 968/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6055 - val_loss: 0.6539\n",
            "Epoch 969/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6013 - val_loss: 0.6611\n",
            "Epoch 970/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5874 - val_loss: 0.6415\n",
            "Epoch 971/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6198 - val_loss: 0.6714\n",
            "Epoch 972/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6309 - val_loss: 0.6451\n",
            "Epoch 973/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6135 - val_loss: 0.6554\n",
            "Epoch 974/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6200 - val_loss: 0.6427\n",
            "Epoch 975/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5966 - val_loss: 0.6594\n",
            "Epoch 976/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6117 - val_loss: 0.6734\n",
            "Epoch 977/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5864 - val_loss: 0.6400\n",
            "Epoch 978/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6126 - val_loss: 0.6472\n",
            "Epoch 979/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6056 - val_loss: 0.6452\n",
            "Epoch 980/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6112 - val_loss: 0.6378\n",
            "Epoch 981/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6236 - val_loss: 0.6304\n",
            "Epoch 982/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6184 - val_loss: 0.6656\n",
            "Epoch 983/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6136 - val_loss: 0.6872\n",
            "Epoch 984/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6219 - val_loss: 0.6481\n",
            "Epoch 985/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5820 - val_loss: 0.6512\n",
            "Epoch 986/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5967 - val_loss: 0.6396\n",
            "Epoch 987/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6146 - val_loss: 0.6518\n",
            "Epoch 988/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6338 - val_loss: 0.6530\n",
            "Epoch 989/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6059 - val_loss: 0.6468\n",
            "Epoch 990/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5744 - val_loss: 0.6323\n",
            "Epoch 991/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5953 - val_loss: 0.6456\n",
            "Epoch 992/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6116 - val_loss: 0.6299\n",
            "Epoch 993/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5888 - val_loss: 0.6547\n",
            "Epoch 995/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6246 - val_loss: 0.6738\n",
            "Epoch 996/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5993 - val_loss: 0.6289\n",
            "Epoch 997/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6206 - val_loss: 0.6374\n",
            "Epoch 998/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5942 - val_loss: 0.6431\n",
            "Epoch 999/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5981 - val_loss: 0.6333\n",
            "Epoch 1000/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6058 - val_loss: 0.6393\n"
          ]
        }
      ],
      "source": [
        "#run call the transformer model\n",
        "#df_read = df.copy()\n",
        "#df_read=(df_read-df_read.mean())/df_read.std()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(df_read, test_size=0.2)\n",
        "\n",
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "test = df5.copy()\n",
        "\n",
        "Y_train = np.array(train['class'])\n",
        "X_train= np.array(train.drop(['class'],axis=1))\n",
        "X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
        "\n",
        "Y_val=np.array(test['class'])\n",
        "X_val = np.array(test.drop(['class'],axis=1))\n",
        "X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])\n",
        "NUM_LAYERS =  4\n",
        "\n",
        "D_MODEL = X_train.shape[2]\n",
        "NUM_HEADS =  4\n",
        "UNITS =  2048\n",
        "DROPOUT = 0.1 #0.1\n",
        "TIME_STEPS= X_train.shape[1]\n",
        "OUTPUT_SIZE=1\n",
        "batch_size=64\n",
        "\n",
        "model = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=NUM_LAYERS,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "\n",
        "#run\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='mae') #org\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss='mse')\n",
        "# model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mae')\n",
        "#\n",
        "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=80, restore_best_weights=True)\n",
        "# history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UxMyaAwpcqdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbac5536-bc88-44b7-c254-69cd4d388538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00018871261412838856\n",
            "(1660, 1, 180)\n",
            "(415, 1, 180)\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "import time\n",
        "st = time.time()\n",
        "p1 = np.array(model(X_val)).flatten()\n",
        "end = time.time()\n",
        "# print(end, st, len(p1))\n",
        "print((end-st)/len(p1))\n",
        "p2 = np.array(model(X_train)).flatten()\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Say3gnZgi331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474769eb-77c7-4845-aba9-25c3a6b52ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.86681849]\n",
            " [0.86681849 1.        ]] SignificanceResult(statistic=0.8595704755169683, pvalue=0.0) 0.6685008679331826\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7960466964815829, 0.7827075294885851, 0.5884092257505961)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import kendalltau\n",
        "print(np.corrcoef(p2, Y_train), stats.spearmanr(p2, Y_train), kendalltau(p2,Y_train).correlation)\n",
        "np.corrcoef(p1, Y_val)[1][0], stats.spearmanr(p1, Y_val).correlation, kendalltau(p1,Y_val).correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ST5WiH4KoeOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfacb2f5-7fcd-4e82-8bb9-419f715e6095"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1660, 1, 180)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ksXNLzMScqg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "bd051117-9962-47b7-ee0e-14cbd2293758"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"audio_dm_3_featu_.jpg\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}