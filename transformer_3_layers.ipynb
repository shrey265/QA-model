{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jP5uwXtZay_",
        "outputId": "03b439bc-5475-40b7-8823-90ba1971c414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRPF7T0b-QZ",
        "outputId": "2074e160-2ac0-4676-c8c6-66f569a748e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spafe\n",
            "  Downloading spafe-0.3.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.11.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from spafe) (4.5.0)\n",
            "Installing collected packages: spafe\n",
            "Successfully installed spafe-0.3.2\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "!pip install -U spafe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s2K5aSHYb-Sm"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-xNIFhPtb-U5"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QgbvNKYAb-YM"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet50,ResNet101\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.model_selection import StratifiedKFold , KFold ,RepeatedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LXsQV7ivcIcc"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  return tf.matmul(attention_weights, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WB2_09_e-qt"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# This allows to the transformer to know where there is real data and where it is padded\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PjEQIR8TcIe3"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8hF4xYdLcSQA"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j7ewC3iBcSSo"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_8IFloblcSWJ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder(time_steps,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            projection,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  if projection=='linear':\n",
        "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
        "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
        "    print('linear')\n",
        "\n",
        "  else:\n",
        "    projection=tf.identity(inputs)\n",
        "    print('none')\n",
        "\n",
        "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NXgJh2PpcIiK"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def transformer(time_steps,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                output_size,\n",
        "                projection,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(tf.dtypes.cast(\n",
        "\n",
        "      #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
        "      # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked\n",
        "      tf.math.reduce_sum(\n",
        "      inputs,\n",
        "      axis=2,\n",
        "      keepdims=False,\n",
        "      name=None\n",
        "  ), tf.int32))\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      time_steps=time_steps,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      projection=projection,\n",
        "      name='encoder'\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  #We reshape for feeding our FC in the next step\n",
        "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
        "\n",
        "  #We predict our class\n",
        "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True, name=\"outputs\")(outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9H-kkyuLcbPZ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# num_batch_size = 32\n",
        "# num_epochs = 500\n",
        "# N_SPLIT = 10\n",
        "# num_labels=5\n",
        "# num_classes=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Iw3KzecbRt",
        "outputId": "a774d030-ea83-454e-cd2d-247acae02601"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2075, 181)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#ajit\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate5_spectral2.csv')\n",
        "# dm\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm_4featu.csv')\n",
        "\n",
        "\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/melspectogram_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/chroma_cqt_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/mfcc_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/spectral_centroid_meandm_2075_200.csv')\n",
        "\n",
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HrbtbLXcbTx",
        "outputId": "1f447337-3af8-402e-ef64-36e0603d6e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "siz=415\n",
        "df_read = df.copy()\n",
        "df1 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df1.index)\n",
        "df2 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df2.index)\n",
        "df3 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df3.index)\n",
        "df4 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df4.index)\n",
        "df5 = df_read.copy()\n",
        "\n",
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)\n",
        "print(df4.shape)\n",
        "print(df5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j19bgqpcbWT",
        "outputId": "e5cb0300-9002-4488-d6c7-7abefa69ca10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int64Index([ 881,  453, 2004, 1353,  281,  941, 1185, 1159, 1138,  599,\n",
            "            ...\n",
            "             834, 1730,  353, 1345, 1190, 1375,  185,  701, 1671, 1982],\n",
            "           dtype='int64', length=415)\n"
          ]
        }
      ],
      "source": [
        "q = list(df1.index)+list(df2.index)+list(df3.index)+list(df4.index)+list(df5.index)\n",
        "print(df1.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uT0Jrfa5cbZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c15a28-aeba-448a-d4fc-46d37ff1c4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear\n",
            "linear\n",
            "linear\n",
            "Epoch 1/1000\n",
            "52/52 [==============================] - 27s 37ms/step - loss: 3.7866 - val_loss: 2.2360\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 2.2902 - val_loss: 1.9412\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 2.2078 - val_loss: 1.8649\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.9491 - val_loss: 1.7194\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.9177 - val_loss: 1.7137\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.8972 - val_loss: 1.6505\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.8052 - val_loss: 1.6334\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.7603 - val_loss: 1.6539\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.7086 - val_loss: 1.6068\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.6521 - val_loss: 1.6353\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.7053 - val_loss: 1.6355\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.5960 - val_loss: 1.5808\n",
            "Epoch 13/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.6098 - val_loss: 1.5083\n",
            "Epoch 14/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.5827 - val_loss: 1.5307\n",
            "Epoch 15/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.5742 - val_loss: 1.4681\n",
            "Epoch 16/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.5129 - val_loss: 1.4981\n",
            "Epoch 17/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.5618 - val_loss: 1.4908\n",
            "Epoch 18/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.5109 - val_loss: 1.4932\n",
            "Epoch 19/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.4544 - val_loss: 1.4266\n",
            "Epoch 20/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.5105 - val_loss: 1.4324\n",
            "Epoch 21/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.4712 - val_loss: 1.3605\n",
            "Epoch 22/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.5020 - val_loss: 1.4344\n",
            "Epoch 23/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.4680 - val_loss: 1.4023\n",
            "Epoch 24/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.4391 - val_loss: 1.3912\n",
            "Epoch 25/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3957 - val_loss: 1.4937\n",
            "Epoch 26/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.4076 - val_loss: 1.4986\n",
            "Epoch 27/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.4272 - val_loss: 1.3072\n",
            "Epoch 28/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.4284 - val_loss: 1.4274\n",
            "Epoch 29/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3694 - val_loss: 1.2674\n",
            "Epoch 30/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 1.3604 - val_loss: 1.3405\n",
            "Epoch 31/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.4289 - val_loss: 1.3542\n",
            "Epoch 32/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3863 - val_loss: 1.3450\n",
            "Epoch 33/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.4007 - val_loss: 1.4333\n",
            "Epoch 34/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3864 - val_loss: 1.3551\n",
            "Epoch 35/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3821 - val_loss: 1.3147\n",
            "Epoch 36/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3636 - val_loss: 1.3594\n",
            "Epoch 37/1000\n",
            "52/52 [==============================] - 1s 17ms/step - loss: 1.3482 - val_loss: 1.4158\n",
            "Epoch 38/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3432 - val_loss: 1.4573\n",
            "Epoch 39/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.3277 - val_loss: 1.4053\n",
            "Epoch 40/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3199 - val_loss: 1.3127\n",
            "Epoch 41/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.3294 - val_loss: 1.3108\n",
            "Epoch 42/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.2938 - val_loss: 1.3234\n",
            "Epoch 43/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.3115 - val_loss: 1.2628\n",
            "Epoch 44/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.2981 - val_loss: 1.3454\n",
            "Epoch 45/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3272 - val_loss: 1.3412\n",
            "Epoch 46/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.2915 - val_loss: 1.3608\n",
            "Epoch 47/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.3271 - val_loss: 1.2918\n",
            "Epoch 48/1000\n",
            "52/52 [==============================] - 1s 17ms/step - loss: 1.2881 - val_loss: 1.2120\n",
            "Epoch 49/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.2385 - val_loss: 1.4167\n",
            "Epoch 50/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.2680 - val_loss: 1.2790\n",
            "Epoch 51/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.2542 - val_loss: 1.3702\n",
            "Epoch 52/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.2732 - val_loss: 1.2491\n",
            "Epoch 53/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 1.2664 - val_loss: 1.3225\n",
            "Epoch 54/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.1955 - val_loss: 1.2217\n",
            "Epoch 55/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.1702 - val_loss: 1.2094\n",
            "Epoch 56/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.2428 - val_loss: 1.3085\n",
            "Epoch 57/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.2429 - val_loss: 1.2920\n",
            "Epoch 58/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.2256 - val_loss: 1.2714\n",
            "Epoch 59/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.2101 - val_loss: 1.3302\n",
            "Epoch 60/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.2261 - val_loss: 1.2056\n",
            "Epoch 61/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1953 - val_loss: 1.2091\n",
            "Epoch 62/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.2392 - val_loss: 1.3466\n",
            "Epoch 63/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1864 - val_loss: 1.3060\n",
            "Epoch 64/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2011 - val_loss: 1.1382\n",
            "Epoch 65/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1438 - val_loss: 1.2578\n",
            "Epoch 66/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.2278 - val_loss: 1.2276\n",
            "Epoch 67/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 1.1824 - val_loss: 1.1940\n",
            "Epoch 68/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.1827 - val_loss: 1.2286\n",
            "Epoch 69/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1606 - val_loss: 1.2099\n",
            "Epoch 70/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.1909 - val_loss: 1.2438\n",
            "Epoch 71/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.1621 - val_loss: 1.2189\n",
            "Epoch 72/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.2084 - val_loss: 1.2599\n",
            "Epoch 73/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1610 - val_loss: 1.2692\n",
            "Epoch 74/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.1286 - val_loss: 1.1878\n",
            "Epoch 75/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1700 - val_loss: 1.2459\n",
            "Epoch 76/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0842 - val_loss: 1.2435\n",
            "Epoch 77/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1593 - val_loss: 1.1616\n",
            "Epoch 78/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.1370 - val_loss: 1.1055\n",
            "Epoch 79/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1681 - val_loss: 1.0928\n",
            "Epoch 80/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.1277 - val_loss: 1.0838\n",
            "Epoch 81/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1030 - val_loss: 1.3014\n",
            "Epoch 82/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1166 - val_loss: 1.2184\n",
            "Epoch 83/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0905 - val_loss: 1.1604\n",
            "Epoch 84/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1298 - val_loss: 1.1424\n",
            "Epoch 85/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.0758 - val_loss: 1.2090\n",
            "Epoch 86/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1017 - val_loss: 1.1698\n",
            "Epoch 87/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.1093 - val_loss: 1.1259\n",
            "Epoch 88/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0970 - val_loss: 1.0834\n",
            "Epoch 89/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0881 - val_loss: 1.1646\n",
            "Epoch 90/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 1.1134 - val_loss: 1.0880\n",
            "Epoch 91/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.0763 - val_loss: 1.1859\n",
            "Epoch 92/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.0738 - val_loss: 1.1418\n",
            "Epoch 93/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0906 - val_loss: 1.0947\n",
            "Epoch 94/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.1159 - val_loss: 1.1201\n",
            "Epoch 95/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0996 - val_loss: 1.0965\n",
            "Epoch 96/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0677 - val_loss: 1.2240\n",
            "Epoch 97/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0546 - val_loss: 1.0724\n",
            "Epoch 98/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0585 - val_loss: 1.1831\n",
            "Epoch 99/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0823 - val_loss: 1.0388\n",
            "Epoch 100/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 1.0436 - val_loss: 1.0615\n",
            "Epoch 101/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0352 - val_loss: 1.1138\n",
            "Epoch 102/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0654 - val_loss: 1.1362\n",
            "Epoch 103/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0756 - val_loss: 1.1168\n",
            "Epoch 104/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0597 - val_loss: 1.0565\n",
            "Epoch 105/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.0784 - val_loss: 1.0214\n",
            "Epoch 106/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 1.0628 - val_loss: 1.0467\n",
            "Epoch 107/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.0518 - val_loss: 1.0063\n",
            "Epoch 108/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0328 - val_loss: 1.0991\n",
            "Epoch 109/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0265 - val_loss: 1.0309\n",
            "Epoch 110/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0478 - val_loss: 1.0158\n",
            "Epoch 111/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9856 - val_loss: 0.9682\n",
            "Epoch 112/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.0231 - val_loss: 1.1107\n",
            "Epoch 113/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.0337 - val_loss: 0.9904\n",
            "Epoch 114/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 1.0140 - val_loss: 1.0494\n",
            "Epoch 115/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.0067 - val_loss: 1.0128\n",
            "Epoch 116/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0226 - val_loss: 1.1200\n",
            "Epoch 117/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.0160 - val_loss: 1.0121\n",
            "Epoch 118/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9862 - val_loss: 0.9870\n",
            "Epoch 119/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.9517 - val_loss: 1.0420\n",
            "Epoch 120/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0092 - val_loss: 1.0045\n",
            "Epoch 121/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.0039 - val_loss: 1.0028\n",
            "Epoch 122/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9746 - val_loss: 1.0452\n",
            "Epoch 123/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0123 - val_loss: 1.0103\n",
            "Epoch 124/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9791 - val_loss: 0.9810\n",
            "Epoch 125/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9653 - val_loss: 1.0272\n",
            "Epoch 126/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9941 - val_loss: 0.9660\n",
            "Epoch 127/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 1.0165 - val_loss: 0.9995\n",
            "Epoch 128/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9721 - val_loss: 0.9324\n",
            "Epoch 129/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9870 - val_loss: 0.9540\n",
            "Epoch 130/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9607 - val_loss: 0.9336\n",
            "Epoch 131/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9837 - val_loss: 1.0000\n",
            "Epoch 132/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9623 - val_loss: 0.9528\n",
            "Epoch 133/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9751 - val_loss: 0.9666\n",
            "Epoch 134/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 1.0049 - val_loss: 0.9543\n",
            "Epoch 135/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9387 - val_loss: 0.9826\n",
            "Epoch 136/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9949 - val_loss: 1.0337\n",
            "Epoch 137/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.9715 - val_loss: 0.9627\n",
            "Epoch 138/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9652 - val_loss: 0.9432\n",
            "Epoch 139/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9708 - val_loss: 0.9567\n",
            "Epoch 140/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9447 - val_loss: 1.0157\n",
            "Epoch 141/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9949 - val_loss: 0.9887\n",
            "Epoch 142/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9449 - val_loss: 1.0330\n",
            "Epoch 143/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.9686 - val_loss: 0.8914\n",
            "Epoch 144/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9199 - val_loss: 0.9569\n",
            "Epoch 145/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9633 - val_loss: 0.9666\n",
            "Epoch 146/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9309 - val_loss: 1.0005\n",
            "Epoch 147/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9409 - val_loss: 0.9603\n",
            "Epoch 148/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9544 - val_loss: 1.0065\n",
            "Epoch 149/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9281 - val_loss: 0.8887\n",
            "Epoch 150/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9478 - val_loss: 0.8716\n",
            "Epoch 151/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9019 - val_loss: 0.9251\n",
            "Epoch 152/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9091 - val_loss: 0.9239\n",
            "Epoch 153/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9201 - val_loss: 0.9219\n",
            "Epoch 154/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9497 - val_loss: 0.9691\n",
            "Epoch 155/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9283 - val_loss: 0.9511\n",
            "Epoch 156/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9110 - val_loss: 0.9611\n",
            "Epoch 157/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9400 - val_loss: 0.9491\n",
            "Epoch 158/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9000 - val_loss: 0.8620\n",
            "Epoch 159/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9115 - val_loss: 0.8512\n",
            "Epoch 160/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9484 - val_loss: 0.8871\n",
            "Epoch 161/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9407 - val_loss: 0.9343\n",
            "Epoch 162/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9203 - val_loss: 0.9237\n",
            "Epoch 163/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9421 - val_loss: 0.8995\n",
            "Epoch 164/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8794 - val_loss: 0.8821\n",
            "Epoch 165/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8970 - val_loss: 0.8760\n",
            "Epoch 166/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8905 - val_loss: 0.9068\n",
            "Epoch 167/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.9073 - val_loss: 0.9256\n",
            "Epoch 168/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9323 - val_loss: 0.8766\n",
            "Epoch 169/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9264 - val_loss: 0.9025\n",
            "Epoch 170/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8974 - val_loss: 0.8808\n",
            "Epoch 171/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8811 - val_loss: 0.8984\n",
            "Epoch 172/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8801 - val_loss: 0.8584\n",
            "Epoch 173/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.9000 - val_loss: 0.8417\n",
            "Epoch 174/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8863 - val_loss: 0.8702\n",
            "Epoch 175/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8596 - val_loss: 0.8826\n",
            "Epoch 176/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8792 - val_loss: 0.8705\n",
            "Epoch 177/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8733 - val_loss: 0.8817\n",
            "Epoch 178/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8758 - val_loss: 0.8596\n",
            "Epoch 179/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8706 - val_loss: 0.8919\n",
            "Epoch 180/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9047 - val_loss: 0.8640\n",
            "Epoch 181/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.8654 - val_loss: 0.8419\n",
            "Epoch 182/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9241 - val_loss: 0.8473\n",
            "Epoch 183/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8934 - val_loss: 0.8474\n",
            "Epoch 184/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8831 - val_loss: 0.8658\n",
            "Epoch 185/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8486 - val_loss: 0.8183\n",
            "Epoch 186/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8778 - val_loss: 0.9828\n",
            "Epoch 187/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8940 - val_loss: 0.8670\n",
            "Epoch 188/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.8543 - val_loss: 0.8799\n",
            "Epoch 189/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8578 - val_loss: 0.8828\n",
            "Epoch 190/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8887 - val_loss: 0.8902\n",
            "Epoch 191/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8480 - val_loss: 0.8624\n",
            "Epoch 192/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8505 - val_loss: 0.7756\n",
            "Epoch 193/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8576 - val_loss: 0.8320\n",
            "Epoch 194/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8548 - val_loss: 0.7801\n",
            "Epoch 195/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8577 - val_loss: 0.8024\n",
            "Epoch 196/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8242 - val_loss: 0.8788\n",
            "Epoch 197/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8525 - val_loss: 0.8283\n",
            "Epoch 198/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8341 - val_loss: 0.8107\n",
            "Epoch 199/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8546 - val_loss: 0.8159\n",
            "Epoch 200/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8660 - val_loss: 0.8199\n",
            "Epoch 201/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8382 - val_loss: 0.8015\n",
            "Epoch 202/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8867 - val_loss: 0.8487\n",
            "Epoch 203/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8417 - val_loss: 0.8906\n",
            "Epoch 204/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8482 - val_loss: 0.8456\n",
            "Epoch 205/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8373 - val_loss: 0.8163\n",
            "Epoch 206/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8331 - val_loss: 0.8318\n",
            "Epoch 207/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8438 - val_loss: 0.8363\n",
            "Epoch 208/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8750 - val_loss: 0.7999\n",
            "Epoch 209/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8437 - val_loss: 0.8085\n",
            "Epoch 210/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8377 - val_loss: 0.7991\n",
            "Epoch 211/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8215 - val_loss: 0.7953\n",
            "Epoch 212/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8232 - val_loss: 0.7696\n",
            "Epoch 213/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8368 - val_loss: 0.8175\n",
            "Epoch 214/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8431 - val_loss: 0.8225\n",
            "Epoch 215/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8127 - val_loss: 0.8149\n",
            "Epoch 216/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8404 - val_loss: 0.7950\n",
            "Epoch 217/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8364 - val_loss: 0.7998\n",
            "Epoch 218/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8275 - val_loss: 0.7827\n",
            "Epoch 219/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8113 - val_loss: 0.7847\n",
            "Epoch 220/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7942 - val_loss: 0.7704\n",
            "Epoch 221/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8525 - val_loss: 0.8077\n",
            "Epoch 222/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8033 - val_loss: 0.7765\n",
            "Epoch 223/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8237 - val_loss: 0.8407\n",
            "Epoch 224/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8269 - val_loss: 0.7821\n",
            "Epoch 225/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8098 - val_loss: 0.7704\n",
            "Epoch 226/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8044 - val_loss: 0.8086\n",
            "Epoch 227/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8524 - val_loss: 0.8220\n",
            "Epoch 228/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7928 - val_loss: 0.7928\n",
            "Epoch 229/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8302 - val_loss: 0.7763\n",
            "Epoch 230/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8031 - val_loss: 0.8216\n",
            "Epoch 231/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8044 - val_loss: 0.8234\n",
            "Epoch 232/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7875 - val_loss: 0.7871\n",
            "Epoch 233/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8402 - val_loss: 0.7932\n",
            "Epoch 234/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8111 - val_loss: 0.7954\n",
            "Epoch 235/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8085 - val_loss: 0.7683\n",
            "Epoch 236/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7959 - val_loss: 0.8160\n",
            "Epoch 237/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8461 - val_loss: 0.8114\n",
            "Epoch 238/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7908 - val_loss: 0.7974\n",
            "Epoch 239/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8060 - val_loss: 0.8284\n",
            "Epoch 240/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8033 - val_loss: 0.8082\n",
            "Epoch 241/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8110 - val_loss: 0.7780\n",
            "Epoch 242/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8112 - val_loss: 0.8054\n",
            "Epoch 243/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7890 - val_loss: 0.8145\n",
            "Epoch 244/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8022 - val_loss: 0.7914\n",
            "Epoch 245/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7711 - val_loss: 0.7716\n",
            "Epoch 246/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8047 - val_loss: 0.7374\n",
            "Epoch 247/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7955 - val_loss: 0.7777\n",
            "Epoch 248/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7798 - val_loss: 0.7849\n",
            "Epoch 249/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.8033 - val_loss: 0.7624\n",
            "Epoch 250/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7960 - val_loss: 0.8124\n",
            "Epoch 251/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7893 - val_loss: 0.8126\n",
            "Epoch 252/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.8102 - val_loss: 0.7621\n",
            "Epoch 253/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7908 - val_loss: 0.7363\n",
            "Epoch 254/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7836 - val_loss: 0.7584\n",
            "Epoch 255/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7970 - val_loss: 0.8216\n",
            "Epoch 256/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7750 - val_loss: 0.7599\n",
            "Epoch 257/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.8077 - val_loss: 0.8040\n",
            "Epoch 258/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8007 - val_loss: 0.8468\n",
            "Epoch 259/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.8051 - val_loss: 0.7556\n",
            "Epoch 260/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7709 - val_loss: 0.7478\n",
            "Epoch 261/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7556 - val_loss: 0.7787\n",
            "Epoch 262/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7976 - val_loss: 0.7545\n",
            "Epoch 263/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7851 - val_loss: 0.7951\n",
            "Epoch 264/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7841 - val_loss: 0.7943\n",
            "Epoch 265/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7793 - val_loss: 0.7737\n",
            "Epoch 266/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7546 - val_loss: 0.7557\n",
            "Epoch 267/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7852 - val_loss: 0.7848\n",
            "Epoch 268/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7707 - val_loss: 0.7547\n",
            "Epoch 269/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7616 - val_loss: 0.7441\n",
            "Epoch 270/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7852 - val_loss: 0.7673\n",
            "Epoch 271/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7744 - val_loss: 0.7422\n",
            "Epoch 272/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7517 - val_loss: 0.7656\n",
            "Epoch 273/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7685 - val_loss: 0.7631\n",
            "Epoch 274/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7617 - val_loss: 0.7187\n",
            "Epoch 275/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7918 - val_loss: 0.7954\n",
            "Epoch 276/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7657 - val_loss: 0.7524\n",
            "Epoch 277/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7772 - val_loss: 0.7746\n",
            "Epoch 278/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7761 - val_loss: 0.7439\n",
            "Epoch 279/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7823 - val_loss: 0.7429\n",
            "Epoch 280/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7483 - val_loss: 0.7704\n",
            "Epoch 281/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7771 - val_loss: 0.7299\n",
            "Epoch 282/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7721 - val_loss: 0.8028\n",
            "Epoch 283/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.7834 - val_loss: 0.7533\n",
            "Epoch 284/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7821 - val_loss: 0.7043\n",
            "Epoch 285/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7724 - val_loss: 0.7422\n",
            "Epoch 286/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7636 - val_loss: 0.7686\n",
            "Epoch 287/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7495 - val_loss: 0.7584\n",
            "Epoch 288/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7541 - val_loss: 0.7260\n",
            "Epoch 289/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7603 - val_loss: 0.7413\n",
            "Epoch 290/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7618 - val_loss: 0.7393\n",
            "Epoch 291/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7717 - val_loss: 0.7374\n",
            "Epoch 292/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7564 - val_loss: 0.7288\n",
            "Epoch 293/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7555 - val_loss: 0.7578\n",
            "Epoch 294/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7587 - val_loss: 0.8063\n",
            "Epoch 295/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7437 - val_loss: 0.7628\n",
            "Epoch 296/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7688 - val_loss: 0.7715\n",
            "Epoch 297/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7622 - val_loss: 0.7398\n",
            "Epoch 298/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7500 - val_loss: 0.7876\n",
            "Epoch 299/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7601 - val_loss: 0.7717\n",
            "Epoch 300/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7856 - val_loss: 0.7679\n",
            "Epoch 301/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7380 - val_loss: 0.7324\n",
            "Epoch 302/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7450 - val_loss: 0.7428\n",
            "Epoch 303/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7422 - val_loss: 0.7429\n",
            "Epoch 304/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7364 - val_loss: 0.7935\n",
            "Epoch 305/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7216 - val_loss: 0.7076\n",
            "Epoch 306/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7399 - val_loss: 0.7123\n",
            "Epoch 307/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7462 - val_loss: 0.7660\n",
            "Epoch 308/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7489 - val_loss: 0.7485\n",
            "Epoch 309/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7508 - val_loss: 0.7819\n",
            "Epoch 310/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7257 - val_loss: 0.7385\n",
            "Epoch 311/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7187 - val_loss: 0.7273\n",
            "Epoch 312/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7337 - val_loss: 0.7216\n",
            "Epoch 313/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7595 - val_loss: 0.7374\n",
            "Epoch 314/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7663 - val_loss: 0.7133\n",
            "Epoch 315/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7508 - val_loss: 0.7811\n",
            "Epoch 316/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7386 - val_loss: 0.7561\n",
            "Epoch 317/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7422 - val_loss: 0.7123\n",
            "Epoch 318/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7294 - val_loss: 0.7798\n",
            "Epoch 319/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7272 - val_loss: 0.7284\n",
            "Epoch 320/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7554 - val_loss: 0.7337\n",
            "Epoch 321/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.7331 - val_loss: 0.7224\n",
            "Epoch 322/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7343 - val_loss: 0.7139\n",
            "Epoch 323/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7258 - val_loss: 0.7437\n",
            "Epoch 324/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7296 - val_loss: 0.6971\n",
            "Epoch 325/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7209 - val_loss: 0.7698\n",
            "Epoch 326/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7398 - val_loss: 0.7480\n",
            "Epoch 327/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7420 - val_loss: 0.7039\n",
            "Epoch 328/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7205 - val_loss: 0.7107\n",
            "Epoch 329/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7283 - val_loss: 0.6894\n",
            "Epoch 330/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7625 - val_loss: 0.7260\n",
            "Epoch 331/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7386 - val_loss: 0.7131\n",
            "Epoch 332/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7312 - val_loss: 0.7664\n",
            "Epoch 333/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7397 - val_loss: 0.7473\n",
            "Epoch 334/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.7081 - val_loss: 0.7297\n",
            "Epoch 335/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7274 - val_loss: 0.7605\n",
            "Epoch 336/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7368 - val_loss: 0.7250\n",
            "Epoch 337/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7225 - val_loss: 0.7189\n",
            "Epoch 338/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7374 - val_loss: 0.7204\n",
            "Epoch 339/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7044 - val_loss: 0.7160\n",
            "Epoch 340/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6917 - val_loss: 0.7252\n",
            "Epoch 341/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6983 - val_loss: 0.7144\n",
            "Epoch 342/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7372 - val_loss: 0.7238\n",
            "Epoch 343/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7001 - val_loss: 0.7425\n",
            "Epoch 344/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6929 - val_loss: 0.7549\n",
            "Epoch 345/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6950 - val_loss: 0.7030\n",
            "Epoch 346/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7111 - val_loss: 0.7031\n",
            "Epoch 347/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6986 - val_loss: 0.7400\n",
            "Epoch 348/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7389 - val_loss: 0.6983\n",
            "Epoch 349/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7210 - val_loss: 0.7325\n",
            "Epoch 350/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6878 - val_loss: 0.7268\n",
            "Epoch 351/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7033 - val_loss: 0.7115\n",
            "Epoch 352/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7276 - val_loss: 0.7316\n",
            "Epoch 353/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7035 - val_loss: 0.7338\n",
            "Epoch 354/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7021 - val_loss: 0.7382\n",
            "Epoch 355/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7027 - val_loss: 0.7601\n",
            "Epoch 356/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6938 - val_loss: 0.7008\n",
            "Epoch 357/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.7106 - val_loss: 0.7247\n",
            "Epoch 358/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7165 - val_loss: 0.7047\n",
            "Epoch 359/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7168 - val_loss: 0.7620\n",
            "Epoch 360/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.7084 - val_loss: 0.7288\n",
            "Epoch 361/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6891 - val_loss: 0.7425\n",
            "Epoch 362/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6998 - val_loss: 0.7292\n",
            "Epoch 363/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6884 - val_loss: 0.7268\n",
            "Epoch 364/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6935 - val_loss: 0.7240\n",
            "Epoch 365/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7240 - val_loss: 0.7153\n",
            "Epoch 366/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6995 - val_loss: 0.7192\n",
            "Epoch 367/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7153 - val_loss: 0.6952\n",
            "Epoch 368/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6773 - val_loss: 0.7164\n",
            "Epoch 369/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7099 - val_loss: 0.6932\n",
            "Epoch 370/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7115 - val_loss: 0.7085\n",
            "Epoch 371/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6877 - val_loss: 0.6948\n",
            "Epoch 372/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6753 - val_loss: 0.7270\n",
            "Epoch 373/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6813 - val_loss: 0.7443\n",
            "Epoch 374/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7046 - val_loss: 0.6947\n",
            "Epoch 375/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7062 - val_loss: 0.7368\n",
            "Epoch 376/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.6845 - val_loss: 0.7294\n",
            "Epoch 377/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6918 - val_loss: 0.7261\n",
            "Epoch 378/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7122 - val_loss: 0.7100\n",
            "Epoch 379/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6734 - val_loss: 0.7160\n",
            "Epoch 380/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6800 - val_loss: 0.7184\n",
            "Epoch 381/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6937 - val_loss: 0.7300\n",
            "Epoch 382/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6937 - val_loss: 0.7054\n",
            "Epoch 383/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6890 - val_loss: 0.7352\n",
            "Epoch 384/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6852 - val_loss: 0.7273\n",
            "Epoch 385/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6838 - val_loss: 0.7019\n",
            "Epoch 386/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6738 - val_loss: 0.6957\n",
            "Epoch 387/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6682 - val_loss: 0.6934\n",
            "Epoch 388/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6726 - val_loss: 0.7080\n",
            "Epoch 389/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6817 - val_loss: 0.6898\n",
            "Epoch 390/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6695 - val_loss: 0.6991\n",
            "Epoch 391/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.6856 - val_loss: 0.7931\n",
            "Epoch 392/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.7073 - val_loss: 0.6917\n",
            "Epoch 393/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6945 - val_loss: 0.7324\n",
            "Epoch 394/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6849 - val_loss: 0.7219\n",
            "Epoch 395/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6832 - val_loss: 0.7207\n",
            "Epoch 396/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6766 - val_loss: 0.7106\n",
            "Epoch 397/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6715 - val_loss: 0.7066\n",
            "Epoch 398/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6765 - val_loss: 0.7224\n",
            "Epoch 399/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6667 - val_loss: 0.6778\n",
            "Epoch 400/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6672 - val_loss: 0.7233\n",
            "Epoch 401/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6900 - val_loss: 0.6940\n",
            "Epoch 402/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6713 - val_loss: 0.7284\n",
            "Epoch 403/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6788 - val_loss: 0.6939\n",
            "Epoch 404/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6527 - val_loss: 0.6908\n",
            "Epoch 405/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6832 - val_loss: 0.7671\n",
            "Epoch 406/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6613 - val_loss: 0.7189\n",
            "Epoch 407/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6866 - val_loss: 0.7623\n",
            "Epoch 408/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6693 - val_loss: 0.7189\n",
            "Epoch 409/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6802 - val_loss: 0.7445\n",
            "Epoch 410/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6709 - val_loss: 0.7158\n",
            "Epoch 411/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.6579 - val_loss: 0.7132\n",
            "Epoch 412/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6867 - val_loss: 0.7002\n",
            "Epoch 413/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6796 - val_loss: 0.7030\n",
            "Epoch 414/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6764 - val_loss: 0.7211\n",
            "Epoch 415/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.7063 - val_loss: 0.7208\n",
            "Epoch 416/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6538 - val_loss: 0.7001\n",
            "Epoch 417/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6635 - val_loss: 0.7213\n",
            "Epoch 418/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6623 - val_loss: 0.7017\n",
            "Epoch 419/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6738 - val_loss: 0.7174\n",
            "Epoch 420/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6939 - val_loss: 0.6955\n",
            "Epoch 421/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6872 - val_loss: 0.6719\n",
            "Epoch 422/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6687 - val_loss: 0.6707\n",
            "Epoch 423/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6713 - val_loss: 0.7103\n",
            "Epoch 424/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.6500 - val_loss: 0.6926\n",
            "Epoch 425/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6525 - val_loss: 0.7257\n",
            "Epoch 426/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6550 - val_loss: 0.7013\n",
            "Epoch 427/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6552 - val_loss: 0.7135\n",
            "Epoch 428/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6654 - val_loss: 0.7168\n",
            "Epoch 429/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6722 - val_loss: 0.6871\n",
            "Epoch 430/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6848 - val_loss: 0.6949\n",
            "Epoch 431/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6665 - val_loss: 0.7056\n",
            "Epoch 432/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6447 - val_loss: 0.6971\n",
            "Epoch 433/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6718 - val_loss: 0.6719\n",
            "Epoch 434/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6661 - val_loss: 0.6934\n",
            "Epoch 435/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6774 - val_loss: 0.7150\n",
            "Epoch 436/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6567 - val_loss: 0.7106\n",
            "Epoch 437/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6761 - val_loss: 0.7069\n",
            "Epoch 438/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6865 - val_loss: 0.7269\n",
            "Epoch 439/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6561 - val_loss: 0.6934\n",
            "Epoch 440/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.6319 - val_loss: 0.7174\n",
            "Epoch 441/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6728 - val_loss: 0.6945\n",
            "Epoch 442/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6568 - val_loss: 0.6936\n",
            "Epoch 443/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6480 - val_loss: 0.6687\n",
            "Epoch 444/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6674 - val_loss: 0.6977\n",
            "Epoch 445/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6443 - val_loss: 0.7102\n",
            "Epoch 446/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6511 - val_loss: 0.6869\n",
            "Epoch 447/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6284 - val_loss: 0.6985\n",
            "Epoch 448/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6614 - val_loss: 0.7278\n",
            "Epoch 449/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6518 - val_loss: 0.7135\n",
            "Epoch 450/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6544 - val_loss: 0.7160\n",
            "Epoch 451/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6543 - val_loss: 0.6879\n",
            "Epoch 452/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6599 - val_loss: 0.6805\n",
            "Epoch 453/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6450 - val_loss: 0.7012\n",
            "Epoch 454/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6369 - val_loss: 0.7217\n",
            "Epoch 455/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6555 - val_loss: 0.6828\n",
            "Epoch 456/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6584 - val_loss: 0.6856\n",
            "Epoch 457/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6431 - val_loss: 0.6723\n",
            "Epoch 458/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6566 - val_loss: 0.7010\n",
            "Epoch 459/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6407 - val_loss: 0.7138\n",
            "Epoch 460/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6556 - val_loss: 0.7257\n",
            "Epoch 461/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6197 - val_loss: 0.7267\n",
            "Epoch 462/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6684 - val_loss: 0.6972\n",
            "Epoch 463/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6420 - val_loss: 0.6650\n",
            "Epoch 464/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6526 - val_loss: 0.6889\n",
            "Epoch 465/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6293 - val_loss: 0.6846\n",
            "Epoch 466/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6484 - val_loss: 0.6949\n",
            "Epoch 467/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6471 - val_loss: 0.7067\n",
            "Epoch 468/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6226 - val_loss: 0.7028\n",
            "Epoch 469/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6355 - val_loss: 0.6851\n",
            "Epoch 470/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6271 - val_loss: 0.7097\n",
            "Epoch 471/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6358 - val_loss: 0.7144\n",
            "Epoch 472/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6222 - val_loss: 0.6978\n",
            "Epoch 473/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6563 - val_loss: 0.6831\n",
            "Epoch 474/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6342 - val_loss: 0.6711\n",
            "Epoch 475/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.6496 - val_loss: 0.6744\n",
            "Epoch 476/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6393 - val_loss: 0.6689\n",
            "Epoch 477/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6403 - val_loss: 0.6791\n",
            "Epoch 478/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6268 - val_loss: 0.6858\n",
            "Epoch 479/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6354 - val_loss: 0.7102\n",
            "Epoch 480/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6431 - val_loss: 0.6902\n",
            "Epoch 481/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6322 - val_loss: 0.6847\n",
            "Epoch 482/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6538 - val_loss: 0.6618\n",
            "Epoch 483/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6402 - val_loss: 0.7044\n",
            "Epoch 484/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6256 - val_loss: 0.6922\n",
            "Epoch 485/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6400 - val_loss: 0.6863\n",
            "Epoch 486/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6310 - val_loss: 0.7108\n",
            "Epoch 487/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6288 - val_loss: 0.6739\n",
            "Epoch 488/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.6375 - val_loss: 0.6772\n",
            "Epoch 489/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6418 - val_loss: 0.7198\n",
            "Epoch 490/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6368 - val_loss: 0.6710\n",
            "Epoch 491/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6415 - val_loss: 0.6795\n",
            "Epoch 492/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.6245 - val_loss: 0.6641\n",
            "Epoch 493/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6377 - val_loss: 0.7010\n",
            "Epoch 494/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6340 - val_loss: 0.6759\n",
            "Epoch 495/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6329 - val_loss: 0.6892\n",
            "Epoch 496/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6394 - val_loss: 0.6770\n",
            "Epoch 497/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6367 - val_loss: 0.6577\n",
            "Epoch 498/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6255 - val_loss: 0.6748\n",
            "Epoch 499/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6166 - val_loss: 0.6834\n",
            "Epoch 500/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6232 - val_loss: 0.6876\n",
            "Epoch 501/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6262 - val_loss: 0.6822\n",
            "Epoch 502/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6394 - val_loss: 0.6562\n",
            "Epoch 503/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6270 - val_loss: 0.6848\n",
            "Epoch 504/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.6279 - val_loss: 0.6871\n",
            "Epoch 505/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6200 - val_loss: 0.7067\n",
            "Epoch 506/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6064 - val_loss: 0.7050\n",
            "Epoch 507/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6097 - val_loss: 0.6759\n",
            "Epoch 508/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6193 - val_loss: 0.7173\n",
            "Epoch 509/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6314 - val_loss: 0.6758\n",
            "Epoch 510/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6307 - val_loss: 0.6768\n",
            "Epoch 511/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6265 - val_loss: 0.6857\n",
            "Epoch 512/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6003 - val_loss: 0.6871\n",
            "Epoch 513/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6242 - val_loss: 0.6727\n",
            "Epoch 514/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6021 - val_loss: 0.7043\n",
            "Epoch 515/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6263 - val_loss: 0.6910\n",
            "Epoch 516/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6281 - val_loss: 0.7126\n",
            "Epoch 517/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6103 - val_loss: 0.6608\n",
            "Epoch 518/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6171 - val_loss: 0.6786\n",
            "Epoch 519/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5841 - val_loss: 0.6790\n",
            "Epoch 520/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6293 - val_loss: 0.6706\n",
            "Epoch 521/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6331 - val_loss: 0.6853\n",
            "Epoch 522/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6232 - val_loss: 0.6725\n",
            "Epoch 523/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6135 - val_loss: 0.6667\n",
            "Epoch 524/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6326 - val_loss: 0.6714\n",
            "Epoch 525/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6132 - val_loss: 0.6935\n",
            "Epoch 526/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.6024 - val_loss: 0.6862\n",
            "Epoch 527/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6401 - val_loss: 0.6799\n",
            "Epoch 528/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6161 - val_loss: 0.6801\n",
            "Epoch 529/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6310 - val_loss: 0.6859\n",
            "Epoch 530/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6248 - val_loss: 0.7034\n",
            "Epoch 531/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5876 - val_loss: 0.6686\n",
            "Epoch 532/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6121 - val_loss: 0.6484\n",
            "Epoch 533/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.6194 - val_loss: 0.6581\n",
            "Epoch 534/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6103 - val_loss: 0.6700\n",
            "Epoch 535/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6184 - val_loss: 0.6731\n",
            "Epoch 536/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5969 - val_loss: 0.6655\n",
            "Epoch 537/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6193 - val_loss: 0.7180\n",
            "Epoch 538/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6213 - val_loss: 0.6967\n",
            "Epoch 539/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6137 - val_loss: 0.6684\n",
            "Epoch 540/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6307 - val_loss: 0.6713\n",
            "Epoch 541/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5905 - val_loss: 0.6855\n",
            "Epoch 542/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.6042 - val_loss: 0.6782\n",
            "Epoch 543/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5829 - val_loss: 0.7041\n",
            "Epoch 544/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.6095 - val_loss: 0.6740\n",
            "Epoch 545/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5999 - val_loss: 0.6726\n",
            "Epoch 546/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6120 - val_loss: 0.6788\n",
            "Epoch 547/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5862 - val_loss: 0.6914\n",
            "Epoch 548/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5875 - val_loss: 0.6975\n",
            "Epoch 549/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6042 - val_loss: 0.6799\n",
            "Epoch 550/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6034 - val_loss: 0.6796\n",
            "Epoch 551/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6113 - val_loss: 0.6970\n",
            "Epoch 552/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5891 - val_loss: 0.6922\n",
            "Epoch 553/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6018 - val_loss: 0.6658\n",
            "Epoch 554/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6153 - val_loss: 0.6840\n",
            "Epoch 555/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5806 - val_loss: 0.6956\n",
            "Epoch 556/1000\n",
            "52/52 [==============================] - 1s 18ms/step - loss: 0.5953 - val_loss: 0.7043\n",
            "Epoch 557/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5967 - val_loss: 0.6785\n",
            "Epoch 558/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.6106 - val_loss: 0.6857\n",
            "Epoch 559/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5829 - val_loss: 0.6990\n",
            "Epoch 560/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5941 - val_loss: 0.7012\n",
            "Epoch 561/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5950 - val_loss: 0.6917\n",
            "Epoch 562/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5915 - val_loss: 0.6617\n",
            "Epoch 563/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5975 - val_loss: 0.6684\n",
            "Epoch 564/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.6023 - val_loss: 0.7003\n",
            "Epoch 565/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5919 - val_loss: 0.6687\n",
            "Epoch 566/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5797 - val_loss: 0.6972\n",
            "Epoch 567/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5992 - val_loss: 0.6891\n",
            "Epoch 568/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5959 - val_loss: 0.6829\n",
            "Epoch 569/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6025 - val_loss: 0.6827\n",
            "Epoch 570/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5968 - val_loss: 0.6491\n",
            "Epoch 571/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5967 - val_loss: 0.6697\n",
            "Epoch 572/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5980 - val_loss: 0.6769\n",
            "Epoch 573/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5814 - val_loss: 0.6684\n",
            "Epoch 574/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6004 - val_loss: 0.6633\n",
            "Epoch 575/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5903 - val_loss: 0.6772\n",
            "Epoch 576/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5930 - val_loss: 0.6665\n",
            "Epoch 577/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6040 - val_loss: 0.6595\n",
            "Epoch 578/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5997 - val_loss: 0.6663\n",
            "Epoch 579/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.5996 - val_loss: 0.6482\n",
            "Epoch 580/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5847 - val_loss: 0.6970\n",
            "Epoch 581/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6136 - val_loss: 0.7001\n",
            "Epoch 582/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5786 - val_loss: 0.6810\n",
            "Epoch 583/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5740 - val_loss: 0.6878\n",
            "Epoch 584/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5884 - val_loss: 0.6742\n",
            "Epoch 585/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5883 - val_loss: 0.6663\n",
            "Epoch 586/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5892 - val_loss: 0.6927\n",
            "Epoch 587/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.6123 - val_loss: 0.6636\n",
            "Epoch 588/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5806 - val_loss: 0.6784\n",
            "Epoch 589/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5950 - val_loss: 0.6654\n",
            "Epoch 590/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5911 - val_loss: 0.6647\n",
            "Epoch 591/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6165 - val_loss: 0.6971\n",
            "Epoch 592/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5878 - val_loss: 0.6623\n",
            "Epoch 593/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6060 - val_loss: 0.6875\n",
            "Epoch 594/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5819 - val_loss: 0.6658\n",
            "Epoch 595/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5842 - val_loss: 0.6568\n",
            "Epoch 596/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5779 - val_loss: 0.6725\n",
            "Epoch 597/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5904 - val_loss: 0.6775\n",
            "Epoch 598/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5838 - val_loss: 0.6765\n",
            "Epoch 599/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5760 - val_loss: 0.6771\n",
            "Epoch 600/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5966 - val_loss: 0.7041\n",
            "Epoch 601/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5786 - val_loss: 0.6687\n",
            "Epoch 602/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5745 - val_loss: 0.6729\n",
            "Epoch 603/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5981 - val_loss: 0.6877\n",
            "Epoch 604/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5755 - val_loss: 0.6689\n",
            "Epoch 605/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6050 - val_loss: 0.6705\n",
            "Epoch 606/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5742 - val_loss: 0.6856\n",
            "Epoch 607/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5822 - val_loss: 0.6572\n",
            "Epoch 608/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5983 - val_loss: 0.6565\n",
            "Epoch 609/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.6032 - val_loss: 0.6751\n",
            "Epoch 610/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5856 - val_loss: 0.6862\n",
            "Epoch 611/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5758 - val_loss: 0.6625\n",
            "Epoch 612/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5800 - val_loss: 0.6659\n",
            "Epoch 613/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5692 - val_loss: 0.6546\n",
            "Epoch 614/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5681 - val_loss: 0.6789\n",
            "Epoch 615/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5906 - val_loss: 0.6768\n",
            "Epoch 616/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5656 - val_loss: 0.6707\n",
            "Epoch 617/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5811 - val_loss: 0.6632\n",
            "Epoch 618/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5826 - val_loss: 0.6645\n",
            "Epoch 619/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5511 - val_loss: 0.6758\n",
            "Epoch 620/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5836 - val_loss: 0.6847\n",
            "Epoch 621/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5562 - val_loss: 0.6816\n",
            "Epoch 622/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5732 - val_loss: 0.6798\n",
            "Epoch 623/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5748 - val_loss: 0.6587\n",
            "Epoch 624/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5794 - val_loss: 0.6850\n",
            "Epoch 625/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5945 - val_loss: 0.6728\n",
            "Epoch 626/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5668 - val_loss: 0.6614\n",
            "Epoch 627/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5573 - val_loss: 0.6559\n",
            "Epoch 628/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5908 - val_loss: 0.6890\n",
            "Epoch 629/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5770 - val_loss: 0.6532\n",
            "Epoch 630/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5677 - val_loss: 0.6688\n",
            "Epoch 631/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5721 - val_loss: 0.6737\n",
            "Epoch 632/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5795 - val_loss: 0.6641\n",
            "Epoch 633/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5653 - val_loss: 0.6731\n",
            "Epoch 634/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5761 - val_loss: 0.7156\n",
            "Epoch 635/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5950 - val_loss: 0.6502\n",
            "Epoch 636/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5717 - val_loss: 0.6642\n",
            "Epoch 637/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5478 - val_loss: 0.6598\n",
            "Epoch 638/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5857 - val_loss: 0.6685\n",
            "Epoch 639/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5789 - val_loss: 0.6703\n",
            "Epoch 640/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5760 - val_loss: 0.6806\n",
            "Epoch 641/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5596 - val_loss: 0.6843\n",
            "Epoch 642/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5616 - val_loss: 0.6584\n",
            "Epoch 643/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.5530 - val_loss: 0.7226\n",
            "Epoch 644/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5696 - val_loss: 0.6912\n",
            "Epoch 645/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5618 - val_loss: 0.6757\n",
            "Epoch 646/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5561 - val_loss: 0.6814\n",
            "Epoch 647/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5583 - val_loss: 0.6807\n",
            "Epoch 648/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5811 - val_loss: 0.7035\n",
            "Epoch 649/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5573 - val_loss: 0.6808\n",
            "Epoch 650/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5568 - val_loss: 0.6685\n",
            "Epoch 651/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5859 - val_loss: 0.6839\n",
            "Epoch 652/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5759 - val_loss: 0.6726\n",
            "Epoch 653/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5726 - val_loss: 0.6741\n",
            "Epoch 654/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5543 - val_loss: 0.6659\n",
            "Epoch 655/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5724 - val_loss: 0.6808\n",
            "Epoch 656/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5623 - val_loss: 0.6835\n",
            "Epoch 657/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5627 - val_loss: 0.6678\n",
            "Epoch 658/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5584 - val_loss: 0.6737\n",
            "Epoch 659/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5584 - val_loss: 0.6827\n",
            "Epoch 660/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5341 - val_loss: 0.6866\n",
            "Epoch 661/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5672 - val_loss: 0.6714\n",
            "Epoch 662/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5511 - val_loss: 0.6872\n",
            "Epoch 663/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5588 - val_loss: 0.6833\n",
            "Epoch 664/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5608 - val_loss: 0.6619\n",
            "Epoch 665/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5480 - val_loss: 0.6780\n",
            "Epoch 666/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5493 - val_loss: 0.6695\n",
            "Epoch 667/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5588 - val_loss: 0.6763\n",
            "Epoch 668/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5858 - val_loss: 0.6607\n",
            "Epoch 669/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5656 - val_loss: 0.6709\n",
            "Epoch 670/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5249 - val_loss: 0.6856\n",
            "Epoch 671/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5472 - val_loss: 0.6920\n",
            "Epoch 672/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5545 - val_loss: 0.6784\n",
            "Epoch 673/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5576 - val_loss: 0.6679\n",
            "Epoch 674/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5755 - val_loss: 0.6855\n",
            "Epoch 675/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5426 - val_loss: 0.6731\n",
            "Epoch 676/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5477 - val_loss: 0.6624\n",
            "Epoch 677/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5383 - val_loss: 0.6560\n",
            "Epoch 678/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5486 - val_loss: 0.6780\n",
            "Epoch 679/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5602 - val_loss: 0.6711\n",
            "Epoch 680/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5613 - val_loss: 0.6698\n",
            "Epoch 681/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5570 - val_loss: 0.6614\n",
            "Epoch 682/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5671 - val_loss: 0.6695\n",
            "Epoch 683/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5579 - val_loss: 0.6797\n",
            "Epoch 684/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5473 - val_loss: 0.6690\n",
            "Epoch 685/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5729 - val_loss: 0.6786\n",
            "Epoch 686/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5499 - val_loss: 0.6858\n",
            "Epoch 687/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5683 - val_loss: 0.6932\n",
            "Epoch 688/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5619 - val_loss: 0.6701\n",
            "Epoch 689/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5661 - val_loss: 0.6687\n",
            "Epoch 690/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5430 - val_loss: 0.6577\n",
            "Epoch 691/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5504 - val_loss: 0.6574\n",
            "Epoch 692/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5581 - val_loss: 0.6728\n",
            "Epoch 693/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5574 - val_loss: 0.6717\n",
            "Epoch 694/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5633 - val_loss: 0.6776\n",
            "Epoch 695/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5474 - val_loss: 0.6679\n",
            "Epoch 696/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5419 - val_loss: 0.6563\n",
            "Epoch 697/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5601 - val_loss: 0.6799\n",
            "Epoch 698/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5415 - val_loss: 0.6770\n",
            "Epoch 699/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5504 - val_loss: 0.6624\n",
            "Epoch 700/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5357 - val_loss: 0.6630\n",
            "Epoch 701/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5611 - val_loss: 0.6712\n",
            "Epoch 702/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5517 - val_loss: 0.6803\n",
            "Epoch 703/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5573 - val_loss: 0.6795\n",
            "Epoch 704/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5599 - val_loss: 0.6883\n",
            "Epoch 705/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5340 - val_loss: 0.6708\n",
            "Epoch 706/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5515 - val_loss: 0.6925\n",
            "Epoch 707/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5661 - val_loss: 0.6626\n",
            "Epoch 708/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5387 - val_loss: 0.6568\n",
            "Epoch 709/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5409 - val_loss: 0.6610\n",
            "Epoch 710/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5430 - val_loss: 0.6465\n",
            "Epoch 711/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5521 - val_loss: 0.7046\n",
            "Epoch 712/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5498 - val_loss: 0.6607\n",
            "Epoch 713/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5389 - val_loss: 0.6605\n",
            "Epoch 714/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5588 - val_loss: 0.6859\n",
            "Epoch 715/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5395 - val_loss: 0.6823\n",
            "Epoch 716/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5375 - val_loss: 0.6680\n",
            "Epoch 717/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5273 - val_loss: 0.6771\n",
            "Epoch 718/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5481 - val_loss: 0.6601\n",
            "Epoch 719/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5658 - val_loss: 0.6587\n",
            "Epoch 720/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5311 - val_loss: 0.7107\n",
            "Epoch 721/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5381 - val_loss: 0.6584\n",
            "Epoch 722/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5291 - val_loss: 0.6600\n",
            "Epoch 723/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5412 - val_loss: 0.6669\n",
            "Epoch 724/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5489 - val_loss: 0.6583\n",
            "Epoch 725/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5359 - val_loss: 0.6492\n",
            "Epoch 726/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5343 - val_loss: 0.6871\n",
            "Epoch 727/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5485 - val_loss: 0.6513\n",
            "Epoch 728/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5423 - val_loss: 0.6653\n",
            "Epoch 729/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5334 - val_loss: 0.6629\n",
            "Epoch 730/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5232 - val_loss: 0.6846\n",
            "Epoch 731/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5288 - val_loss: 0.6793\n",
            "Epoch 732/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5244 - val_loss: 0.6552\n",
            "Epoch 733/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5483 - val_loss: 0.6538\n",
            "Epoch 734/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5214 - val_loss: 0.6540\n",
            "Epoch 735/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5213 - val_loss: 0.6672\n",
            "Epoch 736/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5268 - val_loss: 0.6732\n",
            "Epoch 737/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5339 - val_loss: 0.6832\n",
            "Epoch 738/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5285 - val_loss: 0.6637\n",
            "Epoch 739/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5474 - val_loss: 0.6732\n",
            "Epoch 740/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5355 - val_loss: 0.6775\n",
            "Epoch 741/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5502 - val_loss: 0.6756\n",
            "Epoch 742/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5204 - val_loss: 0.6680\n",
            "Epoch 743/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5349 - val_loss: 0.6628\n",
            "Epoch 744/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5305 - val_loss: 0.6649\n",
            "Epoch 745/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5313 - val_loss: 0.6479\n",
            "Epoch 746/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5090 - val_loss: 0.6600\n",
            "Epoch 747/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5239 - val_loss: 0.6537\n",
            "Epoch 748/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5158 - val_loss: 0.6853\n",
            "Epoch 749/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5397 - val_loss: 0.6789\n",
            "Epoch 750/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5402 - val_loss: 0.6533\n",
            "Epoch 751/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5371 - val_loss: 0.6782\n",
            "Epoch 752/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5330 - val_loss: 0.6748\n",
            "Epoch 753/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5354 - val_loss: 0.6551\n",
            "Epoch 754/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5260 - val_loss: 0.6824\n",
            "Epoch 755/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5181 - val_loss: 0.6553\n",
            "Epoch 756/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5320 - val_loss: 0.6659\n",
            "Epoch 757/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5305 - val_loss: 0.6352\n",
            "Epoch 758/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5245 - val_loss: 0.6507\n",
            "Epoch 759/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5338 - val_loss: 0.6505\n",
            "Epoch 760/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5199 - val_loss: 0.6759\n",
            "Epoch 761/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5091 - val_loss: 0.6513\n",
            "Epoch 762/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5400 - val_loss: 0.6649\n",
            "Epoch 763/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5314 - val_loss: 0.6501\n",
            "Epoch 764/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.5211 - val_loss: 0.6588\n",
            "Epoch 765/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.4986 - val_loss: 0.6712\n",
            "Epoch 766/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5154 - val_loss: 0.6629\n",
            "Epoch 767/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5359 - val_loss: 0.6575\n",
            "Epoch 768/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5282 - val_loss: 0.6355\n",
            "Epoch 769/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5284 - val_loss: 0.6422\n",
            "Epoch 770/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5262 - val_loss: 0.6701\n",
            "Epoch 771/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5421 - val_loss: 0.6646\n",
            "Epoch 772/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5392 - val_loss: 0.6523\n",
            "Epoch 773/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5381 - val_loss: 0.6685\n",
            "Epoch 774/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5122 - val_loss: 0.6480\n",
            "Epoch 775/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5243 - val_loss: 0.6717\n",
            "Epoch 776/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5279 - val_loss: 0.6907\n",
            "Epoch 777/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5183 - val_loss: 0.6669\n",
            "Epoch 778/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5239 - val_loss: 0.6671\n",
            "Epoch 779/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5240 - val_loss: 0.6675\n",
            "Epoch 780/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5185 - val_loss: 0.6654\n",
            "Epoch 781/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5048 - val_loss: 0.6671\n",
            "Epoch 782/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5231 - val_loss: 0.6697\n",
            "Epoch 783/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5329 - val_loss: 0.6918\n",
            "Epoch 784/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5248 - val_loss: 0.6416\n",
            "Epoch 785/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5183 - val_loss: 0.6560\n",
            "Epoch 786/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5181 - val_loss: 0.6619\n",
            "Epoch 787/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5155 - val_loss: 0.6427\n",
            "Epoch 788/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5226 - val_loss: 0.6553\n",
            "Epoch 789/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5430 - val_loss: 0.6546\n",
            "Epoch 790/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5099 - val_loss: 0.6630\n",
            "Epoch 791/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5255 - val_loss: 0.6410\n",
            "Epoch 792/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5051 - val_loss: 0.6563\n",
            "Epoch 793/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5176 - val_loss: 0.6596\n",
            "Epoch 794/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5087 - val_loss: 0.6685\n",
            "Epoch 795/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5259 - val_loss: 0.6798\n",
            "Epoch 796/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5132 - val_loss: 0.6734\n",
            "Epoch 797/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5247 - val_loss: 0.6498\n",
            "Epoch 798/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5555 - val_loss: 0.6473\n",
            "Epoch 799/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5262 - val_loss: 0.6597\n",
            "Epoch 800/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5211 - val_loss: 0.6464\n",
            "Epoch 801/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5105 - val_loss: 0.6381\n",
            "Epoch 802/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5191 - val_loss: 0.6438\n",
            "Epoch 803/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5202 - val_loss: 0.6405\n",
            "Epoch 804/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5196 - val_loss: 0.6647\n",
            "Epoch 805/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5278 - val_loss: 0.6582\n",
            "Epoch 806/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5274 - val_loss: 0.6824\n",
            "Epoch 807/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5090 - val_loss: 0.6383\n",
            "Epoch 808/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5050 - val_loss: 0.6619\n",
            "Epoch 809/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5279 - val_loss: 0.6571\n",
            "Epoch 810/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5213 - val_loss: 0.6493\n",
            "Epoch 811/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5127 - val_loss: 0.6306\n",
            "Epoch 812/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5216 - val_loss: 0.6322\n",
            "Epoch 813/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5360 - val_loss: 0.6495\n",
            "Epoch 814/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5367 - val_loss: 0.6521\n",
            "Epoch 815/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5290 - val_loss: 0.6566\n",
            "Epoch 816/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.5223 - val_loss: 0.6620\n",
            "Epoch 817/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5325 - val_loss: 0.6493\n",
            "Epoch 818/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5174 - val_loss: 0.6730\n",
            "Epoch 819/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4919 - val_loss: 0.6567\n",
            "Epoch 820/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5046 - val_loss: 0.6928\n",
            "Epoch 821/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5144 - val_loss: 0.6574\n",
            "Epoch 822/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5170 - val_loss: 0.6453\n",
            "Epoch 823/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5078 - val_loss: 0.6535\n",
            "Epoch 824/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5210 - val_loss: 0.6725\n",
            "Epoch 825/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5107 - val_loss: 0.6497\n",
            "Epoch 826/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5136 - val_loss: 0.6448\n",
            "Epoch 827/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5184 - val_loss: 0.6368\n",
            "Epoch 828/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5018 - val_loss: 0.6499\n",
            "Epoch 829/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5008 - val_loss: 0.6434\n",
            "Epoch 830/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5070 - val_loss: 0.6544\n",
            "Epoch 831/1000\n",
            "52/52 [==============================] - 1s 19ms/step - loss: 0.5007 - val_loss: 0.6470\n",
            "Epoch 832/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5137 - val_loss: 0.6714\n",
            "Epoch 833/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5237 - val_loss: 0.6496\n",
            "Epoch 834/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4999 - val_loss: 0.6438\n",
            "Epoch 835/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5247 - val_loss: 0.6396\n",
            "Epoch 836/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5247 - val_loss: 0.6465\n",
            "Epoch 837/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4998 - val_loss: 0.6507\n",
            "Epoch 838/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4847 - val_loss: 0.6595\n",
            "Epoch 839/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5321 - val_loss: 0.6398\n",
            "Epoch 840/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.4948 - val_loss: 0.6465\n",
            "Epoch 841/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.4989 - val_loss: 0.6666\n",
            "Epoch 842/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.5272 - val_loss: 0.6630\n",
            "Epoch 843/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5111 - val_loss: 0.6570\n",
            "Epoch 844/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4841 - val_loss: 0.6756\n",
            "Epoch 845/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4917 - val_loss: 0.6839\n",
            "Epoch 846/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4967 - val_loss: 0.6718\n",
            "Epoch 847/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5140 - val_loss: 0.6485\n",
            "Epoch 848/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4847 - val_loss: 0.6462\n",
            "Epoch 849/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5114 - val_loss: 0.6692\n",
            "Epoch 850/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4955 - val_loss: 0.6657\n",
            "Epoch 851/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4971 - val_loss: 0.6374\n",
            "Epoch 852/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.4925 - val_loss: 0.6488\n",
            "Epoch 853/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5065 - val_loss: 0.6394\n",
            "Epoch 854/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.4920 - val_loss: 0.6508\n",
            "Epoch 855/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5079 - val_loss: 0.6580\n",
            "Epoch 856/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4926 - val_loss: 0.6411\n",
            "Epoch 857/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4980 - val_loss: 0.6474\n",
            "Epoch 858/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4877 - val_loss: 0.6538\n",
            "Epoch 859/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4899 - val_loss: 0.6517\n",
            "Epoch 860/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4910 - val_loss: 0.6649\n",
            "Epoch 861/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4905 - val_loss: 0.6496\n",
            "Epoch 862/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4861 - val_loss: 0.6652\n",
            "Epoch 863/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4933 - val_loss: 0.6541\n",
            "Epoch 864/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5113 - val_loss: 0.6444\n",
            "Epoch 865/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4825 - val_loss: 0.6372\n",
            "Epoch 866/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.4866 - val_loss: 0.6523\n",
            "Epoch 867/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.4969 - val_loss: 0.6719\n",
            "Epoch 868/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4903 - val_loss: 0.6482\n",
            "Epoch 869/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5015 - val_loss: 0.6384\n",
            "Epoch 870/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5164 - val_loss: 0.6330\n",
            "Epoch 871/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4806 - val_loss: 0.6585\n",
            "Epoch 872/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4959 - val_loss: 0.6270\n",
            "Epoch 873/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5019 - val_loss: 0.6440\n",
            "Epoch 874/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5005 - val_loss: 0.6441\n",
            "Epoch 875/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4987 - val_loss: 0.6551\n",
            "Epoch 876/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5046 - val_loss: 0.6498\n",
            "Epoch 877/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.4963 - val_loss: 0.6448\n",
            "Epoch 878/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.4993 - val_loss: 0.6563\n",
            "Epoch 879/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5137 - val_loss: 0.6308\n",
            "Epoch 880/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5101 - val_loss: 0.6476\n",
            "Epoch 881/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4861 - val_loss: 0.6219\n",
            "Epoch 882/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4861 - val_loss: 0.6446\n",
            "Epoch 883/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4836 - val_loss: 0.6517\n",
            "Epoch 884/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.5074 - val_loss: 0.6538\n",
            "Epoch 885/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4928 - val_loss: 0.6538\n",
            "Epoch 886/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4801 - val_loss: 0.6437\n",
            "Epoch 887/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4963 - val_loss: 0.6479\n",
            "Epoch 888/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4838 - val_loss: 0.6587\n",
            "Epoch 889/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5096 - val_loss: 0.6498\n",
            "Epoch 890/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4862 - val_loss: 0.6537\n",
            "Epoch 891/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4862 - val_loss: 0.6349\n",
            "Epoch 892/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5045 - val_loss: 0.6385\n",
            "Epoch 893/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4747 - val_loss: 0.6709\n",
            "Epoch 894/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4971 - val_loss: 0.6519\n",
            "Epoch 895/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4860 - val_loss: 0.6535\n",
            "Epoch 896/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5064 - val_loss: 0.6551\n",
            "Epoch 897/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4787 - val_loss: 0.6450\n",
            "Epoch 898/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4911 - val_loss: 0.6438\n",
            "Epoch 899/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4972 - val_loss: 0.6583\n",
            "Epoch 900/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4967 - val_loss: 0.6423\n",
            "Epoch 901/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4795 - val_loss: 0.6456\n",
            "Epoch 902/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4870 - val_loss: 0.6356\n",
            "Epoch 903/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4921 - val_loss: 0.6496\n",
            "Epoch 904/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.4850 - val_loss: 0.6528\n",
            "Epoch 905/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4760 - val_loss: 0.6547\n",
            "Epoch 906/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.5096 - val_loss: 0.6553\n",
            "Epoch 907/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4777 - val_loss: 0.6518\n",
            "Epoch 908/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4876 - val_loss: 0.6542\n",
            "Epoch 909/1000\n",
            "52/52 [==============================] - 1s 20ms/step - loss: 0.4770 - val_loss: 0.6423\n",
            "Epoch 910/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4707 - val_loss: 0.6477\n",
            "Epoch 911/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5026 - val_loss: 0.6432\n",
            "Epoch 912/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4763 - val_loss: 0.6657\n",
            "Epoch 913/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.4704 - val_loss: 0.6375\n",
            "Epoch 914/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4975 - val_loss: 0.6360\n",
            "Epoch 915/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4728 - val_loss: 0.6366\n",
            "Epoch 916/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5149 - val_loss: 0.6530\n",
            "Epoch 917/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4718 - val_loss: 0.6550\n",
            "Epoch 918/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4717 - val_loss: 0.6548\n",
            "Epoch 919/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4904 - val_loss: 0.6436\n",
            "Epoch 920/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4934 - val_loss: 0.6379\n",
            "Epoch 921/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4788 - val_loss: 0.6488\n",
            "Epoch 922/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4718 - val_loss: 0.6371\n",
            "Epoch 923/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4772 - val_loss: 0.6554\n",
            "Epoch 924/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4804 - val_loss: 0.6303\n",
            "Epoch 925/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.4805 - val_loss: 0.6424\n",
            "Epoch 926/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4809 - val_loss: 0.6435\n",
            "Epoch 927/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4741 - val_loss: 0.6355\n",
            "Epoch 928/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.4901 - val_loss: 0.6428\n",
            "Epoch 929/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4936 - val_loss: 0.6409\n",
            "Epoch 930/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4596 - val_loss: 0.6449\n",
            "Epoch 931/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4953 - val_loss: 0.6451\n",
            "Epoch 932/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4843 - val_loss: 0.6353\n",
            "Epoch 933/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4896 - val_loss: 0.6487\n",
            "Epoch 934/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4779 - val_loss: 0.6229\n",
            "Epoch 935/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4736 - val_loss: 0.6429\n",
            "Epoch 936/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4661 - val_loss: 0.6339\n",
            "Epoch 937/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.4705 - val_loss: 0.6417\n",
            "Epoch 938/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4748 - val_loss: 0.6458\n",
            "Epoch 939/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.4938 - val_loss: 0.6428\n",
            "Epoch 940/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.4875 - val_loss: 0.6518\n",
            "Epoch 941/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4878 - val_loss: 0.6372\n",
            "Epoch 942/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4656 - val_loss: 0.6307\n",
            "Epoch 943/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4810 - val_loss: 0.6329\n",
            "Epoch 944/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4567 - val_loss: 0.6306\n",
            "Epoch 945/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4860 - val_loss: 0.6563\n",
            "Epoch 946/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4618 - val_loss: 0.6529\n",
            "Epoch 947/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.4632 - val_loss: 0.6446\n",
            "Epoch 948/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4658 - val_loss: 0.6423\n",
            "Epoch 949/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4648 - val_loss: 0.6505\n",
            "Epoch 950/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.4711 - val_loss: 0.6486\n",
            "Epoch 951/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4657 - val_loss: 0.6690\n",
            "Epoch 952/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4616 - val_loss: 0.6533\n",
            "Epoch 953/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4906 - val_loss: 0.6657\n",
            "Epoch 954/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4554 - val_loss: 0.6446\n",
            "Epoch 955/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4779 - val_loss: 0.6460\n",
            "Epoch 956/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4923 - val_loss: 0.6514\n",
            "Epoch 957/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4880 - val_loss: 0.6531\n",
            "Epoch 958/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4933 - val_loss: 0.6405\n",
            "Epoch 959/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4992 - val_loss: 0.6590\n",
            "Epoch 960/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.4669 - val_loss: 0.6415\n",
            "Epoch 961/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4560 - val_loss: 0.6560\n",
            "Epoch 962/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4663 - val_loss: 0.6419\n",
            "Epoch 963/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4618 - val_loss: 0.6443\n",
            "Epoch 964/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4822 - val_loss: 0.6443\n",
            "Epoch 965/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4775 - val_loss: 0.6535\n",
            "Epoch 966/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4597 - val_loss: 0.6387\n",
            "Epoch 967/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4711 - val_loss: 0.6378\n",
            "Epoch 968/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4729 - val_loss: 0.6286\n",
            "Epoch 969/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4857 - val_loss: 0.6252\n",
            "Epoch 970/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4645 - val_loss: 0.6297\n",
            "Epoch 971/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4621 - val_loss: 0.6443\n",
            "Epoch 972/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4780 - val_loss: 0.6312\n",
            "Epoch 973/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4556 - val_loss: 0.6424\n",
            "Epoch 974/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4675 - val_loss: 0.6317\n",
            "Epoch 975/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.4766 - val_loss: 0.6336\n",
            "Epoch 976/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.4821 - val_loss: 0.6403\n",
            "Epoch 977/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4861 - val_loss: 0.6440\n",
            "Epoch 978/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4679 - val_loss: 0.6785\n",
            "Epoch 979/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4667 - val_loss: 0.6593\n",
            "Epoch 980/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4743 - val_loss: 0.6271\n",
            "Epoch 981/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4611 - val_loss: 0.6685\n",
            "Epoch 982/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4607 - val_loss: 0.6541\n",
            "Epoch 983/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.4789 - val_loss: 0.6539\n",
            "Epoch 984/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.4904 - val_loss: 0.6373\n",
            "Epoch 985/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.4833 - val_loss: 0.6286\n",
            "Epoch 986/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.4740 - val_loss: 0.6331\n",
            "Epoch 987/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4661 - val_loss: 0.6436\n",
            "Epoch 988/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4571 - val_loss: 0.6469\n",
            "Epoch 989/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4705 - val_loss: 0.6317\n",
            "Epoch 990/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4314 - val_loss: 0.6513\n",
            "Epoch 991/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4793 - val_loss: 0.6630\n",
            "Epoch 992/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4659 - val_loss: 0.6361\n",
            "Epoch 993/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4820 - val_loss: 0.6379\n",
            "Epoch 994/1000\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 0.4754 - val_loss: 0.6344\n",
            "Epoch 995/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.4784 - val_loss: 0.6450\n",
            "Epoch 996/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4631 - val_loss: 0.6356\n",
            "Epoch 997/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.4574 - val_loss: 0.6624\n",
            "Epoch 998/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.4712 - val_loss: 0.6440\n",
            "Epoch 999/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4871 - val_loss: 0.6546\n",
            "Epoch 1000/1000\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.4767 - val_loss: 0.6306\n"
          ]
        }
      ],
      "source": [
        "#run call the transformer model\n",
        "#df_read = df.copy()\n",
        "#df_read=(df_read-df_read.mean())/df_read.std()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(df_read, test_size=0.2)\n",
        "\n",
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "test = df5.copy()\n",
        "\n",
        "Y_train = np.array(train['class'])\n",
        "X_train= np.array(train.drop(['class'],axis=1))\n",
        "X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
        "\n",
        "Y_val=np.array(test['class'])\n",
        "X_val = np.array(test.drop(['class'],axis=1))\n",
        "X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])\n",
        "NUM_LAYERS =  4\n",
        "\n",
        "D_MODEL = X_train.shape[2]\n",
        "NUM_HEADS =  4\n",
        "UNITS =  2048\n",
        "DROPOUT = 0.1 #0.1\n",
        "TIME_STEPS= X_train.shape[1]\n",
        "OUTPUT_SIZE=1\n",
        "batch_size=64\n",
        "\n",
        "model1 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=3,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "model2 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=5,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "model3 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=6,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "\n",
        "#run\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='mae') #org\n",
        "model1.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss='mse')\n",
        "# model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mae')\n",
        "#\n",
        "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=80, restore_best_weights=True)\n",
        "# history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "history = model1.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UxMyaAwpcqdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343a654a-0284-401d-a8ca-428c5a668f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00027735779084355\n",
            "(1660, 1, 180)\n",
            "(415, 1, 180)\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "import time\n",
        "st = time.time()\n",
        "p1 = np.array(model1(X_val)).flatten()\n",
        "end = time.time()\n",
        "# print(end, st, len(p1))\n",
        "print((end-st)/len(p1))\n",
        "p2 = np.array(model1(X_train)).flatten()\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Say3gnZgi331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacfa6f7-f220-4890-d85f-0cfbbe4ac7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.        0.8910283]\n",
            " [0.8910283 1.       ]] SignificanceResult(statistic=0.8874648464604122, pvalue=0.0) 0.7061803424289614\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.801409578980255, 0.7940676569921825, 0.5995110661204609)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import kendalltau\n",
        "print(np.corrcoef(p2, Y_train), stats.spearmanr(p2, Y_train), kendalltau(p2,Y_train).correlation)\n",
        "np.corrcoef(p1, Y_val)[1][0], stats.spearmanr(p1, Y_val).correlation, kendalltau(p1,Y_val).correlation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ST5WiH4KoeOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f725ec1-d0c2-40f2-d7c3-cc7e64f81992"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1660, 1, 180)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ksXNLzMScqg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "2916653f-640d-4638-942e-27e3b6d334d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"audio_dm_3_featu_.jpg\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}