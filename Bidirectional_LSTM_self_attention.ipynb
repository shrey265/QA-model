{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OEO0uJVJiwe7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional, Reshape\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CF5kNcdZi3Wl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "0nePuKcLhpbj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kXVqubki6p-",
        "outputId": "634dd266-0a1e-4f4c-d7c7-397375aa6f1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/audio-interspeech/2075_concatenate_dm_4featu.csv')\n",
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TuIFKZmi7R9",
        "outputId": "6f36c1c9-09d0-4473-e5ce-d449c1fdc272"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2075, 181)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "siz=415\n",
        "df_read = df.copy()\n",
        "df1 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df1.index)\n",
        "df2 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df2.index)\n",
        "df3 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df3.index)\n",
        "df4 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df4.index)\n",
        "df5 = df_read.copy()\n",
        "\n",
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)\n",
        "print(df4.shape)\n",
        "print(df5.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9IdmEYSjAD8",
        "outputId": "0ab68d8e-bb3c-43a3-8a81-1fde7667fdee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = list(df1.index)+list(df2.index)+list(df3.index)+list(df4.index)+list(df5.index)\n",
        "print(df1.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G392P-h4jA1m",
        "outputId": "ac4929ae-5e20-4242-d01e-3d14c4a48257"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int64Index([ 881,  453, 2004, 1353,  281,  941, 1185, 1159, 1138,  599,\n",
            "            ...\n",
            "             834, 1730,  353, 1345, 1190, 1375,  185,  701, 1671, 1982],\n",
            "           dtype='int64', length=415)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "test = df5.copy()\n",
        "\n",
        "Y_train = np.array(train['class'])\n",
        "X_train= np.array(train.drop(['class'],axis=1))\n",
        "# X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
        "\n",
        "Y_val=np.array(test['class'])\n",
        "X_val = np.array(test.drop(['class'],axis=1))\n",
        "# X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])"
      ],
      "metadata": {
        "id": "ELQGB5K6jC5d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TIME_STEPS= X_train.shape[1]"
      ],
      "metadata": {
        "id": "rumecB5Bj6wu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32) # scaler value\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  return tf.matmul(attention_weights, value)"
      ],
      "metadata": {
        "id": "44ncfAdzhBGp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "# This allows to the transformer to know where there is real data and where it is padded\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # check if a values of the sequence are 0 or not\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "metadata": {
        "id": "xbFRxxYvhEbr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "3ktV5pMWhEVd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "H6026AvuhI9_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  # outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) # extra layer added\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "jXf5zPUVhNgr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "def encoder(time_steps,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            projection,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  if projection=='linear':\n",
        "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
        "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
        "    print('linear')\n",
        "\n",
        "  else:\n",
        "    projection=tf.identity(inputs)\n",
        "    print('none')\n",
        "\n",
        "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "0SX9XFCQhP1j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "def transformer_BiLSTM(time_steps,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                output_size,\n",
        "                projection,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(tf.dtypes.cast(\n",
        "\n",
        "      #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
        "      # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked\n",
        "      tf.math.reduce_sum(\n",
        "      inputs,\n",
        "      axis=2,\n",
        "      keepdims=False,\n",
        "      name=None\n",
        "  ), tf.int32))\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      time_steps=time_steps,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      projection=projection,\n",
        "      name='encoder'\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  #We reshape for feeding our FC in the next step\n",
        "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
        "\n",
        "  #We predict our class\n",
        "  # model.add(Reshape((input_shape[0],1), input_shape=input_shape))\n",
        "  outputs = tf.keras.layers.Reshape((d_model,1))(outputs)\n",
        "  outputs = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(d_model))(outputs)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=d_model/2,use_bias=True, name=\"outputs_1\")(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model/4,use_bias=True, name=\"outputs_2\")(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True, name=\"outputs_3\")(outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
      ],
      "metadata": {
        "id": "2w-2wlRChkUU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_BiLSTM(hidden_units, dense_units, input_shape, activation):\n",
        "#     model = Sequential()\n",
        "#     # model.add(LSTM(hidden_units,input_shape=input_shape))\n",
        "#     model.add(Reshape((input_shape[0],1), input_shape=input_shape))\n",
        "#     model.add(Bidirectional(LSTM(hidden_units)))\n",
        "#     model.add(Dense(units = input_shape[0]//2, activation='relu'))\n",
        "#     model.add(Dropout(0.5))\n",
        "#     model.add(Dense(units=input_shape[0]//4, activation='relu'))\n",
        "#     model.add(Dropout(0.5))\n",
        "#     model.add(Dense(units=1))\n",
        "#     model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        "#     return model"
      ],
      "metadata": {
        "id": "WNJlu1omjGtM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = create_BiLSTM(hidden_units=180, dense_units=1, input_shape=(TIME_STEPS,1),\n",
        "#                    activation=['tanh', 'tanh'])\n",
        "# model.fit(X_train, Y_train, epochs=1000, batch_size=64, verbose=2,validation_data=(X_val, Y_val))"
      ],
      "metadata": {
        "id": "SknHlFOKmf0n"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run call the transformer model\n",
        "#df_read = df.copy()\n",
        "#df_read=(df_read-df_read.mean())/df_read.std()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(df_read, test_size=0.2)\n",
        "\n",
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "test = df5.copy()\n",
        "\n",
        "Y_train = np.array(train['class'])\n",
        "X_train= np.array(train.drop(['class'],axis=1))\n",
        "X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
        "\n",
        "Y_val=np.array(test['class'])\n",
        "X_val = np.array(test.drop(['class'],axis=1))\n",
        "X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])\n",
        "NUM_LAYERS =  1\n",
        "\n",
        "D_MODEL = X_train.shape[2]\n",
        "NUM_HEADS =  4\n",
        "UNITS =  2048\n",
        "DROPOUT = 0.1 #0.1\n",
        "TIME_STEPS= X_train.shape[1]\n",
        "OUTPUT_SIZE=1\n",
        "batch_size=64\n",
        "print(D_MODEL)\n",
        "model = transformer_BiLSTM(time_steps=TIME_STEPS,\n",
        "  num_layers=NUM_LAYERS,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "\n",
        "#run\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='mae') #org\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss='mse')\n",
        "# model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mae')\n",
        "#\n",
        "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=80, restore_best_weights=True)\n",
        "# history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val))"
      ],
      "metadata": {
        "id": "BDE3q15OiUMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c16c3c3-2bfc-4ed8-e89f-bbd5148aa081"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180\n",
            "linear\n",
            "Epoch 1/1000\n",
            "52/52 [==============================] - 16s 45ms/step - loss: 10.5074 - val_loss: 9.4969\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 9.9129 - val_loss: 8.6862\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 8.9548 - val_loss: 7.4738\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 7.6530 - val_loss: 5.9930\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 6.1572 - val_loss: 4.5137\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 4.7327 - val_loss: 3.2909\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 3.5929 - val_loss: 2.4580\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 2.7661 - val_loss: 1.9846\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 2.2565 - val_loss: 1.7874\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 2.0054 - val_loss: 1.7398\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.8339 - val_loss: 1.7435\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.7824 - val_loss: 1.7521\n",
            "Epoch 13/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 1.7538 - val_loss: 1.7458\n",
            "Epoch 14/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.7328 - val_loss: 1.7306\n",
            "Epoch 15/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.6641 - val_loss: 1.7148\n",
            "Epoch 16/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.6739 - val_loss: 1.6772\n",
            "Epoch 17/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.6490 - val_loss: 1.6586\n",
            "Epoch 18/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.6096 - val_loss: 1.6317\n",
            "Epoch 19/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.6270 - val_loss: 1.6179\n",
            "Epoch 20/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.5839 - val_loss: 1.6004\n",
            "Epoch 21/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.5822 - val_loss: 1.5805\n",
            "Epoch 22/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.5525 - val_loss: 1.5679\n",
            "Epoch 23/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.5583 - val_loss: 1.5526\n",
            "Epoch 24/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.5700 - val_loss: 1.5516\n",
            "Epoch 25/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.5470 - val_loss: 1.5263\n",
            "Epoch 26/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.5511 - val_loss: 1.5138\n",
            "Epoch 27/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.5261 - val_loss: 1.4843\n",
            "Epoch 28/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4899 - val_loss: 1.4655\n",
            "Epoch 29/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.5100 - val_loss: 1.4836\n",
            "Epoch 30/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4824 - val_loss: 1.4671\n",
            "Epoch 31/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.4888 - val_loss: 1.4566\n",
            "Epoch 32/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.4699 - val_loss: 1.4487\n",
            "Epoch 33/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4894 - val_loss: 1.4497\n",
            "Epoch 34/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.4654 - val_loss: 1.4331\n",
            "Epoch 35/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.4680 - val_loss: 1.4041\n",
            "Epoch 36/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.4633 - val_loss: 1.4096\n",
            "Epoch 37/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.4143 - val_loss: 1.3981\n",
            "Epoch 38/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4520 - val_loss: 1.3926\n",
            "Epoch 39/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4367 - val_loss: 1.3889\n",
            "Epoch 40/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3867 - val_loss: 1.3568\n",
            "Epoch 41/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3772 - val_loss: 1.3554\n",
            "Epoch 42/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.4032 - val_loss: 1.3573\n",
            "Epoch 43/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3928 - val_loss: 1.3438\n",
            "Epoch 44/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3578 - val_loss: 1.3286\n",
            "Epoch 45/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.3910 - val_loss: 1.3337\n",
            "Epoch 46/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.3969 - val_loss: 1.3412\n",
            "Epoch 47/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3423 - val_loss: 1.3502\n",
            "Epoch 48/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3342 - val_loss: 1.3337\n",
            "Epoch 49/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.3770 - val_loss: 1.3309\n",
            "Epoch 50/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3372 - val_loss: 1.3213\n",
            "Epoch 51/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3223 - val_loss: 1.2955\n",
            "Epoch 52/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3456 - val_loss: 1.3215\n",
            "Epoch 53/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3407 - val_loss: 1.2991\n",
            "Epoch 54/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3079 - val_loss: 1.3030\n",
            "Epoch 55/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 1.3312 - val_loss: 1.3211\n",
            "Epoch 56/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.3325 - val_loss: 1.2809\n",
            "Epoch 57/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.3209 - val_loss: 1.2842\n",
            "Epoch 58/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3284 - val_loss: 1.2766\n",
            "Epoch 59/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3354 - val_loss: 1.3059\n",
            "Epoch 60/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3011 - val_loss: 1.2976\n",
            "Epoch 61/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3064 - val_loss: 1.2860\n",
            "Epoch 62/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2934 - val_loss: 1.2935\n",
            "Epoch 63/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.3130 - val_loss: 1.3026\n",
            "Epoch 64/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2991 - val_loss: 1.2855\n",
            "Epoch 65/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.2599 - val_loss: 1.2873\n",
            "Epoch 66/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.2947 - val_loss: 1.3026\n",
            "Epoch 67/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.2600 - val_loss: 1.2880\n",
            "Epoch 68/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2663 - val_loss: 1.2673\n",
            "Epoch 69/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2798 - val_loss: 1.2741\n",
            "Epoch 70/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.2295 - val_loss: 1.2593\n",
            "Epoch 71/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2531 - val_loss: 1.2502\n",
            "Epoch 72/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2464 - val_loss: 1.2879\n",
            "Epoch 73/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2758 - val_loss: 1.2733\n",
            "Epoch 74/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2322 - val_loss: 1.2542\n",
            "Epoch 75/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2490 - val_loss: 1.2545\n",
            "Epoch 76/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.2425 - val_loss: 1.2723\n",
            "Epoch 77/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 1.2080 - val_loss: 1.2774\n",
            "Epoch 78/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1862 - val_loss: 1.2373\n",
            "Epoch 79/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.1845 - val_loss: 1.2571\n",
            "Epoch 80/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1911 - val_loss: 1.2664\n",
            "Epoch 81/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1850 - val_loss: 1.2783\n",
            "Epoch 82/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2306 - val_loss: 1.2483\n",
            "Epoch 83/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2064 - val_loss: 1.2509\n",
            "Epoch 84/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1837 - val_loss: 1.2752\n",
            "Epoch 85/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2035 - val_loss: 1.2282\n",
            "Epoch 86/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.2175 - val_loss: 1.2569\n",
            "Epoch 87/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 1.2374 - val_loss: 1.2824\n",
            "Epoch 88/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1852 - val_loss: 1.2547\n",
            "Epoch 89/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1333 - val_loss: 1.2668\n",
            "Epoch 90/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1670 - val_loss: 1.3099\n",
            "Epoch 91/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1608 - val_loss: 1.2722\n",
            "Epoch 92/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1611 - val_loss: 1.2326\n",
            "Epoch 93/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1665 - val_loss: 1.2922\n",
            "Epoch 94/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1940 - val_loss: 1.2441\n",
            "Epoch 95/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.1572 - val_loss: 1.2357\n",
            "Epoch 96/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.1686 - val_loss: 1.2295\n",
            "Epoch 97/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.1929 - val_loss: 1.2403\n",
            "Epoch 98/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1748 - val_loss: 1.2399\n",
            "Epoch 99/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1860 - val_loss: 1.2191\n",
            "Epoch 100/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1114 - val_loss: 1.2771\n",
            "Epoch 101/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1317 - val_loss: 1.2497\n",
            "Epoch 102/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1542 - val_loss: 1.1840\n",
            "Epoch 103/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.1526 - val_loss: 1.2085\n",
            "Epoch 104/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.1029 - val_loss: 1.2550\n",
            "Epoch 105/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.1472 - val_loss: 1.2293\n",
            "Epoch 106/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1426 - val_loss: 1.2145\n",
            "Epoch 107/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1392 - val_loss: 1.2762\n",
            "Epoch 108/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1331 - val_loss: 1.2354\n",
            "Epoch 109/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1340 - val_loss: 1.2716\n",
            "Epoch 110/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1407 - val_loss: 1.2042\n",
            "Epoch 111/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1236 - val_loss: 1.2274\n",
            "Epoch 112/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1291 - val_loss: 1.2136\n",
            "Epoch 113/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1259 - val_loss: 1.2241\n",
            "Epoch 114/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1132 - val_loss: 1.2435\n",
            "Epoch 115/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1062 - val_loss: 1.2034\n",
            "Epoch 116/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.1293 - val_loss: 1.2084\n",
            "Epoch 117/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.0848 - val_loss: 1.2548\n",
            "Epoch 118/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0980 - val_loss: 1.2755\n",
            "Epoch 119/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1217 - val_loss: 1.2286\n",
            "Epoch 120/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0912 - val_loss: 1.2383\n",
            "Epoch 121/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.1042 - val_loss: 1.2089\n",
            "Epoch 122/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0745 - val_loss: 1.1915\n",
            "Epoch 123/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0999 - val_loss: 1.1899\n",
            "Epoch 124/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0806 - val_loss: 1.1957\n",
            "Epoch 125/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0982 - val_loss: 1.1927\n",
            "Epoch 126/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.0689 - val_loss: 1.1888\n",
            "Epoch 127/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 1.1059 - val_loss: 1.1956\n",
            "Epoch 128/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0439 - val_loss: 1.2190\n",
            "Epoch 129/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.1075 - val_loss: 1.2071\n",
            "Epoch 130/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0488 - val_loss: 1.1989\n",
            "Epoch 131/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0714 - val_loss: 1.2102\n",
            "Epoch 132/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0728 - val_loss: 1.1817\n",
            "Epoch 133/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.0876 - val_loss: 1.1571\n",
            "Epoch 134/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0875 - val_loss: 1.1818\n",
            "Epoch 135/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0791 - val_loss: 1.1612\n",
            "Epoch 136/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.0822 - val_loss: 1.1469\n",
            "Epoch 137/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0536 - val_loss: 1.1780\n",
            "Epoch 138/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.0904 - val_loss: 1.2061\n",
            "Epoch 139/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0559 - val_loss: 1.1509\n",
            "Epoch 140/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0747 - val_loss: 1.2096\n",
            "Epoch 141/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.0449 - val_loss: 1.1479\n",
            "Epoch 142/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0572 - val_loss: 1.2052\n",
            "Epoch 143/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0491 - val_loss: 1.1829\n",
            "Epoch 144/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0607 - val_loss: 1.1612\n",
            "Epoch 145/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.0719 - val_loss: 1.1223\n",
            "Epoch 146/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.0400 - val_loss: 1.1474\n",
            "Epoch 147/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.0314 - val_loss: 1.2065\n",
            "Epoch 148/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0468 - val_loss: 1.1294\n",
            "Epoch 149/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0617 - val_loss: 1.1488\n",
            "Epoch 150/1000\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0219 - val_loss: 1.1820\n",
            "Epoch 151/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0423 - val_loss: 1.1794\n",
            "Epoch 152/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0746 - val_loss: 1.1665\n",
            "Epoch 153/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0419 - val_loss: 1.1346\n",
            "Epoch 154/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0568 - val_loss: 1.1429\n",
            "Epoch 155/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0126 - val_loss: 1.1632\n",
            "Epoch 156/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0050 - val_loss: 1.1469\n",
            "Epoch 157/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0187 - val_loss: 1.0976\n",
            "Epoch 158/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0124 - val_loss: 1.1448\n",
            "Epoch 159/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9893 - val_loss: 1.1152\n",
            "Epoch 160/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0197 - val_loss: 1.1292\n",
            "Epoch 161/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0215 - val_loss: 1.1791\n",
            "Epoch 162/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0107 - val_loss: 1.1100\n",
            "Epoch 163/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9779 - val_loss: 1.1851\n",
            "Epoch 164/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0484 - val_loss: 1.1612\n",
            "Epoch 165/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.0051 - val_loss: 1.1542\n",
            "Epoch 166/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 1.0548 - val_loss: 1.1023\n",
            "Epoch 167/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9655 - val_loss: 1.1492\n",
            "Epoch 168/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9972 - val_loss: 1.1134\n",
            "Epoch 169/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.0238 - val_loss: 1.1800\n",
            "Epoch 170/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0136 - val_loss: 1.1387\n",
            "Epoch 171/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0264 - val_loss: 1.0988\n",
            "Epoch 172/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0164 - val_loss: 1.1482\n",
            "Epoch 173/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9840 - val_loss: 1.1120\n",
            "Epoch 174/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0049 - val_loss: 1.1158\n",
            "Epoch 175/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0053 - val_loss: 1.0972\n",
            "Epoch 176/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9863 - val_loss: 1.1363\n",
            "Epoch 177/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.0028 - val_loss: 1.0924\n",
            "Epoch 178/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.0470 - val_loss: 1.0847\n",
            "Epoch 179/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0080 - val_loss: 1.0899\n",
            "Epoch 180/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9959 - val_loss: 1.1310\n",
            "Epoch 181/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9965 - val_loss: 1.1151\n",
            "Epoch 182/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9555 - val_loss: 1.1627\n",
            "Epoch 183/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9607 - val_loss: 1.1175\n",
            "Epoch 184/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.0236 - val_loss: 1.1516\n",
            "Epoch 185/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9499 - val_loss: 1.1261\n",
            "Epoch 186/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9936 - val_loss: 1.1332\n",
            "Epoch 187/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.9627 - val_loss: 1.0765\n",
            "Epoch 188/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.9953 - val_loss: 1.1007\n",
            "Epoch 189/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9914 - val_loss: 1.0955\n",
            "Epoch 190/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9689 - val_loss: 1.0825\n",
            "Epoch 191/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.9685 - val_loss: 1.1421\n",
            "Epoch 192/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9908 - val_loss: 1.0755\n",
            "Epoch 193/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9522 - val_loss: 1.1198\n",
            "Epoch 194/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9818 - val_loss: 1.0916\n",
            "Epoch 195/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9247 - val_loss: 1.1208\n",
            "Epoch 196/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9558 - val_loss: 1.0603\n",
            "Epoch 197/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.9886 - val_loss: 1.0802\n",
            "Epoch 198/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9574 - val_loss: 1.0526\n",
            "Epoch 199/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9733 - val_loss: 1.0513\n",
            "Epoch 200/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9567 - val_loss: 1.1019\n",
            "Epoch 201/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9644 - val_loss: 1.0557\n",
            "Epoch 202/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9890 - val_loss: 1.1079\n",
            "Epoch 203/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9371 - val_loss: 1.0582\n",
            "Epoch 204/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9895 - val_loss: 1.0899\n",
            "Epoch 205/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9645 - val_loss: 1.0447\n",
            "Epoch 206/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9678 - val_loss: 1.0809\n",
            "Epoch 207/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9524 - val_loss: 1.0247\n",
            "Epoch 208/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9493 - val_loss: 1.1029\n",
            "Epoch 209/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9353 - val_loss: 1.0648\n",
            "Epoch 210/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9729 - val_loss: 1.0707\n",
            "Epoch 211/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9919 - val_loss: 1.0706\n",
            "Epoch 212/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9581 - val_loss: 1.0416\n",
            "Epoch 213/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9598 - val_loss: 1.0997\n",
            "Epoch 214/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9559 - val_loss: 1.0450\n",
            "Epoch 215/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9521 - val_loss: 1.0779\n",
            "Epoch 216/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9519 - val_loss: 1.0790\n",
            "Epoch 217/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9459 - val_loss: 1.0430\n",
            "Epoch 218/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9543 - val_loss: 1.0491\n",
            "Epoch 219/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9317 - val_loss: 1.1129\n",
            "Epoch 220/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9588 - val_loss: 1.0492\n",
            "Epoch 221/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9418 - val_loss: 1.0742\n",
            "Epoch 222/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9591 - val_loss: 1.0884\n",
            "Epoch 223/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9613 - val_loss: 1.0031\n",
            "Epoch 224/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9428 - val_loss: 1.0716\n",
            "Epoch 225/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9540 - val_loss: 1.0785\n",
            "Epoch 226/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9146 - val_loss: 1.0586\n",
            "Epoch 227/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9592 - val_loss: 1.0685\n",
            "Epoch 228/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9321 - val_loss: 1.0477\n",
            "Epoch 229/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9388 - val_loss: 1.0050\n",
            "Epoch 230/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9035 - val_loss: 1.0557\n",
            "Epoch 231/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9276 - val_loss: 1.0306\n",
            "Epoch 232/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9039 - val_loss: 1.0719\n",
            "Epoch 233/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9323 - val_loss: 1.0619\n",
            "Epoch 234/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9330 - val_loss: 1.0488\n",
            "Epoch 235/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9316 - val_loss: 1.0292\n",
            "Epoch 236/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8912 - val_loss: 1.0596\n",
            "Epoch 237/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9029 - val_loss: 1.0509\n",
            "Epoch 238/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9407 - val_loss: 1.0353\n",
            "Epoch 239/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9207 - val_loss: 0.9724\n",
            "Epoch 240/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9240 - val_loss: 1.0564\n",
            "Epoch 241/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9536 - val_loss: 1.0374\n",
            "Epoch 242/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8866 - val_loss: 1.0618\n",
            "Epoch 243/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9216 - val_loss: 1.0608\n",
            "Epoch 244/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9498 - val_loss: 1.0692\n",
            "Epoch 245/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9420 - val_loss: 1.0260\n",
            "Epoch 246/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9250 - val_loss: 1.0418\n",
            "Epoch 247/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9107 - val_loss: 1.0324\n",
            "Epoch 248/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9242 - val_loss: 1.0094\n",
            "Epoch 249/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9128 - val_loss: 1.0143\n",
            "Epoch 250/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9269 - val_loss: 1.0111\n",
            "Epoch 251/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9201 - val_loss: 0.9998\n",
            "Epoch 252/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9048 - val_loss: 0.9917\n",
            "Epoch 253/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8823 - val_loss: 1.0069\n",
            "Epoch 254/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8888 - val_loss: 1.0358\n",
            "Epoch 255/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9254 - val_loss: 1.0458\n",
            "Epoch 256/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9039 - val_loss: 1.0199\n",
            "Epoch 257/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9378 - val_loss: 1.0254\n",
            "Epoch 258/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9019 - val_loss: 1.0504\n",
            "Epoch 259/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9078 - val_loss: 1.0599\n",
            "Epoch 260/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8917 - val_loss: 0.9691\n",
            "Epoch 261/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9013 - val_loss: 1.0185\n",
            "Epoch 262/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8619 - val_loss: 0.9861\n",
            "Epoch 263/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.9054 - val_loss: 0.9997\n",
            "Epoch 264/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8963 - val_loss: 1.0336\n",
            "Epoch 265/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9169 - val_loss: 1.0094\n",
            "Epoch 266/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9054 - val_loss: 1.0427\n",
            "Epoch 267/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9103 - val_loss: 1.0290\n",
            "Epoch 268/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9072 - val_loss: 0.9776\n",
            "Epoch 269/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.9026 - val_loss: 0.9777\n",
            "Epoch 270/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8707 - val_loss: 0.9847\n",
            "Epoch 271/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9144 - val_loss: 0.9482\n",
            "Epoch 272/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8948 - val_loss: 0.9763\n",
            "Epoch 273/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8863 - val_loss: 1.0147\n",
            "Epoch 274/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8839 - val_loss: 1.0058\n",
            "Epoch 275/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.9046 - val_loss: 0.9912\n",
            "Epoch 276/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8850 - val_loss: 1.0135\n",
            "Epoch 277/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8748 - val_loss: 1.0079\n",
            "Epoch 278/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8577 - val_loss: 0.9972\n",
            "Epoch 279/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8864 - val_loss: 0.9959\n",
            "Epoch 280/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8683 - val_loss: 1.0163\n",
            "Epoch 281/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8814 - val_loss: 1.0075\n",
            "Epoch 282/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8694 - val_loss: 1.0255\n",
            "Epoch 283/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8690 - val_loss: 0.9990\n",
            "Epoch 284/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8933 - val_loss: 0.9644\n",
            "Epoch 285/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8818 - val_loss: 1.0031\n",
            "Epoch 286/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8665 - val_loss: 0.9841\n",
            "Epoch 287/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8606 - val_loss: 1.0197\n",
            "Epoch 288/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8695 - val_loss: 0.9536\n",
            "Epoch 289/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8448 - val_loss: 0.9600\n",
            "Epoch 290/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8709 - val_loss: 0.9601\n",
            "Epoch 291/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8877 - val_loss: 0.9308\n",
            "Epoch 292/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8703 - val_loss: 0.9738\n",
            "Epoch 293/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8478 - val_loss: 0.9625\n",
            "Epoch 294/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8426 - val_loss: 1.0060\n",
            "Epoch 295/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8767 - val_loss: 0.9985\n",
            "Epoch 296/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8675 - val_loss: 0.9808\n",
            "Epoch 297/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8648 - val_loss: 0.9885\n",
            "Epoch 298/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8680 - val_loss: 0.9401\n",
            "Epoch 299/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8639 - val_loss: 0.9786\n",
            "Epoch 300/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8789 - val_loss: 0.9691\n",
            "Epoch 301/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8618 - val_loss: 0.9849\n",
            "Epoch 302/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8385 - val_loss: 1.0260\n",
            "Epoch 303/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8603 - val_loss: 0.9984\n",
            "Epoch 304/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8358 - val_loss: 1.0454\n",
            "Epoch 305/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8704 - val_loss: 0.9444\n",
            "Epoch 306/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8608 - val_loss: 0.9687\n",
            "Epoch 307/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8388 - val_loss: 0.9709\n",
            "Epoch 308/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8258 - val_loss: 0.9786\n",
            "Epoch 309/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8436 - val_loss: 0.9716\n",
            "Epoch 310/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8628 - val_loss: 0.9938\n",
            "Epoch 311/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8568 - val_loss: 0.9938\n",
            "Epoch 312/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8734 - val_loss: 0.9283\n",
            "Epoch 313/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8193 - val_loss: 0.9841\n",
            "Epoch 314/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8693 - val_loss: 0.9678\n",
            "Epoch 315/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8712 - val_loss: 0.9679\n",
            "Epoch 316/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8547 - val_loss: 0.9315\n",
            "Epoch 317/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8696 - val_loss: 0.9860\n",
            "Epoch 318/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8467 - val_loss: 0.8935\n",
            "Epoch 319/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8799 - val_loss: 0.9533\n",
            "Epoch 320/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8338 - val_loss: 0.9319\n",
            "Epoch 321/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8452 - val_loss: 0.9590\n",
            "Epoch 322/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8446 - val_loss: 0.9350\n",
            "Epoch 323/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8309 - val_loss: 0.9012\n",
            "Epoch 324/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8551 - val_loss: 0.9369\n",
            "Epoch 325/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8420 - val_loss: 0.9839\n",
            "Epoch 326/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8458 - val_loss: 0.9271\n",
            "Epoch 327/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8451 - val_loss: 0.9648\n",
            "Epoch 328/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8656 - val_loss: 0.9189\n",
            "Epoch 329/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8323 - val_loss: 0.9477\n",
            "Epoch 330/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8192 - val_loss: 0.9559\n",
            "Epoch 331/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8596 - val_loss: 0.9552\n",
            "Epoch 332/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8342 - val_loss: 0.9333\n",
            "Epoch 333/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8360 - val_loss: 0.8902\n",
            "Epoch 334/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8505 - val_loss: 0.9116\n",
            "Epoch 335/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8295 - val_loss: 0.9231\n",
            "Epoch 336/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8529 - val_loss: 0.9607\n",
            "Epoch 337/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8230 - val_loss: 0.8956\n",
            "Epoch 338/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8331 - val_loss: 0.8907\n",
            "Epoch 339/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8580 - val_loss: 0.9155\n",
            "Epoch 340/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8440 - val_loss: 0.9351\n",
            "Epoch 341/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8516 - val_loss: 0.8967\n",
            "Epoch 342/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7955 - val_loss: 0.9486\n",
            "Epoch 343/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8162 - val_loss: 0.9609\n",
            "Epoch 344/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8110 - val_loss: 0.9238\n",
            "Epoch 345/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8339 - val_loss: 0.9712\n",
            "Epoch 346/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8278 - val_loss: 0.9183\n",
            "Epoch 347/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8425 - val_loss: 0.8887\n",
            "Epoch 348/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8270 - val_loss: 0.9714\n",
            "Epoch 349/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8188 - val_loss: 0.8855\n",
            "Epoch 350/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8328 - val_loss: 0.9391\n",
            "Epoch 351/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7979 - val_loss: 0.8974\n",
            "Epoch 352/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8307 - val_loss: 0.9400\n",
            "Epoch 353/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8523 - val_loss: 0.9048\n",
            "Epoch 354/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8195 - val_loss: 0.9499\n",
            "Epoch 355/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8203 - val_loss: 0.9293\n",
            "Epoch 356/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8367 - val_loss: 0.9364\n",
            "Epoch 357/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8177 - val_loss: 0.9199\n",
            "Epoch 358/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8285 - val_loss: 0.9309\n",
            "Epoch 359/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8218 - val_loss: 0.9244\n",
            "Epoch 360/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8369 - val_loss: 0.9386\n",
            "Epoch 361/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8034 - val_loss: 0.9599\n",
            "Epoch 362/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8086 - val_loss: 0.9210\n",
            "Epoch 363/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8206 - val_loss: 0.8894\n",
            "Epoch 364/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8209 - val_loss: 0.9949\n",
            "Epoch 365/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8203 - val_loss: 0.8845\n",
            "Epoch 366/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8268 - val_loss: 0.8698\n",
            "Epoch 367/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8346 - val_loss: 0.9556\n",
            "Epoch 368/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8347 - val_loss: 0.9423\n",
            "Epoch 369/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8197 - val_loss: 0.9350\n",
            "Epoch 370/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7943 - val_loss: 0.9019\n",
            "Epoch 371/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8253 - val_loss: 0.8497\n",
            "Epoch 372/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7967 - val_loss: 0.9146\n",
            "Epoch 373/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8028 - val_loss: 0.8804\n",
            "Epoch 374/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8031 - val_loss: 0.8555\n",
            "Epoch 375/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8321 - val_loss: 0.8700\n",
            "Epoch 376/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8022 - val_loss: 0.8911\n",
            "Epoch 377/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8149 - val_loss: 0.8977\n",
            "Epoch 378/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8408 - val_loss: 0.8704\n",
            "Epoch 379/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7936 - val_loss: 0.8738\n",
            "Epoch 380/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8396 - val_loss: 0.9100\n",
            "Epoch 381/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7873 - val_loss: 0.8834\n",
            "Epoch 382/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8254 - val_loss: 0.8884\n",
            "Epoch 383/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7943 - val_loss: 0.8936\n",
            "Epoch 384/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8163 - val_loss: 0.9136\n",
            "Epoch 385/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8032 - val_loss: 0.8361\n",
            "Epoch 386/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8156 - val_loss: 0.8839\n",
            "Epoch 387/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7948 - val_loss: 0.8972\n",
            "Epoch 388/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7841 - val_loss: 0.9062\n",
            "Epoch 389/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8149 - val_loss: 0.9282\n",
            "Epoch 390/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8417 - val_loss: 0.8730\n",
            "Epoch 391/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8138 - val_loss: 0.9232\n",
            "Epoch 392/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.8285 - val_loss: 0.8911\n",
            "Epoch 393/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8218 - val_loss: 0.8203\n",
            "Epoch 394/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.8270 - val_loss: 0.8618\n",
            "Epoch 395/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7887 - val_loss: 0.8735\n",
            "Epoch 396/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8077 - val_loss: 0.9402\n",
            "Epoch 397/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8107 - val_loss: 0.8856\n",
            "Epoch 398/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7890 - val_loss: 0.8369\n",
            "Epoch 399/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.8161 - val_loss: 0.8857\n",
            "Epoch 400/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7963 - val_loss: 0.8827\n",
            "Epoch 401/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7889 - val_loss: 0.8622\n",
            "Epoch 402/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7933 - val_loss: 0.8763\n",
            "Epoch 403/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7888 - val_loss: 0.8937\n",
            "Epoch 404/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7923 - val_loss: 0.9021\n",
            "Epoch 405/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7793 - val_loss: 0.9051\n",
            "Epoch 406/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7989 - val_loss: 0.8896\n",
            "Epoch 407/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7706 - val_loss: 0.8601\n",
            "Epoch 408/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7728 - val_loss: 0.8677\n",
            "Epoch 409/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7839 - val_loss: 0.9073\n",
            "Epoch 410/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7710 - val_loss: 0.8824\n",
            "Epoch 411/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7767 - val_loss: 0.9213\n",
            "Epoch 412/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7927 - val_loss: 0.8728\n",
            "Epoch 413/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7705 - val_loss: 0.9128\n",
            "Epoch 414/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7832 - val_loss: 0.8879\n",
            "Epoch 415/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7960 - val_loss: 0.8582\n",
            "Epoch 416/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7485 - val_loss: 0.9252\n",
            "Epoch 417/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7784 - val_loss: 0.9014\n",
            "Epoch 418/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7842 - val_loss: 0.8827\n",
            "Epoch 419/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7703 - val_loss: 0.8917\n",
            "Epoch 420/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7644 - val_loss: 0.8810\n",
            "Epoch 421/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7962 - val_loss: 0.9386\n",
            "Epoch 422/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7779 - val_loss: 0.8603\n",
            "Epoch 423/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7979 - val_loss: 0.8920\n",
            "Epoch 424/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7909 - val_loss: 0.8656\n",
            "Epoch 425/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.8015 - val_loss: 0.9106\n",
            "Epoch 426/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7651 - val_loss: 0.8556\n",
            "Epoch 427/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7603 - val_loss: 0.8601\n",
            "Epoch 428/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7793 - val_loss: 0.9033\n",
            "Epoch 429/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7646 - val_loss: 0.8349\n",
            "Epoch 430/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7910 - val_loss: 0.8380\n",
            "Epoch 431/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7782 - val_loss: 0.8512\n",
            "Epoch 432/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7644 - val_loss: 0.8797\n",
            "Epoch 433/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7593 - val_loss: 0.8930\n",
            "Epoch 434/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7953 - val_loss: 0.9066\n",
            "Epoch 435/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7851 - val_loss: 0.8544\n",
            "Epoch 436/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7759 - val_loss: 0.8514\n",
            "Epoch 437/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7919 - val_loss: 0.8832\n",
            "Epoch 438/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7635 - val_loss: 0.8709\n",
            "Epoch 439/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7507 - val_loss: 0.8959\n",
            "Epoch 440/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7458 - val_loss: 0.8996\n",
            "Epoch 441/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7654 - val_loss: 0.8393\n",
            "Epoch 442/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7773 - val_loss: 0.8819\n",
            "Epoch 443/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7557 - val_loss: 0.8679\n",
            "Epoch 444/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7833 - val_loss: 0.8598\n",
            "Epoch 445/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7767 - val_loss: 0.8532\n",
            "Epoch 446/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7498 - val_loss: 0.8841\n",
            "Epoch 447/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7423 - val_loss: 0.8835\n",
            "Epoch 448/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7678 - val_loss: 0.8548\n",
            "Epoch 449/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7976 - val_loss: 0.8912\n",
            "Epoch 450/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7571 - val_loss: 0.8682\n",
            "Epoch 451/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7699 - val_loss: 0.8231\n",
            "Epoch 452/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7575 - val_loss: 0.8733\n",
            "Epoch 453/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7536 - val_loss: 0.8256\n",
            "Epoch 454/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7447 - val_loss: 0.8640\n",
            "Epoch 455/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7714 - val_loss: 0.8326\n",
            "Epoch 456/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7803 - val_loss: 0.9077\n",
            "Epoch 457/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7481 - val_loss: 0.8340\n",
            "Epoch 458/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7659 - val_loss: 0.8876\n",
            "Epoch 459/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7674 - val_loss: 0.8813\n",
            "Epoch 460/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7393 - val_loss: 0.8736\n",
            "Epoch 461/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7528 - val_loss: 0.9193\n",
            "Epoch 462/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7605 - val_loss: 0.8365\n",
            "Epoch 463/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7572 - val_loss: 0.8425\n",
            "Epoch 464/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7424 - val_loss: 0.9135\n",
            "Epoch 465/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7605 - val_loss: 0.8647\n",
            "Epoch 466/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7593 - val_loss: 0.8677\n",
            "Epoch 467/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7421 - val_loss: 0.8358\n",
            "Epoch 468/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7448 - val_loss: 0.9230\n",
            "Epoch 469/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7820 - val_loss: 0.8440\n",
            "Epoch 470/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7599 - val_loss: 0.8297\n",
            "Epoch 471/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7372 - val_loss: 0.7838\n",
            "Epoch 472/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7431 - val_loss: 0.8672\n",
            "Epoch 473/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7764 - val_loss: 0.8207\n",
            "Epoch 474/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7376 - val_loss: 0.8380\n",
            "Epoch 475/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7418 - val_loss: 0.8485\n",
            "Epoch 476/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7126 - val_loss: 0.8547\n",
            "Epoch 477/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7478 - val_loss: 0.8173\n",
            "Epoch 478/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7583 - val_loss: 0.8734\n",
            "Epoch 479/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7474 - val_loss: 0.9584\n",
            "Epoch 480/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7653 - val_loss: 0.8136\n",
            "Epoch 481/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7342 - val_loss: 0.8546\n",
            "Epoch 482/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7360 - val_loss: 0.8408\n",
            "Epoch 483/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7283 - val_loss: 0.8659\n",
            "Epoch 484/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7330 - val_loss: 0.8709\n",
            "Epoch 485/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7510 - val_loss: 0.8697\n",
            "Epoch 486/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7557 - val_loss: 0.8503\n",
            "Epoch 487/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7285 - val_loss: 0.8689\n",
            "Epoch 488/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7698 - val_loss: 0.8583\n",
            "Epoch 489/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7547 - val_loss: 0.8567\n",
            "Epoch 490/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7417 - val_loss: 0.8306\n",
            "Epoch 491/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7590 - val_loss: 0.8360\n",
            "Epoch 492/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7690 - val_loss: 0.8194\n",
            "Epoch 493/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7297 - val_loss: 0.8179\n",
            "Epoch 494/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7448 - val_loss: 0.8977\n",
            "Epoch 495/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7295 - val_loss: 0.8401\n",
            "Epoch 496/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7263 - val_loss: 0.8222\n",
            "Epoch 497/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7132 - val_loss: 0.8252\n",
            "Epoch 498/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7417 - val_loss: 0.8622\n",
            "Epoch 499/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7560 - val_loss: 0.8269\n",
            "Epoch 500/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7335 - val_loss: 0.8623\n",
            "Epoch 501/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7716 - val_loss: 0.8698\n",
            "Epoch 502/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7263 - val_loss: 0.9103\n",
            "Epoch 503/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7372 - val_loss: 0.9111\n",
            "Epoch 504/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7195 - val_loss: 0.8126\n",
            "Epoch 505/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7139 - val_loss: 0.7996\n",
            "Epoch 506/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7532 - val_loss: 0.8574\n",
            "Epoch 507/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7457 - val_loss: 0.8022\n",
            "Epoch 508/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7371 - val_loss: 0.8793\n",
            "Epoch 509/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7475 - val_loss: 0.7848\n",
            "Epoch 510/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7456 - val_loss: 0.8515\n",
            "Epoch 511/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7477 - val_loss: 0.8635\n",
            "Epoch 512/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7326 - val_loss: 0.8289\n",
            "Epoch 513/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7542 - val_loss: 0.8279\n",
            "Epoch 514/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7336 - val_loss: 0.8125\n",
            "Epoch 515/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7256 - val_loss: 0.8257\n",
            "Epoch 516/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7457 - val_loss: 0.8804\n",
            "Epoch 517/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7431 - val_loss: 0.8422\n",
            "Epoch 518/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7206 - val_loss: 0.8443\n",
            "Epoch 519/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7304 - val_loss: 0.8982\n",
            "Epoch 520/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7242 - val_loss: 0.8348\n",
            "Epoch 521/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7607 - val_loss: 0.8266\n",
            "Epoch 522/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7129 - val_loss: 0.7994\n",
            "Epoch 523/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7457 - val_loss: 0.8855\n",
            "Epoch 524/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7208 - val_loss: 0.8582\n",
            "Epoch 525/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7357 - val_loss: 0.8300\n",
            "Epoch 526/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7239 - val_loss: 0.8388\n",
            "Epoch 527/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7369 - val_loss: 0.8624\n",
            "Epoch 528/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7425 - val_loss: 0.8536\n",
            "Epoch 529/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7544 - val_loss: 0.8934\n",
            "Epoch 530/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7374 - val_loss: 0.8429\n",
            "Epoch 531/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7389 - val_loss: 0.8409\n",
            "Epoch 532/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7153 - val_loss: 0.7631\n",
            "Epoch 533/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7246 - val_loss: 0.8275\n",
            "Epoch 534/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7513 - val_loss: 0.7973\n",
            "Epoch 535/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7367 - val_loss: 0.8498\n",
            "Epoch 536/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7043 - val_loss: 0.8453\n",
            "Epoch 537/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7322 - val_loss: 0.8110\n",
            "Epoch 538/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7322 - val_loss: 0.8242\n",
            "Epoch 539/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7272 - val_loss: 0.8421\n",
            "Epoch 540/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6896 - val_loss: 0.8151\n",
            "Epoch 541/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7274 - val_loss: 0.8907\n",
            "Epoch 542/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7110 - val_loss: 0.8196\n",
            "Epoch 543/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.7186 - val_loss: 0.8572\n",
            "Epoch 544/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7211 - val_loss: 0.7956\n",
            "Epoch 545/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7333 - val_loss: 0.7694\n",
            "Epoch 546/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7187 - val_loss: 0.8209\n",
            "Epoch 547/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7314 - val_loss: 0.8332\n",
            "Epoch 548/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6996 - val_loss: 0.8086\n",
            "Epoch 549/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7317 - val_loss: 0.8158\n",
            "Epoch 550/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7161 - val_loss: 0.7875\n",
            "Epoch 551/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7083 - val_loss: 0.8109\n",
            "Epoch 552/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7011 - val_loss: 0.7562\n",
            "Epoch 553/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6962 - val_loss: 0.8410\n",
            "Epoch 554/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7103 - val_loss: 0.8720\n",
            "Epoch 555/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7125 - val_loss: 0.8287\n",
            "Epoch 556/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7115 - val_loss: 0.8188\n",
            "Epoch 557/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7226 - val_loss: 0.8286\n",
            "Epoch 558/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7550 - val_loss: 0.8406\n",
            "Epoch 559/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7016 - val_loss: 0.8686\n",
            "Epoch 560/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7260 - val_loss: 0.8132\n",
            "Epoch 561/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7232 - val_loss: 0.8301\n",
            "Epoch 562/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6942 - val_loss: 0.8247\n",
            "Epoch 563/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6821 - val_loss: 0.8281\n",
            "Epoch 564/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7372 - val_loss: 0.8459\n",
            "Epoch 565/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7155 - val_loss: 0.8234\n",
            "Epoch 566/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7284 - val_loss: 0.8476\n",
            "Epoch 567/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7192 - val_loss: 0.8231\n",
            "Epoch 568/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6922 - val_loss: 0.8141\n",
            "Epoch 569/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6803 - val_loss: 0.8342\n",
            "Epoch 570/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7101 - val_loss: 0.7942\n",
            "Epoch 571/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7179 - val_loss: 0.8230\n",
            "Epoch 572/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6999 - val_loss: 0.8223\n",
            "Epoch 573/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7127 - val_loss: 0.8134\n",
            "Epoch 574/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7094 - val_loss: 0.7936\n",
            "Epoch 575/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7020 - val_loss: 0.8298\n",
            "Epoch 576/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6921 - val_loss: 0.8029\n",
            "Epoch 577/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7009 - val_loss: 0.8631\n",
            "Epoch 578/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7089 - val_loss: 0.8606\n",
            "Epoch 579/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7034 - val_loss: 0.8188\n",
            "Epoch 580/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6864 - val_loss: 0.8057\n",
            "Epoch 581/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7229 - val_loss: 0.8079\n",
            "Epoch 582/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7135 - val_loss: 0.8052\n",
            "Epoch 583/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6996 - val_loss: 0.8536\n",
            "Epoch 584/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6953 - val_loss: 0.7828\n",
            "Epoch 585/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6783 - val_loss: 0.8065\n",
            "Epoch 586/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7212 - val_loss: 0.8043\n",
            "Epoch 587/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6959 - val_loss: 0.8001\n",
            "Epoch 588/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7042 - val_loss: 0.8565\n",
            "Epoch 589/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6846 - val_loss: 0.8040\n",
            "Epoch 590/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6669 - val_loss: 0.8311\n",
            "Epoch 591/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7056 - val_loss: 0.7758\n",
            "Epoch 592/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7010 - val_loss: 0.8356\n",
            "Epoch 593/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6699 - val_loss: 0.8742\n",
            "Epoch 594/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7116 - val_loss: 0.7760\n",
            "Epoch 595/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7059 - val_loss: 0.7796\n",
            "Epoch 596/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6965 - val_loss: 0.8001\n",
            "Epoch 597/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7087 - val_loss: 0.8168\n",
            "Epoch 598/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6857 - val_loss: 0.8202\n",
            "Epoch 599/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6926 - val_loss: 0.7969\n",
            "Epoch 600/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7156 - val_loss: 0.8289\n",
            "Epoch 601/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6972 - val_loss: 0.7564\n",
            "Epoch 602/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6858 - val_loss: 0.8018\n",
            "Epoch 603/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6841 - val_loss: 0.8293\n",
            "Epoch 604/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6841 - val_loss: 0.8198\n",
            "Epoch 605/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7026 - val_loss: 0.8253\n",
            "Epoch 606/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6928 - val_loss: 0.7989\n",
            "Epoch 607/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6846 - val_loss: 0.8274\n",
            "Epoch 608/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6957 - val_loss: 0.8650\n",
            "Epoch 609/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6751 - val_loss: 0.8344\n",
            "Epoch 610/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7164 - val_loss: 0.7798\n",
            "Epoch 611/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6983 - val_loss: 0.8619\n",
            "Epoch 612/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.7175 - val_loss: 0.8382\n",
            "Epoch 613/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6582 - val_loss: 0.8451\n",
            "Epoch 614/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6962 - val_loss: 0.7880\n",
            "Epoch 615/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7211 - val_loss: 0.8303\n",
            "Epoch 616/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6851 - val_loss: 0.8503\n",
            "Epoch 617/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6772 - val_loss: 0.8329\n",
            "Epoch 618/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6994 - val_loss: 0.8178\n",
            "Epoch 619/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.7055 - val_loss: 0.8035\n",
            "Epoch 620/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6955 - val_loss: 0.7860\n",
            "Epoch 621/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6850 - val_loss: 0.8130\n",
            "Epoch 622/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6908 - val_loss: 0.7861\n",
            "Epoch 623/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6674 - val_loss: 0.7921\n",
            "Epoch 624/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7065 - val_loss: 0.8376\n",
            "Epoch 625/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6697 - val_loss: 0.7752\n",
            "Epoch 626/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6422 - val_loss: 0.7806\n",
            "Epoch 627/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6783 - val_loss: 0.8003\n",
            "Epoch 628/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6812 - val_loss: 0.8163\n",
            "Epoch 629/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6853 - val_loss: 0.8109\n",
            "Epoch 630/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6815 - val_loss: 0.7580\n",
            "Epoch 631/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6727 - val_loss: 0.7947\n",
            "Epoch 632/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6826 - val_loss: 0.7846\n",
            "Epoch 633/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6786 - val_loss: 0.7810\n",
            "Epoch 634/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6814 - val_loss: 0.7910\n",
            "Epoch 635/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6874 - val_loss: 0.8770\n",
            "Epoch 636/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6824 - val_loss: 0.7838\n",
            "Epoch 637/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6632 - val_loss: 0.8127\n",
            "Epoch 638/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6594 - val_loss: 0.7912\n",
            "Epoch 639/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6634 - val_loss: 0.8074\n",
            "Epoch 640/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6656 - val_loss: 0.7867\n",
            "Epoch 641/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6918 - val_loss: 0.7712\n",
            "Epoch 642/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6796 - val_loss: 0.8091\n",
            "Epoch 643/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6741 - val_loss: 0.8026\n",
            "Epoch 644/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6843 - val_loss: 0.7962\n",
            "Epoch 645/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6334 - val_loss: 0.7972\n",
            "Epoch 646/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7031 - val_loss: 0.8567\n",
            "Epoch 647/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6578 - val_loss: 0.8116\n",
            "Epoch 648/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6672 - val_loss: 0.8351\n",
            "Epoch 649/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6680 - val_loss: 0.7667\n",
            "Epoch 650/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6803 - val_loss: 0.7891\n",
            "Epoch 651/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6685 - val_loss: 0.7754\n",
            "Epoch 652/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6888 - val_loss: 0.7628\n",
            "Epoch 653/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6899 - val_loss: 0.7758\n",
            "Epoch 654/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6968 - val_loss: 0.7832\n",
            "Epoch 655/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6953 - val_loss: 0.8062\n",
            "Epoch 656/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6647 - val_loss: 0.8295\n",
            "Epoch 657/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6607 - val_loss: 0.7876\n",
            "Epoch 658/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6730 - val_loss: 0.7837\n",
            "Epoch 659/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6575 - val_loss: 0.7832\n",
            "Epoch 660/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6752 - val_loss: 0.7879\n",
            "Epoch 661/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6750 - val_loss: 0.8452\n",
            "Epoch 662/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6821 - val_loss: 0.8325\n",
            "Epoch 663/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6665 - val_loss: 0.8180\n",
            "Epoch 664/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6643 - val_loss: 0.8270\n",
            "Epoch 665/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6843 - val_loss: 0.7870\n",
            "Epoch 666/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6524 - val_loss: 0.8041\n",
            "Epoch 667/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6900 - val_loss: 0.7587\n",
            "Epoch 668/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6530 - val_loss: 0.8370\n",
            "Epoch 669/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6694 - val_loss: 0.7915\n",
            "Epoch 670/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6668 - val_loss: 0.8340\n",
            "Epoch 671/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6534 - val_loss: 0.8140\n",
            "Epoch 672/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6611 - val_loss: 0.8088\n",
            "Epoch 673/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6545 - val_loss: 0.8300\n",
            "Epoch 674/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6696 - val_loss: 0.8118\n",
            "Epoch 675/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6738 - val_loss: 0.8060\n",
            "Epoch 676/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6667 - val_loss: 0.8075\n",
            "Epoch 677/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6938 - val_loss: 0.7671\n",
            "Epoch 678/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6821 - val_loss: 0.8433\n",
            "Epoch 679/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6725 - val_loss: 0.7892\n",
            "Epoch 680/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6730 - val_loss: 0.8279\n",
            "Epoch 681/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6656 - val_loss: 0.7960\n",
            "Epoch 682/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6529 - val_loss: 0.7682\n",
            "Epoch 683/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6927 - val_loss: 0.7222\n",
            "Epoch 684/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6587 - val_loss: 0.8401\n",
            "Epoch 685/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7006 - val_loss: 0.7868\n",
            "Epoch 686/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6423 - val_loss: 0.7673\n",
            "Epoch 687/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6372 - val_loss: 0.8195\n",
            "Epoch 688/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6689 - val_loss: 0.8336\n",
            "Epoch 689/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6587 - val_loss: 0.8067\n",
            "Epoch 690/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6674 - val_loss: 0.7986\n",
            "Epoch 691/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6794 - val_loss: 0.8093\n",
            "Epoch 692/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6291 - val_loss: 0.7936\n",
            "Epoch 693/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6604 - val_loss: 0.7569\n",
            "Epoch 694/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6579 - val_loss: 0.7752\n",
            "Epoch 695/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6637 - val_loss: 0.7168\n",
            "Epoch 696/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6633 - val_loss: 0.8077\n",
            "Epoch 697/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6479 - val_loss: 0.8218\n",
            "Epoch 698/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6700 - val_loss: 0.7947\n",
            "Epoch 699/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6533 - val_loss: 0.7884\n",
            "Epoch 700/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6420 - val_loss: 0.7772\n",
            "Epoch 701/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6788 - val_loss: 0.7753\n",
            "Epoch 702/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6556 - val_loss: 0.7903\n",
            "Epoch 703/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6557 - val_loss: 0.7986\n",
            "Epoch 704/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6630 - val_loss: 0.8302\n",
            "Epoch 705/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6603 - val_loss: 0.8416\n",
            "Epoch 706/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6539 - val_loss: 0.7955\n",
            "Epoch 707/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6520 - val_loss: 0.8092\n",
            "Epoch 708/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6599 - val_loss: 0.7270\n",
            "Epoch 709/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6424 - val_loss: 0.7714\n",
            "Epoch 710/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6652 - val_loss: 0.8032\n",
            "Epoch 711/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6311 - val_loss: 0.7823\n",
            "Epoch 712/1000\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.6715 - val_loss: 0.7810\n",
            "Epoch 713/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6672 - val_loss: 0.7508\n",
            "Epoch 714/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6538 - val_loss: 0.7266\n",
            "Epoch 715/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6669 - val_loss: 0.7608\n",
            "Epoch 716/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6572 - val_loss: 0.7709\n",
            "Epoch 717/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6282 - val_loss: 0.8160\n",
            "Epoch 718/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6306 - val_loss: 0.8183\n",
            "Epoch 719/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6417 - val_loss: 0.8592\n",
            "Epoch 720/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6548 - val_loss: 0.8177\n",
            "Epoch 721/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6578 - val_loss: 0.8370\n",
            "Epoch 722/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6884 - val_loss: 0.7482\n",
            "Epoch 723/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6313 - val_loss: 0.8241\n",
            "Epoch 724/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6163 - val_loss: 0.7505\n",
            "Epoch 725/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6254 - val_loss: 0.8394\n",
            "Epoch 726/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6427 - val_loss: 0.8142\n",
            "Epoch 727/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6363 - val_loss: 0.7798\n",
            "Epoch 728/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6558 - val_loss: 0.7975\n",
            "Epoch 729/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6545 - val_loss: 0.8165\n",
            "Epoch 730/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6449 - val_loss: 0.8042\n",
            "Epoch 731/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6475 - val_loss: 0.7799\n",
            "Epoch 732/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6427 - val_loss: 0.7728\n",
            "Epoch 733/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6390 - val_loss: 0.7964\n",
            "Epoch 734/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6493 - val_loss: 0.8024\n",
            "Epoch 735/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6344 - val_loss: 0.7544\n",
            "Epoch 736/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6404 - val_loss: 0.7868\n",
            "Epoch 737/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6306 - val_loss: 0.8257\n",
            "Epoch 738/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6569 - val_loss: 0.7485\n",
            "Epoch 739/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6351 - val_loss: 0.7933\n",
            "Epoch 740/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6518 - val_loss: 0.8000\n",
            "Epoch 741/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6648 - val_loss: 0.7728\n",
            "Epoch 742/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6486 - val_loss: 0.8352\n",
            "Epoch 743/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6318 - val_loss: 0.7748\n",
            "Epoch 744/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6482 - val_loss: 0.7777\n",
            "Epoch 745/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6607 - val_loss: 0.8018\n",
            "Epoch 746/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6356 - val_loss: 0.7554\n",
            "Epoch 747/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6301 - val_loss: 0.8160\n",
            "Epoch 748/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6431 - val_loss: 0.7330\n",
            "Epoch 749/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6445 - val_loss: 0.7739\n",
            "Epoch 750/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6347 - val_loss: 0.8064\n",
            "Epoch 751/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6385 - val_loss: 0.7253\n",
            "Epoch 752/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6470 - val_loss: 0.7915\n",
            "Epoch 753/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6564 - val_loss: 0.7829\n",
            "Epoch 754/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6476 - val_loss: 0.7434\n",
            "Epoch 755/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6479 - val_loss: 0.8075\n",
            "Epoch 756/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6579 - val_loss: 0.7620\n",
            "Epoch 757/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6480 - val_loss: 0.7229\n",
            "Epoch 758/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6658 - val_loss: 0.7825\n",
            "Epoch 759/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6602 - val_loss: 0.7795\n",
            "Epoch 760/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6005 - val_loss: 0.7622\n",
            "Epoch 761/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6473 - val_loss: 0.7250\n",
            "Epoch 762/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6127 - val_loss: 0.7223\n",
            "Epoch 763/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6748 - val_loss: 0.7952\n",
            "Epoch 764/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6448 - val_loss: 0.7622\n",
            "Epoch 765/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6449 - val_loss: 0.6988\n",
            "Epoch 766/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6490 - val_loss: 0.7564\n",
            "Epoch 767/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6381 - val_loss: 0.8445\n",
            "Epoch 768/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6526 - val_loss: 0.7524\n",
            "Epoch 769/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6312 - val_loss: 0.7667\n",
            "Epoch 770/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6380 - val_loss: 0.7557\n",
            "Epoch 771/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6468 - val_loss: 0.7492\n",
            "Epoch 772/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6564 - val_loss: 0.7943\n",
            "Epoch 773/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6130 - val_loss: 0.7846\n",
            "Epoch 774/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6543 - val_loss: 0.7427\n",
            "Epoch 775/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6368 - val_loss: 0.8167\n",
            "Epoch 776/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6396 - val_loss: 0.7441\n",
            "Epoch 777/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6413 - val_loss: 0.7544\n",
            "Epoch 778/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6242 - val_loss: 0.7857\n",
            "Epoch 779/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6077 - val_loss: 0.7645\n",
            "Epoch 780/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6427 - val_loss: 0.7494\n",
            "Epoch 781/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6442 - val_loss: 0.8313\n",
            "Epoch 782/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6338 - val_loss: 0.7544\n",
            "Epoch 783/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6309 - val_loss: 0.7808\n",
            "Epoch 784/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6462 - val_loss: 0.7879\n",
            "Epoch 785/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6418 - val_loss: 0.7881\n",
            "Epoch 786/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6176 - val_loss: 0.8147\n",
            "Epoch 787/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6541 - val_loss: 0.7872\n",
            "Epoch 788/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6269 - val_loss: 0.7935\n",
            "Epoch 789/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6490 - val_loss: 0.7716\n",
            "Epoch 790/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6525 - val_loss: 0.7888\n",
            "Epoch 791/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6352 - val_loss: 0.7545\n",
            "Epoch 792/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6276 - val_loss: 0.7969\n",
            "Epoch 793/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6567 - val_loss: 0.7443\n",
            "Epoch 794/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6344 - val_loss: 0.7607\n",
            "Epoch 795/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6453 - val_loss: 0.7648\n",
            "Epoch 796/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6437 - val_loss: 0.7722\n",
            "Epoch 797/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6272 - val_loss: 0.7963\n",
            "Epoch 798/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6362 - val_loss: 0.8159\n",
            "Epoch 799/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6225 - val_loss: 0.7607\n",
            "Epoch 800/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6138 - val_loss: 0.7684\n",
            "Epoch 801/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6083 - val_loss: 0.8016\n",
            "Epoch 802/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6289 - val_loss: 0.7321\n",
            "Epoch 803/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6306 - val_loss: 0.7492\n",
            "Epoch 804/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6029 - val_loss: 0.7482\n",
            "Epoch 805/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6355 - val_loss: 0.7491\n",
            "Epoch 806/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6240 - val_loss: 0.7808\n",
            "Epoch 807/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6124 - val_loss: 0.7426\n",
            "Epoch 808/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6370 - val_loss: 0.7401\n",
            "Epoch 809/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6340 - val_loss: 0.7827\n",
            "Epoch 810/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6116 - val_loss: 0.7830\n",
            "Epoch 811/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6261 - val_loss: 0.7715\n",
            "Epoch 812/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6375 - val_loss: 0.7512\n",
            "Epoch 813/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6310 - val_loss: 0.8343\n",
            "Epoch 814/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6186 - val_loss: 0.7900\n",
            "Epoch 815/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6315 - val_loss: 0.7324\n",
            "Epoch 816/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6225 - val_loss: 0.7500\n",
            "Epoch 817/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6020 - val_loss: 0.7939\n",
            "Epoch 818/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6563 - val_loss: 0.7713\n",
            "Epoch 819/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6281 - val_loss: 0.7699\n",
            "Epoch 820/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6180 - val_loss: 0.7388\n",
            "Epoch 821/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6229 - val_loss: 0.7701\n",
            "Epoch 822/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6260 - val_loss: 0.7599\n",
            "Epoch 823/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6206 - val_loss: 0.7983\n",
            "Epoch 824/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6057 - val_loss: 0.7464\n",
            "Epoch 825/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6308 - val_loss: 0.7427\n",
            "Epoch 826/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6253 - val_loss: 0.7995\n",
            "Epoch 827/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6152 - val_loss: 0.7801\n",
            "Epoch 828/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6121 - val_loss: 0.7932\n",
            "Epoch 829/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6254 - val_loss: 0.7925\n",
            "Epoch 830/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6234 - val_loss: 0.7304\n",
            "Epoch 831/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6087 - val_loss: 0.7389\n",
            "Epoch 832/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6174 - val_loss: 0.7716\n",
            "Epoch 833/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6234 - val_loss: 0.7384\n",
            "Epoch 834/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6098 - val_loss: 0.7999\n",
            "Epoch 835/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6118 - val_loss: 0.7940\n",
            "Epoch 836/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6178 - val_loss: 0.8065\n",
            "Epoch 837/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6372 - val_loss: 0.7643\n",
            "Epoch 838/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6158 - val_loss: 0.7653\n",
            "Epoch 839/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6175 - val_loss: 0.8104\n",
            "Epoch 840/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6042 - val_loss: 0.7453\n",
            "Epoch 841/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6207 - val_loss: 0.7615\n",
            "Epoch 842/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6402 - val_loss: 0.7591\n",
            "Epoch 843/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6128 - val_loss: 0.7368\n",
            "Epoch 844/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6187 - val_loss: 0.7749\n",
            "Epoch 845/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6061 - val_loss: 0.7787\n",
            "Epoch 846/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6224 - val_loss: 0.7472\n",
            "Epoch 847/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6044 - val_loss: 0.7586\n",
            "Epoch 848/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5954 - val_loss: 0.7434\n",
            "Epoch 849/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6358 - val_loss: 0.8084\n",
            "Epoch 850/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6125 - val_loss: 0.7952\n",
            "Epoch 851/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6114 - val_loss: 0.8057\n",
            "Epoch 852/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5948 - val_loss: 0.7256\n",
            "Epoch 853/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6199 - val_loss: 0.7904\n",
            "Epoch 854/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6375 - val_loss: 0.7448\n",
            "Epoch 855/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5995 - val_loss: 0.7815\n",
            "Epoch 856/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5948 - val_loss: 0.7631\n",
            "Epoch 857/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5979 - val_loss: 0.7779\n",
            "Epoch 858/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6090 - val_loss: 0.8435\n",
            "Epoch 859/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6021 - val_loss: 0.7698\n",
            "Epoch 860/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6239 - val_loss: 0.7700\n",
            "Epoch 861/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6170 - val_loss: 0.7684\n",
            "Epoch 862/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5978 - val_loss: 0.7700\n",
            "Epoch 863/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6141 - val_loss: 0.7540\n",
            "Epoch 864/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6429 - val_loss: 0.7835\n",
            "Epoch 865/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5953 - val_loss: 0.8217\n",
            "Epoch 866/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6281 - val_loss: 0.7886\n",
            "Epoch 867/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6142 - val_loss: 0.8187\n",
            "Epoch 868/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6209 - val_loss: 0.7557\n",
            "Epoch 869/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6119 - val_loss: 0.7138\n",
            "Epoch 870/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6113 - val_loss: 0.7634\n",
            "Epoch 871/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6145 - val_loss: 0.7544\n",
            "Epoch 872/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6373 - val_loss: 0.7637\n",
            "Epoch 873/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6062 - val_loss: 0.7651\n",
            "Epoch 874/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5962 - val_loss: 0.7500\n",
            "Epoch 875/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5703 - val_loss: 0.8302\n",
            "Epoch 876/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6066 - val_loss: 0.7519\n",
            "Epoch 877/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6138 - val_loss: 0.7740\n",
            "Epoch 878/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6213 - val_loss: 0.7632\n",
            "Epoch 879/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5941 - val_loss: 0.7653\n",
            "Epoch 880/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5889 - val_loss: 0.7789\n",
            "Epoch 881/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5973 - val_loss: 0.7292\n",
            "Epoch 882/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5949 - val_loss: 0.7665\n",
            "Epoch 883/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6069 - val_loss: 0.7192\n",
            "Epoch 884/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6175 - val_loss: 0.7473\n",
            "Epoch 885/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5988 - val_loss: 0.7586\n",
            "Epoch 886/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6199 - val_loss: 0.7250\n",
            "Epoch 887/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6071 - val_loss: 0.6969\n",
            "Epoch 888/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6108 - val_loss: 0.7950\n",
            "Epoch 889/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6088 - val_loss: 0.7509\n",
            "Epoch 890/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6161 - val_loss: 0.7451\n",
            "Epoch 891/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6274 - val_loss: 0.7276\n",
            "Epoch 892/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6192 - val_loss: 0.7381\n",
            "Epoch 893/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5777 - val_loss: 0.7603\n",
            "Epoch 894/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6137 - val_loss: 0.7689\n",
            "Epoch 895/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5999 - val_loss: 0.7759\n",
            "Epoch 896/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6086 - val_loss: 0.7555\n",
            "Epoch 897/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5863 - val_loss: 0.7586\n",
            "Epoch 898/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6262 - val_loss: 0.7663\n",
            "Epoch 899/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5926 - val_loss: 0.7309\n",
            "Epoch 900/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6220 - val_loss: 0.7367\n",
            "Epoch 901/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6162 - val_loss: 0.7469\n",
            "Epoch 902/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6067 - val_loss: 0.7371\n",
            "Epoch 903/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6218 - val_loss: 0.7279\n",
            "Epoch 904/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5914 - val_loss: 0.7399\n",
            "Epoch 905/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6106 - val_loss: 0.7918\n",
            "Epoch 906/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5771 - val_loss: 0.7474\n",
            "Epoch 907/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5937 - val_loss: 0.7155\n",
            "Epoch 908/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6064 - val_loss: 0.7306\n",
            "Epoch 909/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5808 - val_loss: 0.7992\n",
            "Epoch 910/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5869 - val_loss: 0.7897\n",
            "Epoch 911/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5997 - val_loss: 0.7620\n",
            "Epoch 912/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5991 - val_loss: 0.7446\n",
            "Epoch 913/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6070 - val_loss: 0.7475\n",
            "Epoch 914/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5837 - val_loss: 0.6836\n",
            "Epoch 915/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5889 - val_loss: 0.7473\n",
            "Epoch 916/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5938 - val_loss: 0.7411\n",
            "Epoch 917/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6181 - val_loss: 0.7653\n",
            "Epoch 918/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5977 - val_loss: 0.7360\n",
            "Epoch 919/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5982 - val_loss: 0.7496\n",
            "Epoch 920/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5948 - val_loss: 0.7659\n",
            "Epoch 921/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.6025 - val_loss: 0.7501\n",
            "Epoch 922/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5955 - val_loss: 0.8027\n",
            "Epoch 923/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5664 - val_loss: 0.7684\n",
            "Epoch 924/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5879 - val_loss: 0.7784\n",
            "Epoch 925/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5930 - val_loss: 0.7375\n",
            "Epoch 926/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6064 - val_loss: 0.7539\n",
            "Epoch 927/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6034 - val_loss: 0.7111\n",
            "Epoch 928/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5889 - val_loss: 0.7641\n",
            "Epoch 929/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6097 - val_loss: 0.7217\n",
            "Epoch 930/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5953 - val_loss: 0.7079\n",
            "Epoch 931/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5987 - val_loss: 0.7407\n",
            "Epoch 932/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5954 - val_loss: 0.7427\n",
            "Epoch 933/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5921 - val_loss: 0.7347\n",
            "Epoch 934/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5806 - val_loss: 0.7769\n",
            "Epoch 935/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5870 - val_loss: 0.7204\n",
            "Epoch 936/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5938 - val_loss: 0.7399\n",
            "Epoch 937/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5978 - val_loss: 0.7540\n",
            "Epoch 938/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6019 - val_loss: 0.7388\n",
            "Epoch 939/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5894 - val_loss: 0.7563\n",
            "Epoch 940/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6100 - val_loss: 0.7500\n",
            "Epoch 941/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5787 - val_loss: 0.7171\n",
            "Epoch 942/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5765 - val_loss: 0.7334\n",
            "Epoch 943/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5707 - val_loss: 0.7317\n",
            "Epoch 944/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5944 - val_loss: 0.7284\n",
            "Epoch 945/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5949 - val_loss: 0.7623\n",
            "Epoch 946/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5915 - val_loss: 0.7516\n",
            "Epoch 947/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6007 - val_loss: 0.7313\n",
            "Epoch 948/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5867 - val_loss: 0.7555\n",
            "Epoch 949/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6069 - val_loss: 0.6971\n",
            "Epoch 950/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6001 - val_loss: 0.7648\n",
            "Epoch 951/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5850 - val_loss: 0.7180\n",
            "Epoch 952/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5868 - val_loss: 0.7312\n",
            "Epoch 953/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5719 - val_loss: 0.7704\n",
            "Epoch 954/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5851 - val_loss: 0.7893\n",
            "Epoch 955/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5822 - val_loss: 0.7814\n",
            "Epoch 956/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5886 - val_loss: 0.7271\n",
            "Epoch 957/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5859 - val_loss: 0.7418\n",
            "Epoch 958/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5748 - val_loss: 0.7118\n",
            "Epoch 959/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5792 - val_loss: 0.7522\n",
            "Epoch 960/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5881 - val_loss: 0.7559\n",
            "Epoch 961/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5866 - val_loss: 0.7912\n",
            "Epoch 962/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5965 - val_loss: 0.7340\n",
            "Epoch 963/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5816 - val_loss: 0.7709\n",
            "Epoch 964/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6124 - val_loss: 0.7806\n",
            "Epoch 965/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6005 - val_loss: 0.7499\n",
            "Epoch 966/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6022 - val_loss: 0.7475\n",
            "Epoch 967/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5753 - val_loss: 0.7252\n",
            "Epoch 968/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5915 - val_loss: 0.7031\n",
            "Epoch 969/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6097 - val_loss: 0.7274\n",
            "Epoch 970/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5830 - val_loss: 0.7056\n",
            "Epoch 971/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5902 - val_loss: 0.7230\n",
            "Epoch 972/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6142 - val_loss: 0.7468\n",
            "Epoch 973/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5732 - val_loss: 0.7701\n",
            "Epoch 974/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5793 - val_loss: 0.7200\n",
            "Epoch 975/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5860 - val_loss: 0.7455\n",
            "Epoch 976/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5801 - val_loss: 0.7144\n",
            "Epoch 977/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6090 - val_loss: 0.7358\n",
            "Epoch 978/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5958 - val_loss: 0.7026\n",
            "Epoch 979/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5676 - val_loss: 0.7782\n",
            "Epoch 980/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5632 - val_loss: 0.7449\n",
            "Epoch 981/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5774 - val_loss: 0.7070\n",
            "Epoch 982/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5895 - val_loss: 0.6948\n",
            "Epoch 983/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5659 - val_loss: 0.7335\n",
            "Epoch 984/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5914 - val_loss: 0.7391\n",
            "Epoch 985/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6072 - val_loss: 0.8105\n",
            "Epoch 986/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5890 - val_loss: 0.7368\n",
            "Epoch 987/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5598 - val_loss: 0.7174\n",
            "Epoch 988/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.5760 - val_loss: 0.7596\n",
            "Epoch 989/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5717 - val_loss: 0.7399\n",
            "Epoch 990/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5837 - val_loss: 0.6943\n",
            "Epoch 991/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.6138 - val_loss: 0.7286\n",
            "Epoch 992/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5866 - val_loss: 0.7441\n",
            "Epoch 993/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5702 - val_loss: 0.7815\n",
            "Epoch 994/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5902 - val_loss: 0.7178\n",
            "Epoch 995/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5560 - val_loss: 0.7026\n",
            "Epoch 996/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5695 - val_loss: 0.7133\n",
            "Epoch 997/1000\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 0.5720 - val_loss: 0.7147\n",
            "Epoch 998/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5593 - val_loss: 0.7106\n",
            "Epoch 999/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.5707 - val_loss: 0.7813\n",
            "Epoch 1000/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5741 - val_loss: 0.7234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "import time\n",
        "st = time.time()\n",
        "p1 = np.array(model(X_val)).flatten()\n",
        "end = time.time()\n",
        "# print(end, st, len(p1))\n",
        "print((end-st)/len(p1))\n",
        "p2 = np.array(model(X_train)).flatten()\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5rsGxienvgC",
        "outputId": "ecb08e10-9c92-480f-99fb-411d0e3b8ce3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00017973428749176393\n",
            "(1660, 1, 180)\n",
            "(415, 1, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import kendalltau\n",
        "# print(np.corrcoef(p2, Y_train), stats.spearmanr(p2, Y_train), kendalltau(p2,Y_train).correlation)\n",
        "print(np.corrcoef(p1, Y_val)[1][0], stats.spearmanr(p1, Y_val).correlation, kendalltau(p1,Y_val).correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jikF50FGnyFo",
        "outputId": "b836c239-4dd4-4f5e-d828-41bc3814e35a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7927345514994997 0.7836971771452386 0.5910736674393636\n"
          ]
        }
      ]
    }
  ]
}