{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jP5uwXtZay_",
        "outputId": "acf4e371-f756-4242-e427-a27da3929493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRPF7T0b-QZ",
        "outputId": "7db72a0f-2216-4687-95e2-1f82badc9db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spafe in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.11.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from spafe) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "!pip install -U spafe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "s2K5aSHYb-Sm"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-xNIFhPtb-U5"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QgbvNKYAb-YM"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet50,ResNet101\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.model_selection import StratifiedKFold , KFold ,RepeatedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LXsQV7ivcIcc"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  return tf.matmul(attention_weights, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7WB2_09_e-qt"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# This allows to the transformer to know where there is real data and where it is padded\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PjEQIR8TcIe3"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8hF4xYdLcSQA"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "j7ewC3iBcSSo"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_8IFloblcSWJ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder(time_steps,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            projection,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  if projection=='linear':\n",
        "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
        "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
        "    print('linear')\n",
        "\n",
        "  else:\n",
        "    projection=tf.identity(inputs)\n",
        "    print('none')\n",
        "\n",
        "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NXgJh2PpcIiK"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def transformer(time_steps,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                output_size,\n",
        "                projection,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(tf.dtypes.cast(\n",
        "\n",
        "      #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
        "      # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked\n",
        "      tf.math.reduce_sum(\n",
        "      inputs,\n",
        "      axis=2,\n",
        "      keepdims=False,\n",
        "      name=None\n",
        "  ), tf.int32))\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      time_steps=time_steps,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      projection=projection,\n",
        "      name='encoder'\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  #We reshape for feeding our FC in the next step\n",
        "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
        "\n",
        "  #We predict our class\n",
        "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True, name=\"outputs\")(outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9H-kkyuLcbPZ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# num_batch_size = 32\n",
        "# num_epochs = 500\n",
        "# N_SPLIT = 10\n",
        "# num_labels=5\n",
        "# num_classes=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Iw3KzecbRt",
        "outputId": "d46ebd37-49e2-4837-ed00-1e3b3a5f7d4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2075, 181)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#ajit\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate5_spectral2.csv')\n",
        "# dm\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm_4featu.csv')\n",
        "\n",
        "\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/melspectogram_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/chroma_cqt_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/mfcc_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/spectral_centroid_meandm_2075_200.csv')\n",
        "\n",
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HrbtbLXcbTx",
        "outputId": "d003a80d-61dd-4f1e-e8ec-b5ec31aefaf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "siz=415\n",
        "df_read = df.copy()\n",
        "df1 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df1.index)\n",
        "df2 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df2.index)\n",
        "df3 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df3.index)\n",
        "df4 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df4.index)\n",
        "df5 = df_read.copy()\n",
        "\n",
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)\n",
        "print(df4.shape)\n",
        "print(df5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j19bgqpcbWT",
        "outputId": "039c7f0a-d4ca-4d55-ec1c-15d9ddaab4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int64Index([ 881,  453, 2004, 1353,  281,  941, 1185, 1159, 1138,  599,\n",
            "            ...\n",
            "             834, 1730,  353, 1345, 1190, 1375,  185,  701, 1671, 1982],\n",
            "           dtype='int64', length=415)\n"
          ]
        }
      ],
      "source": [
        "q = list(df1.index)+list(df2.index)+list(df3.index)+list(df4.index)+list(df5.index)\n",
        "print(df1.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uT0Jrfa5cbZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e40a1d7-9ce5-4ea0-aa36-2251c28a8d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear\n",
            "linear\n",
            "linear\n",
            "Epoch 1/1000\n",
            "52/52 [==============================] - 35s 49ms/step - loss: 4.7539 - val_loss: 2.7125\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 2.1145 - val_loss: 2.0492\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 2.0629 - val_loss: 1.8412\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.8430 - val_loss: 1.7999\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.8011 - val_loss: 1.6907\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.7765 - val_loss: 1.6356\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.6711 - val_loss: 1.4466\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.7285 - val_loss: 1.6885\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.6024 - val_loss: 1.4542\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.5872 - val_loss: 1.5918\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.5605 - val_loss: 1.6638\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.5524 - val_loss: 1.5440\n",
            "Epoch 13/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.6122 - val_loss: 1.5317\n",
            "Epoch 14/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.5739 - val_loss: 1.6833\n",
            "Epoch 15/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.5612 - val_loss: 1.5429\n",
            "Epoch 16/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.5276 - val_loss: 1.4859\n",
            "Epoch 17/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.4821 - val_loss: 1.5697\n",
            "Epoch 18/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.4555 - val_loss: 1.4539\n",
            "Epoch 19/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.4280 - val_loss: 1.4795\n",
            "Epoch 20/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.4691 - val_loss: 1.5490\n",
            "Epoch 21/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.4482 - val_loss: 1.4836\n",
            "Epoch 22/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.3828 - val_loss: 1.3772\n",
            "Epoch 23/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.4289 - val_loss: 1.3343\n",
            "Epoch 24/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.4274 - val_loss: 1.4263\n",
            "Epoch 25/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.4003 - val_loss: 1.3840\n",
            "Epoch 26/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.4130 - val_loss: 1.3312\n",
            "Epoch 27/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3985 - val_loss: 1.3713\n",
            "Epoch 28/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3410 - val_loss: 1.3710\n",
            "Epoch 29/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.4010 - val_loss: 1.3434\n",
            "Epoch 30/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.4085 - val_loss: 1.4934\n",
            "Epoch 31/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.3780 - val_loss: 1.2954\n",
            "Epoch 32/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.3411 - val_loss: 1.5545\n",
            "Epoch 33/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3632 - val_loss: 1.2709\n",
            "Epoch 34/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3288 - val_loss: 1.4817\n",
            "Epoch 35/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3605 - val_loss: 1.3831\n",
            "Epoch 36/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3446 - val_loss: 1.3919\n",
            "Epoch 37/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3022 - val_loss: 1.3274\n",
            "Epoch 38/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.3193 - val_loss: 1.3411\n",
            "Epoch 39/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.2557 - val_loss: 1.3851\n",
            "Epoch 40/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.3114 - val_loss: 1.4121\n",
            "Epoch 41/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.3145 - val_loss: 1.2986\n",
            "Epoch 42/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.2701 - val_loss: 1.2720\n",
            "Epoch 43/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2765 - val_loss: 1.3731\n",
            "Epoch 44/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.2546 - val_loss: 1.2887\n",
            "Epoch 45/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2535 - val_loss: 1.3057\n",
            "Epoch 46/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.2592 - val_loss: 1.2139\n",
            "Epoch 47/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2645 - val_loss: 1.2056\n",
            "Epoch 48/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.2012 - val_loss: 1.2680\n",
            "Epoch 49/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.2375 - val_loss: 1.2173\n",
            "Epoch 50/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.2820 - val_loss: 1.2511\n",
            "Epoch 51/1000\n",
            "52/52 [==============================] - 3s 57ms/step - loss: 1.2365 - val_loss: 1.2380\n",
            "Epoch 52/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 1.2285 - val_loss: 1.3127\n",
            "Epoch 53/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2352 - val_loss: 1.2343\n",
            "Epoch 54/1000\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.1702 - val_loss: 1.2260\n",
            "Epoch 55/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.2219 - val_loss: 1.3101\n",
            "Epoch 56/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.1950 - val_loss: 1.2722\n",
            "Epoch 57/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1672 - val_loss: 1.2938\n",
            "Epoch 58/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.2059 - val_loss: 1.2389\n",
            "Epoch 59/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.2190 - val_loss: 1.3672\n",
            "Epoch 60/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.1803 - val_loss: 1.3003\n",
            "Epoch 61/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.2092 - val_loss: 1.1472\n",
            "Epoch 62/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1610 - val_loss: 1.1969\n",
            "Epoch 63/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.1982 - val_loss: 1.1730\n",
            "Epoch 64/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 1.1570 - val_loss: 1.1410\n",
            "Epoch 65/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1673 - val_loss: 1.0970\n",
            "Epoch 66/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.1275 - val_loss: 1.1671\n",
            "Epoch 67/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1211 - val_loss: 1.2008\n",
            "Epoch 68/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1241 - val_loss: 1.0969\n",
            "Epoch 69/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.1516 - val_loss: 1.1501\n",
            "Epoch 70/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1519 - val_loss: 1.1487\n",
            "Epoch 71/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.1616 - val_loss: 1.0970\n",
            "Epoch 72/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.1206 - val_loss: 1.1714\n",
            "Epoch 73/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1135 - val_loss: 1.0409\n",
            "Epoch 74/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1319 - val_loss: 1.1851\n",
            "Epoch 75/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1156 - val_loss: 1.1038\n",
            "Epoch 76/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0808 - val_loss: 1.0931\n",
            "Epoch 77/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1068 - val_loss: 1.0858\n",
            "Epoch 78/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.0837 - val_loss: 1.1218\n",
            "Epoch 79/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.1208 - val_loss: 1.0776\n",
            "Epoch 80/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.0875 - val_loss: 1.0907\n",
            "Epoch 81/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.0962 - val_loss: 1.0961\n",
            "Epoch 82/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0784 - val_loss: 1.1336\n",
            "Epoch 83/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.1190 - val_loss: 1.0988\n",
            "Epoch 84/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0505 - val_loss: 1.0161\n",
            "Epoch 85/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.0778 - val_loss: 1.0517\n",
            "Epoch 86/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0963 - val_loss: 1.0194\n",
            "Epoch 87/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.0453 - val_loss: 1.0479\n",
            "Epoch 88/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.0691 - val_loss: 1.0871\n",
            "Epoch 89/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.0693 - val_loss: 1.0362\n",
            "Epoch 90/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.0632 - val_loss: 1.0645\n",
            "Epoch 91/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.0678 - val_loss: 1.0652\n",
            "Epoch 92/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0729 - val_loss: 1.0237\n",
            "Epoch 93/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9832 - val_loss: 1.1643\n",
            "Epoch 94/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0418 - val_loss: 1.0048\n",
            "Epoch 95/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0750 - val_loss: 1.0603\n",
            "Epoch 96/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.0711 - val_loss: 0.9830\n",
            "Epoch 97/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 1.0543 - val_loss: 0.9993\n",
            "Epoch 98/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.0447 - val_loss: 1.0289\n",
            "Epoch 99/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.0254 - val_loss: 0.9504\n",
            "Epoch 100/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0164 - val_loss: 1.0112\n",
            "Epoch 101/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0290 - val_loss: 1.0240\n",
            "Epoch 102/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 1.0314 - val_loss: 0.9718\n",
            "Epoch 103/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 1.0169 - val_loss: 1.0093\n",
            "Epoch 104/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9989 - val_loss: 0.9362\n",
            "Epoch 105/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.0642 - val_loss: 0.8891\n",
            "Epoch 106/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 1.0248 - val_loss: 0.9095\n",
            "Epoch 107/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9837 - val_loss: 0.9159\n",
            "Epoch 108/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 1.0101 - val_loss: 0.9344\n",
            "Epoch 109/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9996 - val_loss: 0.9334\n",
            "Epoch 110/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.9745 - val_loss: 0.9799\n",
            "Epoch 111/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9669 - val_loss: 0.9530\n",
            "Epoch 112/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 1.0082 - val_loss: 0.9725\n",
            "Epoch 113/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9857 - val_loss: 0.9580\n",
            "Epoch 114/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.9852 - val_loss: 0.9225\n",
            "Epoch 115/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.9639 - val_loss: 0.9172\n",
            "Epoch 116/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9634 - val_loss: 0.8961\n",
            "Epoch 117/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9686 - val_loss: 0.9503\n",
            "Epoch 118/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9536 - val_loss: 0.8971\n",
            "Epoch 119/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9661 - val_loss: 0.9263\n",
            "Epoch 120/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.9572 - val_loss: 0.9494\n",
            "Epoch 121/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.9556 - val_loss: 0.9219\n",
            "Epoch 122/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.9402 - val_loss: 0.9266\n",
            "Epoch 123/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.9578 - val_loss: 0.8870\n",
            "Epoch 124/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9731 - val_loss: 0.8581\n",
            "Epoch 125/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9553 - val_loss: 0.8891\n",
            "Epoch 126/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9210 - val_loss: 0.8711\n",
            "Epoch 127/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9217 - val_loss: 0.9484\n",
            "Epoch 128/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9573 - val_loss: 0.9266\n",
            "Epoch 129/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9405 - val_loss: 0.8924\n",
            "Epoch 130/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.9182 - val_loss: 0.8716\n",
            "Epoch 131/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.9171 - val_loss: 0.9588\n",
            "Epoch 132/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9856 - val_loss: 0.9135\n",
            "Epoch 133/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.9144 - val_loss: 0.9035\n",
            "Epoch 134/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9140 - val_loss: 0.8887\n",
            "Epoch 135/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9152 - val_loss: 0.9545\n",
            "Epoch 136/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9743 - val_loss: 0.8369\n",
            "Epoch 137/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.9366 - val_loss: 0.8531\n",
            "Epoch 138/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9437 - val_loss: 0.9205\n",
            "Epoch 139/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.9525 - val_loss: 0.9095\n",
            "Epoch 140/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9204 - val_loss: 0.8166\n",
            "Epoch 141/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9477 - val_loss: 0.8242\n",
            "Epoch 142/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9152 - val_loss: 0.8908\n",
            "Epoch 143/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9005 - val_loss: 0.8276\n",
            "Epoch 144/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9045 - val_loss: 0.9317\n",
            "Epoch 145/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.9291 - val_loss: 0.8346\n",
            "Epoch 146/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9277 - val_loss: 0.8834\n",
            "Epoch 147/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.9257 - val_loss: 0.9139\n",
            "Epoch 148/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.9022 - val_loss: 0.8562\n",
            "Epoch 149/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9089 - val_loss: 0.8784\n",
            "Epoch 150/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.9135 - val_loss: 0.8129\n",
            "Epoch 151/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8984 - val_loss: 0.8914\n",
            "Epoch 152/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8821 - val_loss: 0.8739\n",
            "Epoch 153/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8986 - val_loss: 0.8582\n",
            "Epoch 154/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.8811 - val_loss: 0.8232\n",
            "Epoch 155/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.8798 - val_loss: 0.8394\n",
            "Epoch 156/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.8777 - val_loss: 0.8402\n",
            "Epoch 157/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9002 - val_loss: 0.7999\n",
            "Epoch 158/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8552 - val_loss: 0.8621\n",
            "Epoch 159/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8751 - val_loss: 0.8429\n",
            "Epoch 160/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9090 - val_loss: 0.8505\n",
            "Epoch 161/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.9099 - val_loss: 0.8257\n",
            "Epoch 162/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.9142 - val_loss: 0.8414\n",
            "Epoch 163/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.8719 - val_loss: 0.7879\n",
            "Epoch 164/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.9092 - val_loss: 0.8327\n",
            "Epoch 165/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8874 - val_loss: 0.8070\n",
            "Epoch 166/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.9010 - val_loss: 0.8422\n",
            "Epoch 167/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8463 - val_loss: 0.8725\n",
            "Epoch 168/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.9055 - val_loss: 0.8240\n",
            "Epoch 169/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.8444 - val_loss: 0.8536\n",
            "Epoch 170/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8603 - val_loss: 0.7789\n",
            "Epoch 171/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.8777 - val_loss: 0.8298\n",
            "Epoch 172/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.8473 - val_loss: 0.8806\n",
            "Epoch 173/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8630 - val_loss: 0.8686\n",
            "Epoch 174/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8741 - val_loss: 0.8621\n",
            "Epoch 175/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.8428 - val_loss: 0.8045\n",
            "Epoch 176/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8310 - val_loss: 0.8932\n",
            "Epoch 177/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8664 - val_loss: 0.8545\n",
            "Epoch 178/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8523 - val_loss: 0.8036\n",
            "Epoch 179/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.8483 - val_loss: 0.8027\n",
            "Epoch 180/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.8501 - val_loss: 0.8346\n",
            "Epoch 181/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.8384 - val_loss: 0.8440\n",
            "Epoch 182/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8432 - val_loss: 0.8356\n",
            "Epoch 183/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8616 - val_loss: 0.8449\n",
            "Epoch 184/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8348 - val_loss: 0.8271\n",
            "Epoch 185/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8488 - val_loss: 0.8203\n",
            "Epoch 186/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8419 - val_loss: 0.8166\n",
            "Epoch 187/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8380 - val_loss: 0.7980\n",
            "Epoch 188/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.8721 - val_loss: 0.8119\n",
            "Epoch 189/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8413 - val_loss: 0.8233\n",
            "Epoch 190/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8051 - val_loss: 0.8366\n",
            "Epoch 191/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8509 - val_loss: 0.7755\n",
            "Epoch 192/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8223 - val_loss: 0.7880\n",
            "Epoch 193/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8418 - val_loss: 0.7928\n",
            "Epoch 194/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8340 - val_loss: 0.8147\n",
            "Epoch 195/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8658 - val_loss: 0.7855\n",
            "Epoch 196/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.8167 - val_loss: 0.7633\n",
            "Epoch 197/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8499 - val_loss: 0.8101\n",
            "Epoch 198/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8478 - val_loss: 0.8200\n",
            "Epoch 199/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8423 - val_loss: 0.8014\n",
            "Epoch 200/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8088 - val_loss: 0.7989\n",
            "Epoch 201/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8359 - val_loss: 0.8054\n",
            "Epoch 202/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8405 - val_loss: 0.7799\n",
            "Epoch 203/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7934 - val_loss: 0.7721\n",
            "Epoch 204/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.8072 - val_loss: 0.7591\n",
            "Epoch 205/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8411 - val_loss: 0.7583\n",
            "Epoch 206/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8316 - val_loss: 0.7869\n",
            "Epoch 207/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8281 - val_loss: 0.7728\n",
            "Epoch 208/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8140 - val_loss: 0.7821\n",
            "Epoch 209/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7944 - val_loss: 0.8120\n",
            "Epoch 210/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.8193 - val_loss: 0.8207\n",
            "Epoch 211/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.7904 - val_loss: 0.7660\n",
            "Epoch 212/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.8082 - val_loss: 0.8077\n",
            "Epoch 213/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8222 - val_loss: 0.7666\n",
            "Epoch 214/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7667 - val_loss: 0.7669\n",
            "Epoch 215/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7967 - val_loss: 0.8496\n",
            "Epoch 216/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8302 - val_loss: 0.7748\n",
            "Epoch 217/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8192 - val_loss: 0.7995\n",
            "Epoch 218/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7985 - val_loss: 0.8053\n",
            "Epoch 219/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.8044 - val_loss: 0.7958\n",
            "Epoch 220/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.7944 - val_loss: 0.7422\n",
            "Epoch 221/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7952 - val_loss: 0.8306\n",
            "Epoch 222/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7905 - val_loss: 0.8067\n",
            "Epoch 223/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.8050 - val_loss: 0.7853\n",
            "Epoch 224/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7952 - val_loss: 0.7459\n",
            "Epoch 225/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7741 - val_loss: 0.7742\n",
            "Epoch 226/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7926 - val_loss: 0.8221\n",
            "Epoch 227/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7921 - val_loss: 0.7402\n",
            "Epoch 228/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.7993 - val_loss: 0.7554\n",
            "Epoch 229/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7858 - val_loss: 0.7642\n",
            "Epoch 230/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.8240 - val_loss: 0.7643\n",
            "Epoch 231/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7625 - val_loss: 0.7654\n",
            "Epoch 232/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7766 - val_loss: 0.7386\n",
            "Epoch 233/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7948 - val_loss: 0.7560\n",
            "Epoch 234/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7883 - val_loss: 0.7491\n",
            "Epoch 235/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7771 - val_loss: 0.7922\n",
            "Epoch 236/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.7969 - val_loss: 0.7466\n",
            "Epoch 237/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8005 - val_loss: 0.8190\n",
            "Epoch 238/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7890 - val_loss: 0.8020\n",
            "Epoch 239/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.8054 - val_loss: 0.7408\n",
            "Epoch 240/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7889 - val_loss: 0.7382\n",
            "Epoch 241/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7760 - val_loss: 0.7790\n",
            "Epoch 242/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7994 - val_loss: 0.7733\n",
            "Epoch 243/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7641 - val_loss: 0.7276\n",
            "Epoch 244/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.7784 - val_loss: 0.7584\n",
            "Epoch 245/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.7861 - val_loss: 0.7690\n",
            "Epoch 246/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7766 - val_loss: 0.7933\n",
            "Epoch 247/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7495 - val_loss: 0.7952\n",
            "Epoch 248/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7803 - val_loss: 0.7372\n",
            "Epoch 249/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7766 - val_loss: 0.7203\n",
            "Epoch 250/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7768 - val_loss: 0.8125\n",
            "Epoch 251/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7413 - val_loss: 0.7878\n",
            "Epoch 252/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.7812 - val_loss: 0.7522\n",
            "Epoch 253/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.7599 - val_loss: 0.7611\n",
            "Epoch 254/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7480 - val_loss: 0.8052\n",
            "Epoch 255/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7390 - val_loss: 0.7768\n",
            "Epoch 256/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7660 - val_loss: 0.8130\n",
            "Epoch 257/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7446 - val_loss: 0.7219\n",
            "Epoch 258/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7635 - val_loss: 0.6976\n",
            "Epoch 259/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7481 - val_loss: 0.7577\n",
            "Epoch 260/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7583 - val_loss: 0.7384\n",
            "Epoch 261/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.7266 - val_loss: 0.7502\n",
            "Epoch 262/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7408 - val_loss: 0.7393\n",
            "Epoch 263/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7528 - val_loss: 0.7251\n",
            "Epoch 264/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7714 - val_loss: 0.7566\n",
            "Epoch 265/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7557 - val_loss: 0.7623\n",
            "Epoch 266/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7594 - val_loss: 0.7603\n",
            "Epoch 267/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7271 - val_loss: 0.7548\n",
            "Epoch 268/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7251 - val_loss: 0.7392\n",
            "Epoch 269/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.7581 - val_loss: 0.7636\n",
            "Epoch 270/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7269 - val_loss: 0.7446\n",
            "Epoch 271/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7628 - val_loss: 0.7714\n",
            "Epoch 272/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7244 - val_loss: 0.7179\n",
            "Epoch 273/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7604 - val_loss: 0.7351\n",
            "Epoch 274/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7292 - val_loss: 0.7815\n",
            "Epoch 275/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7579 - val_loss: 0.7139\n",
            "Epoch 276/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7166 - val_loss: 0.7532\n",
            "Epoch 277/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.7219 - val_loss: 0.7477\n",
            "Epoch 278/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.7406 - val_loss: 0.7585\n",
            "Epoch 279/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7210 - val_loss: 0.7041\n",
            "Epoch 280/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7562 - val_loss: 0.7205\n",
            "Epoch 281/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7190 - val_loss: 0.7354\n",
            "Epoch 282/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7531 - val_loss: 0.8116\n",
            "Epoch 283/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6958 - val_loss: 0.7182\n",
            "Epoch 284/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7385 - val_loss: 0.7365\n",
            "Epoch 285/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.7292 - val_loss: 0.7143\n",
            "Epoch 286/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.7342 - val_loss: 0.7673\n",
            "Epoch 287/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7238 - val_loss: 0.7279\n",
            "Epoch 288/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7101 - val_loss: 0.7238\n",
            "Epoch 289/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7221 - val_loss: 0.7465\n",
            "Epoch 290/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7349 - val_loss: 0.7253\n",
            "Epoch 291/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7166 - val_loss: 0.7080\n",
            "Epoch 292/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6975 - val_loss: 0.7246\n",
            "Epoch 293/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7454 - val_loss: 0.7059\n",
            "Epoch 294/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6887 - val_loss: 0.7688\n",
            "Epoch 295/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7624 - val_loss: 0.7225\n",
            "Epoch 296/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7241 - val_loss: 0.7275\n",
            "Epoch 297/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7123 - val_loss: 0.7573\n",
            "Epoch 298/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6961 - val_loss: 0.7367\n",
            "Epoch 299/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7195 - val_loss: 0.7154\n",
            "Epoch 300/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7149 - val_loss: 0.7060\n",
            "Epoch 301/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7189 - val_loss: 0.7305\n",
            "Epoch 302/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.7093 - val_loss: 0.7596\n",
            "Epoch 303/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7033 - val_loss: 0.7414\n",
            "Epoch 304/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7317 - val_loss: 0.7192\n",
            "Epoch 305/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7102 - val_loss: 0.7201\n",
            "Epoch 306/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6945 - val_loss: 0.7186\n",
            "Epoch 307/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6838 - val_loss: 0.7240\n",
            "Epoch 308/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7164 - val_loss: 0.7914\n",
            "Epoch 309/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6846 - val_loss: 0.7439\n",
            "Epoch 310/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.6912 - val_loss: 0.7526\n",
            "Epoch 311/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.7149 - val_loss: 0.7016\n",
            "Epoch 312/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7198 - val_loss: 0.7111\n",
            "Epoch 313/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6962 - val_loss: 0.7438\n",
            "Epoch 314/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7047 - val_loss: 0.7410\n",
            "Epoch 315/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6809 - val_loss: 0.7444\n",
            "Epoch 316/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7007 - val_loss: 0.7416\n",
            "Epoch 317/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6800 - val_loss: 0.7397\n",
            "Epoch 318/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6933 - val_loss: 0.7416\n",
            "Epoch 319/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.6782 - val_loss: 0.6918\n",
            "Epoch 320/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.7155 - val_loss: 0.7103\n",
            "Epoch 321/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7054 - val_loss: 0.6936\n",
            "Epoch 322/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.7100 - val_loss: 0.7225\n",
            "Epoch 323/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6994 - val_loss: 0.7424\n",
            "Epoch 324/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7327 - val_loss: 0.7166\n",
            "Epoch 325/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6948 - val_loss: 0.7178\n",
            "Epoch 326/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.7006 - val_loss: 0.7296\n",
            "Epoch 327/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.6821 - val_loss: 0.7360\n",
            "Epoch 328/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.7038 - val_loss: 0.7457\n",
            "Epoch 329/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6795 - val_loss: 0.7143\n",
            "Epoch 330/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7090 - val_loss: 0.7185\n",
            "Epoch 331/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6740 - val_loss: 0.6906\n",
            "Epoch 332/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6781 - val_loss: 0.6935\n",
            "Epoch 333/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6778 - val_loss: 0.7581\n",
            "Epoch 334/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6978 - val_loss: 0.7152\n",
            "Epoch 335/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.6555 - val_loss: 0.7069\n",
            "Epoch 336/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6991 - val_loss: 0.7194\n",
            "Epoch 337/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7080 - val_loss: 0.7211\n",
            "Epoch 338/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.7165 - val_loss: 0.6708\n",
            "Epoch 339/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6798 - val_loss: 0.7187\n",
            "Epoch 340/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6668 - val_loss: 0.7455\n",
            "Epoch 341/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.7006 - val_loss: 0.7234\n",
            "Epoch 342/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6723 - val_loss: 0.7116\n",
            "Epoch 343/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6805 - val_loss: 0.7206\n",
            "Epoch 344/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6784 - val_loss: 0.7283\n",
            "Epoch 345/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6836 - val_loss: 0.6895\n",
            "Epoch 346/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6728 - val_loss: 0.7300\n",
            "Epoch 347/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6813 - val_loss: 0.7038\n",
            "Epoch 348/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.7165 - val_loss: 0.7478\n",
            "Epoch 349/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6964 - val_loss: 0.7039\n",
            "Epoch 350/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6591 - val_loss: 0.7356\n",
            "Epoch 351/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6955 - val_loss: 0.6971\n",
            "Epoch 352/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.6690 - val_loss: 0.7011\n",
            "Epoch 353/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6933 - val_loss: 0.7612\n",
            "Epoch 354/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.7002 - val_loss: 0.7175\n",
            "Epoch 355/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6679 - val_loss: 0.6915\n",
            "Epoch 356/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6862 - val_loss: 0.6870\n",
            "Epoch 357/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6728 - val_loss: 0.7452\n",
            "Epoch 358/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6798 - val_loss: 0.6754\n",
            "Epoch 359/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6768 - val_loss: 0.6835\n",
            "Epoch 360/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6841 - val_loss: 0.7051\n",
            "Epoch 361/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6528 - val_loss: 0.7241\n",
            "Epoch 362/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6522 - val_loss: 0.7333\n",
            "Epoch 363/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6816 - val_loss: 0.7383\n",
            "Epoch 364/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6789 - val_loss: 0.7129\n",
            "Epoch 365/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6568 - val_loss: 0.7531\n",
            "Epoch 366/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6472 - val_loss: 0.7403\n",
            "Epoch 367/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6911 - val_loss: 0.7156\n",
            "Epoch 368/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.6717 - val_loss: 0.7336\n",
            "Epoch 369/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6741 - val_loss: 0.6914\n",
            "Epoch 370/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6606 - val_loss: 0.7314\n",
            "Epoch 371/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6633 - val_loss: 0.7238\n",
            "Epoch 372/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6701 - val_loss: 0.7328\n",
            "Epoch 373/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6854 - val_loss: 0.6734\n",
            "Epoch 374/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6697 - val_loss: 0.6935\n",
            "Epoch 375/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6616 - val_loss: 0.6763\n",
            "Epoch 376/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6821 - val_loss: 0.7081\n",
            "Epoch 377/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6889 - val_loss: 0.6969\n",
            "Epoch 378/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6507 - val_loss: 0.7106\n",
            "Epoch 379/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6639 - val_loss: 0.6911\n",
            "Epoch 380/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6618 - val_loss: 0.7104\n",
            "Epoch 381/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6362 - val_loss: 0.7441\n",
            "Epoch 382/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6819 - val_loss: 0.7179\n",
            "Epoch 383/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6684 - val_loss: 0.6921\n",
            "Epoch 384/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6499 - val_loss: 0.7180\n",
            "Epoch 385/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6653 - val_loss: 0.7016\n",
            "Epoch 386/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6550 - val_loss: 0.6962\n",
            "Epoch 387/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6438 - val_loss: 0.7057\n",
            "Epoch 388/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6228 - val_loss: 0.6794\n",
            "Epoch 389/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6602 - val_loss: 0.7058\n",
            "Epoch 390/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6273 - val_loss: 0.6858\n",
            "Epoch 391/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6780 - val_loss: 0.7366\n",
            "Epoch 392/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6547 - val_loss: 0.7259\n",
            "Epoch 393/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.6635 - val_loss: 0.7435\n",
            "Epoch 394/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6371 - val_loss: 0.7463\n",
            "Epoch 395/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6478 - val_loss: 0.7012\n",
            "Epoch 396/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6284 - val_loss: 0.7462\n",
            "Epoch 397/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6495 - val_loss: 0.7201\n",
            "Epoch 398/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6650 - val_loss: 0.6898\n",
            "Epoch 399/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6548 - val_loss: 0.6865\n",
            "Epoch 400/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6460 - val_loss: 0.6903\n",
            "Epoch 401/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6505 - val_loss: 0.7091\n",
            "Epoch 402/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6577 - val_loss: 0.6984\n",
            "Epoch 403/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6599 - val_loss: 0.7282\n",
            "Epoch 404/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6271 - val_loss: 0.6980\n",
            "Epoch 405/1000\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.6515 - val_loss: 0.7219\n",
            "Epoch 406/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6529 - val_loss: 0.7029\n",
            "Epoch 407/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6428 - val_loss: 0.6686\n",
            "Epoch 408/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6579 - val_loss: 0.6911\n",
            "Epoch 409/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6541 - val_loss: 0.6815\n",
            "Epoch 410/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6493 - val_loss: 0.6640\n",
            "Epoch 411/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6503 - val_loss: 0.7474\n",
            "Epoch 412/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6308 - val_loss: 0.7383\n",
            "Epoch 413/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6379 - val_loss: 0.7094\n",
            "Epoch 414/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6655 - val_loss: 0.6947\n",
            "Epoch 415/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6201 - val_loss: 0.7393\n",
            "Epoch 416/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.6195 - val_loss: 0.6980\n",
            "Epoch 417/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6275 - val_loss: 0.6864\n",
            "Epoch 418/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.6492 - val_loss: 0.6755\n",
            "Epoch 419/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6398 - val_loss: 0.7069\n",
            "Epoch 420/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6169 - val_loss: 0.7141\n",
            "Epoch 421/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6434 - val_loss: 0.7262\n",
            "Epoch 422/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6189 - val_loss: 0.6981\n",
            "Epoch 423/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.6448 - val_loss: 0.7165\n",
            "Epoch 424/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6466 - val_loss: 0.7198\n",
            "Epoch 425/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6498 - val_loss: 0.7147\n",
            "Epoch 426/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.6455 - val_loss: 0.7345\n",
            "Epoch 427/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6536 - val_loss: 0.7557\n",
            "Epoch 428/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6254 - val_loss: 0.6834\n",
            "Epoch 429/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6464 - val_loss: 0.6636\n",
            "Epoch 430/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6224 - val_loss: 0.6760\n",
            "Epoch 431/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6161 - val_loss: 0.6865\n",
            "Epoch 432/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6174 - val_loss: 0.6917\n",
            "Epoch 433/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6330 - val_loss: 0.6859\n",
            "Epoch 434/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.6271 - val_loss: 0.6969\n",
            "Epoch 435/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.6050 - val_loss: 0.6988\n",
            "Epoch 436/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6223 - val_loss: 0.6830\n",
            "Epoch 437/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6265 - val_loss: 0.6920\n",
            "Epoch 438/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6346 - val_loss: 0.6999\n",
            "Epoch 439/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6192 - val_loss: 0.6721\n",
            "Epoch 440/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6357 - val_loss: 0.7058\n",
            "Epoch 441/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6329 - val_loss: 0.6878\n",
            "Epoch 442/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6419 - val_loss: 0.6997\n",
            "Epoch 443/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.6189 - val_loss: 0.6962\n",
            "Epoch 444/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6473 - val_loss: 0.7067\n",
            "Epoch 445/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6024 - val_loss: 0.6867\n",
            "Epoch 446/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6367 - val_loss: 0.7246\n",
            "Epoch 447/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6288 - val_loss: 0.6717\n",
            "Epoch 448/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6325 - val_loss: 0.7035\n",
            "Epoch 449/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6268 - val_loss: 0.6942\n",
            "Epoch 450/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.6173 - val_loss: 0.6976\n",
            "Epoch 451/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6063 - val_loss: 0.6747\n",
            "Epoch 452/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6071 - val_loss: 0.6807\n",
            "Epoch 453/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6176 - val_loss: 0.6925\n",
            "Epoch 454/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6135 - val_loss: 0.7003\n",
            "Epoch 455/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6338 - val_loss: 0.6993\n",
            "Epoch 456/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5935 - val_loss: 0.6908\n",
            "Epoch 457/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5931 - val_loss: 0.6905\n",
            "Epoch 458/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.6211 - val_loss: 0.6850\n",
            "Epoch 459/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6073 - val_loss: 0.7005\n",
            "Epoch 460/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6039 - val_loss: 0.6762\n",
            "Epoch 461/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6068 - val_loss: 0.6579\n",
            "Epoch 462/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6125 - val_loss: 0.6797\n",
            "Epoch 463/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6258 - val_loss: 0.6622\n",
            "Epoch 464/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6111 - val_loss: 0.6881\n",
            "Epoch 465/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.6133 - val_loss: 0.6950\n",
            "Epoch 466/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6133 - val_loss: 0.6792\n",
            "Epoch 467/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5969 - val_loss: 0.6888\n",
            "Epoch 468/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6115 - val_loss: 0.6774\n",
            "Epoch 469/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6058 - val_loss: 0.7050\n",
            "Epoch 470/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5873 - val_loss: 0.6758\n",
            "Epoch 471/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6233 - val_loss: 0.7060\n",
            "Epoch 472/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6018 - val_loss: 0.7023\n",
            "Epoch 473/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5917 - val_loss: 0.7176\n",
            "Epoch 474/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6235 - val_loss: 0.7202\n",
            "Epoch 475/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5996 - val_loss: 0.6884\n",
            "Epoch 476/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6079 - val_loss: 0.6811\n",
            "Epoch 477/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6086 - val_loss: 0.6684\n",
            "Epoch 478/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5802 - val_loss: 0.7194\n",
            "Epoch 479/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5859 - val_loss: 0.6647\n",
            "Epoch 480/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5986 - val_loss: 0.6654\n",
            "Epoch 481/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.6130 - val_loss: 0.6989\n",
            "Epoch 482/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.5493 - val_loss: 0.6859\n",
            "Epoch 483/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6104 - val_loss: 0.6490\n",
            "Epoch 484/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5710 - val_loss: 0.6838\n",
            "Epoch 485/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5746 - val_loss: 0.6997\n",
            "Epoch 486/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5934 - val_loss: 0.7040\n",
            "Epoch 487/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6070 - val_loss: 0.6780\n",
            "Epoch 488/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6089 - val_loss: 0.6752\n",
            "Epoch 489/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.5970 - val_loss: 0.6537\n",
            "Epoch 490/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6064 - val_loss: 0.6623\n",
            "Epoch 491/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6067 - val_loss: 0.6607\n",
            "Epoch 492/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6110 - val_loss: 0.6803\n",
            "Epoch 493/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5884 - val_loss: 0.7217\n",
            "Epoch 494/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6069 - val_loss: 0.6693\n",
            "Epoch 495/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6094 - val_loss: 0.6682\n",
            "Epoch 496/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6076 - val_loss: 0.6684\n",
            "Epoch 497/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5985 - val_loss: 0.6451\n",
            "Epoch 498/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.5770 - val_loss: 0.6848\n",
            "Epoch 499/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5818 - val_loss: 0.6627\n",
            "Epoch 500/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.5948 - val_loss: 0.6557\n",
            "Epoch 501/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.6026 - val_loss: 0.6748\n",
            "Epoch 502/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5850 - val_loss: 0.6888\n",
            "Epoch 503/1000\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 0.5953 - val_loss: 0.6955\n",
            "Epoch 504/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.6123 - val_loss: 0.6877\n",
            "Epoch 505/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5653 - val_loss: 0.6784\n",
            "Epoch 506/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5923 - val_loss: 0.6836\n",
            "Epoch 507/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5769 - val_loss: 0.6965\n",
            "Epoch 508/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5946 - val_loss: 0.6742\n",
            "Epoch 509/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5876 - val_loss: 0.6801\n",
            "Epoch 510/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.6116 - val_loss: 0.6790\n",
            "Epoch 511/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5902 - val_loss: 0.6794\n",
            "Epoch 512/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5875 - val_loss: 0.6955\n",
            "Epoch 513/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5670 - val_loss: 0.6709\n",
            "Epoch 514/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6076 - val_loss: 0.6873\n",
            "Epoch 515/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5916 - val_loss: 0.6616\n",
            "Epoch 516/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5878 - val_loss: 0.6910\n",
            "Epoch 517/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5874 - val_loss: 0.6788\n",
            "Epoch 518/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5942 - val_loss: 0.6873\n",
            "Epoch 519/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5874 - val_loss: 0.6830\n",
            "Epoch 520/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5863 - val_loss: 0.6503\n",
            "Epoch 521/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5948 - val_loss: 0.6699\n",
            "Epoch 522/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5645 - val_loss: 0.6947\n",
            "Epoch 523/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5966 - val_loss: 0.6422\n",
            "Epoch 524/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5979 - val_loss: 0.6991\n",
            "Epoch 525/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5917 - val_loss: 0.7034\n",
            "Epoch 526/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5720 - val_loss: 0.6558\n",
            "Epoch 527/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5672 - val_loss: 0.6576\n",
            "Epoch 528/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5768 - val_loss: 0.6643\n",
            "Epoch 529/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5913 - val_loss: 0.6701\n",
            "Epoch 530/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5712 - val_loss: 0.6866\n",
            "Epoch 531/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6095 - val_loss: 0.6612\n",
            "Epoch 532/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5801 - val_loss: 0.6757\n",
            "Epoch 533/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5984 - val_loss: 0.6791\n",
            "Epoch 534/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5752 - val_loss: 0.6626\n",
            "Epoch 535/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5768 - val_loss: 0.6852\n",
            "Epoch 536/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5763 - val_loss: 0.6915\n",
            "Epoch 537/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5776 - val_loss: 0.7016\n",
            "Epoch 538/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5868 - val_loss: 0.6972\n",
            "Epoch 539/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.6049 - val_loss: 0.7042\n",
            "Epoch 540/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5629 - val_loss: 0.6844\n",
            "Epoch 541/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5736 - val_loss: 0.6994\n",
            "Epoch 542/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5593 - val_loss: 0.6744\n",
            "Epoch 543/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5740 - val_loss: 0.6951\n",
            "Epoch 544/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5627 - val_loss: 0.6544\n",
            "Epoch 545/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5574 - val_loss: 0.6946\n",
            "Epoch 546/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5486 - val_loss: 0.6675\n",
            "Epoch 547/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5693 - val_loss: 0.6634\n",
            "Epoch 548/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5852 - val_loss: 0.6614\n",
            "Epoch 549/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5983 - val_loss: 0.6643\n",
            "Epoch 550/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5861 - val_loss: 0.6512\n",
            "Epoch 551/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5777 - val_loss: 0.6667\n",
            "Epoch 552/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5593 - val_loss: 0.6590\n",
            "Epoch 553/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5490 - val_loss: 0.6481\n",
            "Epoch 554/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5866 - val_loss: 0.6692\n",
            "Epoch 555/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5787 - val_loss: 0.6809\n",
            "Epoch 556/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5787 - val_loss: 0.6926\n",
            "Epoch 557/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5637 - val_loss: 0.6642\n",
            "Epoch 558/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5863 - val_loss: 0.6652\n",
            "Epoch 559/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5683 - val_loss: 0.6722\n",
            "Epoch 560/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5688 - val_loss: 0.6686\n",
            "Epoch 561/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5645 - val_loss: 0.6757\n",
            "Epoch 562/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5766 - val_loss: 0.6619\n",
            "Epoch 563/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5879 - val_loss: 0.6763\n",
            "Epoch 564/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5821 - val_loss: 0.6755\n",
            "Epoch 565/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5556 - val_loss: 0.6747\n",
            "Epoch 566/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5661 - val_loss: 0.6778\n",
            "Epoch 567/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5700 - val_loss: 0.6591\n",
            "Epoch 568/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5785 - val_loss: 0.6915\n",
            "Epoch 569/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5593 - val_loss: 0.6389\n",
            "Epoch 570/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5751 - val_loss: 0.6903\n",
            "Epoch 571/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5739 - val_loss: 0.6653\n",
            "Epoch 572/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5855 - val_loss: 0.6726\n",
            "Epoch 573/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5727 - val_loss: 0.6716\n",
            "Epoch 574/1000\n",
            "52/52 [==============================] - 1s 29ms/step - loss: 0.5648 - val_loss: 0.6891\n",
            "Epoch 575/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5539 - val_loss: 0.6762\n",
            "Epoch 576/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5768 - val_loss: 0.6569\n",
            "Epoch 577/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5521 - val_loss: 0.6827\n",
            "Epoch 578/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5565 - val_loss: 0.6700\n",
            "Epoch 579/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5746 - val_loss: 0.6660\n",
            "Epoch 580/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5620 - val_loss: 0.6708\n",
            "Epoch 581/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5747 - val_loss: 0.6585\n",
            "Epoch 582/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5532 - val_loss: 0.6795\n",
            "Epoch 583/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5656 - val_loss: 0.6578\n",
            "Epoch 584/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5503 - val_loss: 0.6796\n",
            "Epoch 585/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5678 - val_loss: 0.6606\n",
            "Epoch 586/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5778 - val_loss: 0.6627\n",
            "Epoch 587/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5601 - val_loss: 0.6357\n",
            "Epoch 588/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5738 - val_loss: 0.6606\n",
            "Epoch 589/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5540 - val_loss: 0.6616\n",
            "Epoch 590/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5678 - val_loss: 0.6894\n",
            "Epoch 591/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5545 - val_loss: 0.7077\n",
            "Epoch 592/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5563 - val_loss: 0.6596\n",
            "Epoch 593/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5342 - val_loss: 0.6742\n",
            "Epoch 594/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5495 - val_loss: 0.6538\n",
            "Epoch 595/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5506 - val_loss: 0.7421\n",
            "Epoch 596/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5433 - val_loss: 0.6535\n",
            "Epoch 597/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5464 - val_loss: 0.6560\n",
            "Epoch 598/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5243 - val_loss: 0.7302\n",
            "Epoch 599/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5429 - val_loss: 0.6783\n",
            "Epoch 600/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5306 - val_loss: 0.6847\n",
            "Epoch 601/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5424 - val_loss: 0.6818\n",
            "Epoch 602/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5421 - val_loss: 0.6713\n",
            "Epoch 603/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5724 - val_loss: 0.6518\n",
            "Epoch 604/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5620 - val_loss: 0.6655\n",
            "Epoch 605/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5593 - val_loss: 0.6773\n",
            "Epoch 606/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5251 - val_loss: 0.6802\n",
            "Epoch 607/1000\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.5705 - val_loss: 0.6831\n",
            "Epoch 608/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5480 - val_loss: 0.6360\n",
            "Epoch 609/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5691 - val_loss: 0.6641\n",
            "Epoch 610/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5458 - val_loss: 0.6454\n",
            "Epoch 611/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5568 - val_loss: 0.6411\n",
            "Epoch 612/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5395 - val_loss: 0.6506\n",
            "Epoch 613/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5402 - val_loss: 0.6625\n",
            "Epoch 614/1000\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 0.5344 - val_loss: 0.6582\n",
            "Epoch 615/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5453 - val_loss: 0.6622\n",
            "Epoch 616/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.5509 - val_loss: 0.6763\n",
            "Epoch 617/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5432 - val_loss: 0.6754\n",
            "Epoch 618/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5647 - val_loss: 0.6426\n",
            "Epoch 619/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5395 - val_loss: 0.6641\n",
            "Epoch 620/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5533 - val_loss: 0.6377\n",
            "Epoch 621/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5124 - val_loss: 0.6395\n",
            "Epoch 622/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5194 - val_loss: 0.6522\n",
            "Epoch 623/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5509 - val_loss: 0.6576\n",
            "Epoch 624/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5583 - val_loss: 0.6512\n",
            "Epoch 625/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5530 - val_loss: 0.6511\n",
            "Epoch 626/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5451 - val_loss: 0.6556\n",
            "Epoch 627/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5359 - val_loss: 0.6874\n",
            "Epoch 628/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5481 - val_loss: 0.6525\n",
            "Epoch 629/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5551 - val_loss: 0.6508\n",
            "Epoch 630/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5410 - val_loss: 0.6558\n",
            "Epoch 631/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5604 - val_loss: 0.6641\n",
            "Epoch 632/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 0.5493 - val_loss: 0.6721\n",
            "Epoch 633/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5520 - val_loss: 0.7127\n",
            "Epoch 634/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5574 - val_loss: 0.6691\n",
            "Epoch 635/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5621 - val_loss: 0.6703\n",
            "Epoch 636/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5430 - val_loss: 0.6655\n",
            "Epoch 637/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5290 - val_loss: 0.6524\n",
            "Epoch 638/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5538 - val_loss: 0.6526\n",
            "Epoch 639/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5418 - val_loss: 0.6548\n",
            "Epoch 640/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5532 - val_loss: 0.6719\n",
            "Epoch 641/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5310 - val_loss: 0.6686\n",
            "Epoch 642/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5430 - val_loss: 0.6717\n",
            "Epoch 643/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5483 - val_loss: 0.6710\n",
            "Epoch 644/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5208 - val_loss: 0.6422\n",
            "Epoch 645/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5212 - val_loss: 0.6624\n",
            "Epoch 646/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5183 - val_loss: 0.6534\n",
            "Epoch 647/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5210 - val_loss: 0.6566\n",
            "Epoch 648/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5562 - val_loss: 0.6613\n",
            "Epoch 649/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5527 - val_loss: 0.6558\n",
            "Epoch 650/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5579 - val_loss: 0.6549\n",
            "Epoch 651/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5497 - val_loss: 0.6541\n",
            "Epoch 652/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5414 - val_loss: 0.6437\n",
            "Epoch 653/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5386 - val_loss: 0.6528\n",
            "Epoch 654/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.5477 - val_loss: 0.6863\n",
            "Epoch 655/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5318 - val_loss: 0.6697\n",
            "Epoch 656/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5341 - val_loss: 0.6859\n",
            "Epoch 657/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5522 - val_loss: 0.6536\n",
            "Epoch 658/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5347 - val_loss: 0.6488\n",
            "Epoch 659/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5332 - val_loss: 0.6476\n",
            "Epoch 660/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5219 - val_loss: 0.6861\n",
            "Epoch 661/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5115 - val_loss: 0.6663\n",
            "Epoch 662/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.5466 - val_loss: 0.6498\n",
            "Epoch 663/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5415 - val_loss: 0.6634\n",
            "Epoch 664/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5363 - val_loss: 0.6346\n",
            "Epoch 665/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5408 - val_loss: 0.6425\n",
            "Epoch 666/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5200 - val_loss: 0.6538\n",
            "Epoch 667/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5299 - val_loss: 0.6442\n",
            "Epoch 668/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5339 - val_loss: 0.6476\n",
            "Epoch 669/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5484 - val_loss: 0.6645\n",
            "Epoch 670/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5345 - val_loss: 0.6691\n",
            "Epoch 671/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5241 - val_loss: 0.6670\n",
            "Epoch 672/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5511 - val_loss: 0.6336\n",
            "Epoch 673/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5271 - val_loss: 0.6407\n",
            "Epoch 674/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5290 - val_loss: 0.6487\n",
            "Epoch 675/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5178 - val_loss: 0.6754\n",
            "Epoch 676/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5367 - val_loss: 0.6860\n",
            "Epoch 677/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5200 - val_loss: 0.6422\n",
            "Epoch 678/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5107 - val_loss: 0.6458\n",
            "Epoch 679/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5183 - val_loss: 0.6567\n",
            "Epoch 680/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5263 - val_loss: 0.6782\n",
            "Epoch 681/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5295 - val_loss: 0.6403\n",
            "Epoch 682/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5286 - val_loss: 0.6565\n",
            "Epoch 683/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5164 - val_loss: 0.6378\n",
            "Epoch 684/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5348 - val_loss: 0.6302\n",
            "Epoch 685/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5121 - val_loss: 0.6516\n",
            "Epoch 686/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5235 - val_loss: 0.6593\n",
            "Epoch 687/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5085 - val_loss: 0.6722\n",
            "Epoch 688/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5250 - val_loss: 0.6694\n",
            "Epoch 689/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5204 - val_loss: 0.6618\n",
            "Epoch 690/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5013 - val_loss: 0.6745\n",
            "Epoch 691/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5086 - val_loss: 0.6588\n",
            "Epoch 692/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5119 - val_loss: 0.6624\n",
            "Epoch 693/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 0.5191 - val_loss: 0.6580\n",
            "Epoch 694/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4909 - val_loss: 0.6699\n",
            "Epoch 695/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5366 - val_loss: 0.6779\n",
            "Epoch 696/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5212 - val_loss: 0.6409\n",
            "Epoch 697/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5262 - val_loss: 0.6527\n",
            "Epoch 698/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5124 - val_loss: 0.6707\n",
            "Epoch 699/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5110 - val_loss: 0.6316\n",
            "Epoch 700/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5169 - val_loss: 0.6416\n",
            "Epoch 701/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.5222 - val_loss: 0.6645\n",
            "Epoch 702/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5290 - val_loss: 0.6495\n",
            "Epoch 703/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5210 - val_loss: 0.6803\n",
            "Epoch 704/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5346 - val_loss: 0.6663\n",
            "Epoch 705/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5028 - val_loss: 0.6430\n",
            "Epoch 706/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5144 - val_loss: 0.6419\n",
            "Epoch 707/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5251 - val_loss: 0.6700\n",
            "Epoch 708/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5183 - val_loss: 0.6752\n",
            "Epoch 709/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5010 - val_loss: 0.6273\n",
            "Epoch 710/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5250 - val_loss: 0.6666\n",
            "Epoch 711/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5232 - val_loss: 0.6456\n",
            "Epoch 712/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5288 - val_loss: 0.6689\n",
            "Epoch 713/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5174 - val_loss: 0.6349\n",
            "Epoch 714/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5126 - val_loss: 0.6518\n",
            "Epoch 715/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4992 - val_loss: 0.6401\n",
            "Epoch 716/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 0.5240 - val_loss: 0.6406\n",
            "Epoch 717/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5008 - val_loss: 0.6719\n",
            "Epoch 718/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4896 - val_loss: 0.6828\n",
            "Epoch 719/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5335 - val_loss: 0.6445\n",
            "Epoch 720/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5225 - val_loss: 0.6523\n",
            "Epoch 721/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5192 - val_loss: 0.6486\n",
            "Epoch 722/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4836 - val_loss: 0.6973\n",
            "Epoch 723/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5192 - val_loss: 0.6615\n",
            "Epoch 724/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.5117 - val_loss: 0.6663\n",
            "Epoch 725/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4976 - val_loss: 0.7022\n",
            "Epoch 726/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5301 - val_loss: 0.6736\n",
            "Epoch 727/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5193 - val_loss: 0.6528\n",
            "Epoch 728/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4974 - val_loss: 0.6589\n",
            "Epoch 729/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4865 - val_loss: 0.6670\n",
            "Epoch 730/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5145 - val_loss: 0.6487\n",
            "Epoch 731/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5099 - val_loss: 0.6795\n",
            "Epoch 732/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5223 - val_loss: 0.6453\n",
            "Epoch 733/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5334 - val_loss: 0.6399\n",
            "Epoch 734/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5007 - val_loss: 0.6531\n",
            "Epoch 735/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5032 - val_loss: 0.6547\n",
            "Epoch 736/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5109 - val_loss: 0.6458\n",
            "Epoch 737/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5319 - val_loss: 0.6360\n",
            "Epoch 738/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5075 - val_loss: 0.6558\n",
            "Epoch 739/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4799 - val_loss: 0.6496\n",
            "Epoch 740/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5092 - val_loss: 0.6327\n",
            "Epoch 741/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4916 - val_loss: 0.6535\n",
            "Epoch 742/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.5135 - val_loss: 0.6386\n",
            "Epoch 743/1000\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.4976 - val_loss: 0.6408\n",
            "Epoch 744/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4995 - val_loss: 0.6887\n",
            "Epoch 745/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4985 - val_loss: 0.6613\n",
            "Epoch 746/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.5035 - val_loss: 0.6402\n",
            "Epoch 747/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5001 - val_loss: 0.6439\n",
            "Epoch 748/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5039 - val_loss: 0.6338\n",
            "Epoch 749/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5126 - val_loss: 0.6659\n",
            "Epoch 750/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5225 - val_loss: 0.6584\n",
            "Epoch 751/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5166 - val_loss: 0.6205\n",
            "Epoch 752/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4931 - val_loss: 0.6346\n",
            "Epoch 753/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5042 - val_loss: 0.6340\n",
            "Epoch 754/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5006 - val_loss: 0.6663\n",
            "Epoch 755/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4807 - val_loss: 0.6291\n",
            "Epoch 756/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5049 - val_loss: 0.6331\n",
            "Epoch 757/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4969 - val_loss: 0.6501\n",
            "Epoch 758/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5056 - val_loss: 0.6587\n",
            "Epoch 759/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4834 - val_loss: 0.6572\n",
            "Epoch 760/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4937 - val_loss: 0.6394\n",
            "Epoch 761/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.4960 - val_loss: 0.6393\n",
            "Epoch 762/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5114 - val_loss: 0.6636\n",
            "Epoch 763/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4971 - val_loss: 0.6622\n",
            "Epoch 764/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5175 - val_loss: 0.6521\n",
            "Epoch 765/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4901 - val_loss: 0.6523\n",
            "Epoch 766/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5028 - val_loss: 0.6493\n",
            "Epoch 767/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4958 - val_loss: 0.6574\n",
            "Epoch 768/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.5170 - val_loss: 0.6516\n",
            "Epoch 769/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4825 - val_loss: 0.6648\n",
            "Epoch 770/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4903 - val_loss: 0.6758\n",
            "Epoch 771/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5034 - val_loss: 0.6499\n",
            "Epoch 772/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5130 - val_loss: 0.6381\n",
            "Epoch 773/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4862 - val_loss: 0.6453\n",
            "Epoch 774/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4820 - val_loss: 0.6609\n",
            "Epoch 775/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4949 - val_loss: 0.6567\n",
            "Epoch 776/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.4688 - val_loss: 0.6591\n",
            "Epoch 777/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4876 - val_loss: 0.6541\n",
            "Epoch 778/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4848 - val_loss: 0.6773\n",
            "Epoch 779/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5024 - val_loss: 0.6575\n",
            "Epoch 780/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4962 - val_loss: 0.6353\n",
            "Epoch 781/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4986 - val_loss: 0.6422\n",
            "Epoch 782/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5002 - val_loss: 0.6590\n",
            "Epoch 783/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.4978 - val_loss: 0.6633\n",
            "Epoch 784/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.5015 - val_loss: 0.6477\n",
            "Epoch 785/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4863 - val_loss: 0.6772\n",
            "Epoch 786/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4849 - val_loss: 0.6649\n",
            "Epoch 787/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5027 - val_loss: 0.6634\n",
            "Epoch 788/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4805 - val_loss: 0.6481\n",
            "Epoch 789/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4824 - val_loss: 0.6728\n",
            "Epoch 790/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4848 - val_loss: 0.6702\n",
            "Epoch 791/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4827 - val_loss: 0.6512\n",
            "Epoch 792/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4938 - val_loss: 0.6537\n",
            "Epoch 793/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4971 - val_loss: 0.6549\n",
            "Epoch 794/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4877 - val_loss: 0.6306\n",
            "Epoch 795/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5182 - val_loss: 0.6404\n",
            "Epoch 796/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4759 - val_loss: 0.6496\n",
            "Epoch 797/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4833 - val_loss: 0.6536\n",
            "Epoch 798/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4833 - val_loss: 0.6623\n",
            "Epoch 799/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4831 - val_loss: 0.6369\n",
            "Epoch 800/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5233 - val_loss: 0.6585\n",
            "Epoch 801/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4901 - val_loss: 0.6289\n",
            "Epoch 802/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4912 - val_loss: 0.6544\n",
            "Epoch 803/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4972 - val_loss: 0.6545\n",
            "Epoch 804/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4916 - val_loss: 0.6286\n",
            "Epoch 805/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5114 - val_loss: 0.6631\n",
            "Epoch 806/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4856 - val_loss: 0.6399\n",
            "Epoch 807/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.4992 - val_loss: 0.6446\n",
            "Epoch 808/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4766 - val_loss: 0.6404\n",
            "Epoch 809/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4953 - val_loss: 0.6611\n",
            "Epoch 810/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4786 - val_loss: 0.6707\n",
            "Epoch 811/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4963 - val_loss: 0.6626\n",
            "Epoch 812/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4963 - val_loss: 0.6650\n",
            "Epoch 813/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4818 - val_loss: 0.6515\n",
            "Epoch 814/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4582 - val_loss: 0.6316\n",
            "Epoch 815/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4979 - val_loss: 0.6698\n",
            "Epoch 816/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4838 - val_loss: 0.6614\n",
            "Epoch 817/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4841 - val_loss: 0.6673\n",
            "Epoch 818/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4738 - val_loss: 0.6405\n",
            "Epoch 819/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4848 - val_loss: 0.6581\n",
            "Epoch 820/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4854 - val_loss: 0.6716\n",
            "Epoch 821/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.4756 - val_loss: 0.6578\n",
            "Epoch 822/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4853 - val_loss: 0.6448\n",
            "Epoch 823/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4685 - val_loss: 0.6750\n",
            "Epoch 824/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4769 - val_loss: 0.6660\n",
            "Epoch 825/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4928 - val_loss: 0.6610\n",
            "Epoch 826/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4938 - val_loss: 0.6513\n",
            "Epoch 827/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4608 - val_loss: 0.6563\n",
            "Epoch 828/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4869 - val_loss: 0.6586\n",
            "Epoch 829/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.4927 - val_loss: 0.6571\n",
            "Epoch 830/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4858 - val_loss: 0.6402\n",
            "Epoch 831/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4726 - val_loss: 0.6494\n",
            "Epoch 832/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4932 - val_loss: 0.6672\n",
            "Epoch 833/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4597 - val_loss: 0.6676\n",
            "Epoch 834/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5005 - val_loss: 0.6644\n",
            "Epoch 835/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4788 - val_loss: 0.6497\n",
            "Epoch 836/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4782 - val_loss: 0.6605\n",
            "Epoch 837/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4859 - val_loss: 0.6749\n",
            "Epoch 838/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4797 - val_loss: 0.6296\n",
            "Epoch 839/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4726 - val_loss: 0.6380\n",
            "Epoch 840/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4839 - val_loss: 0.6535\n",
            "Epoch 841/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4807 - val_loss: 0.6430\n",
            "Epoch 842/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4934 - val_loss: 0.6537\n",
            "Epoch 843/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4464 - val_loss: 0.6503\n",
            "Epoch 844/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4792 - val_loss: 0.6675\n",
            "Epoch 845/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4944 - val_loss: 0.6493\n",
            "Epoch 846/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4706 - val_loss: 0.6718\n",
            "Epoch 847/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4710 - val_loss: 0.6719\n",
            "Epoch 848/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4930 - val_loss: 0.6445\n",
            "Epoch 849/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4753 - val_loss: 0.6607\n",
            "Epoch 850/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4821 - val_loss: 0.6542\n",
            "Epoch 851/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 0.4680 - val_loss: 0.6538\n",
            "Epoch 852/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4875 - val_loss: 0.6563\n",
            "Epoch 853/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4787 - val_loss: 0.6581\n",
            "Epoch 854/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4538 - val_loss: 0.6612\n",
            "Epoch 855/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4372 - val_loss: 0.6924\n",
            "Epoch 856/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4767 - val_loss: 0.6383\n",
            "Epoch 857/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4813 - val_loss: 0.6266\n",
            "Epoch 858/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.4677 - val_loss: 0.6432\n",
            "Epoch 859/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4951 - val_loss: 0.6282\n",
            "Epoch 860/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4692 - val_loss: 0.6707\n",
            "Epoch 861/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4511 - val_loss: 0.6279\n",
            "Epoch 862/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4790 - val_loss: 0.6422\n",
            "Epoch 863/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4573 - val_loss: 0.6451\n",
            "Epoch 864/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4758 - val_loss: 0.6400\n",
            "Epoch 865/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4841 - val_loss: 0.6277\n",
            "Epoch 866/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4811 - val_loss: 0.6487\n",
            "Epoch 867/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4707 - val_loss: 0.6222\n",
            "Epoch 868/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4686 - val_loss: 0.6600\n",
            "Epoch 869/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4645 - val_loss: 0.6389\n",
            "Epoch 870/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4825 - val_loss: 0.6282\n",
            "Epoch 871/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4748 - val_loss: 0.6288\n",
            "Epoch 872/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4654 - val_loss: 0.6505\n",
            "Epoch 873/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.4609 - val_loss: 0.6475\n",
            "Epoch 874/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4792 - val_loss: 0.6791\n",
            "Epoch 875/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4318 - val_loss: 0.6491\n",
            "Epoch 876/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4909 - val_loss: 0.6658\n",
            "Epoch 877/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4510 - val_loss: 0.6700\n",
            "Epoch 878/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4666 - val_loss: 0.6712\n",
            "Epoch 879/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4659 - val_loss: 0.6555\n",
            "Epoch 880/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4816 - val_loss: 0.6524\n",
            "Epoch 881/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.4755 - val_loss: 0.6442\n",
            "Epoch 882/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4637 - val_loss: 0.6422\n",
            "Epoch 883/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4713 - val_loss: 0.6403\n",
            "Epoch 884/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4839 - val_loss: 0.6470\n",
            "Epoch 885/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4631 - val_loss: 0.6560\n",
            "Epoch 886/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4718 - val_loss: 0.6727\n",
            "Epoch 887/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4819 - val_loss: 0.6389\n",
            "Epoch 888/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4592 - val_loss: 0.6123\n",
            "Epoch 889/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4659 - val_loss: 0.6410\n",
            "Epoch 890/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4475 - val_loss: 0.6497\n",
            "Epoch 891/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4607 - val_loss: 0.6447\n",
            "Epoch 892/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4703 - val_loss: 0.6351\n",
            "Epoch 893/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4768 - val_loss: 0.6578\n",
            "Epoch 894/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4661 - val_loss: 0.6611\n",
            "Epoch 895/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4681 - val_loss: 0.6632\n",
            "Epoch 896/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4486 - val_loss: 0.6243\n",
            "Epoch 897/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4635 - val_loss: 0.6188\n",
            "Epoch 898/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4555 - val_loss: 0.6374\n",
            "Epoch 899/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4485 - val_loss: 0.6260\n",
            "Epoch 900/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4653 - val_loss: 0.6320\n",
            "Epoch 901/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4576 - val_loss: 0.6567\n",
            "Epoch 902/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4765 - val_loss: 0.6228\n",
            "Epoch 903/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4552 - val_loss: 0.6547\n",
            "Epoch 904/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.4907 - val_loss: 0.6459\n",
            "Epoch 905/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4746 - val_loss: 0.6467\n",
            "Epoch 906/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4649 - val_loss: 0.6411\n",
            "Epoch 907/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4726 - val_loss: 0.6532\n",
            "Epoch 908/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4405 - val_loss: 0.6548\n",
            "Epoch 909/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4426 - val_loss: 0.6454\n",
            "Epoch 910/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4627 - val_loss: 0.6225\n",
            "Epoch 911/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4763 - val_loss: 0.6276\n",
            "Epoch 912/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4681 - val_loss: 0.6495\n",
            "Epoch 913/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4761 - val_loss: 0.6712\n",
            "Epoch 914/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4694 - val_loss: 0.6413\n",
            "Epoch 915/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4622 - val_loss: 0.6551\n",
            "Epoch 916/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4489 - val_loss: 0.6526\n",
            "Epoch 917/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4756 - val_loss: 0.6406\n",
            "Epoch 918/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.4586 - val_loss: 0.6551\n",
            "Epoch 919/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4505 - val_loss: 0.6339\n",
            "Epoch 920/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4647 - val_loss: 0.6328\n",
            "Epoch 921/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4649 - val_loss: 0.6425\n",
            "Epoch 922/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4519 - val_loss: 0.6455\n",
            "Epoch 923/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4428 - val_loss: 0.6404\n",
            "Epoch 924/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4525 - val_loss: 0.6549\n",
            "Epoch 925/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4681 - val_loss: 0.6581\n",
            "Epoch 926/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.4214 - val_loss: 0.6483\n",
            "Epoch 927/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4573 - val_loss: 0.6478\n",
            "Epoch 928/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4393 - val_loss: 0.6508\n",
            "Epoch 929/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4514 - val_loss: 0.6580\n",
            "Epoch 930/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4508 - val_loss: 0.6429\n",
            "Epoch 931/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4328 - val_loss: 0.6480\n",
            "Epoch 932/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4530 - val_loss: 0.6574\n",
            "Epoch 933/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4481 - val_loss: 0.6382\n",
            "Epoch 934/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.4598 - val_loss: 0.6525\n",
            "Epoch 935/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4508 - val_loss: 0.6536\n",
            "Epoch 936/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4528 - val_loss: 0.6462\n",
            "Epoch 937/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4401 - val_loss: 0.6545\n",
            "Epoch 938/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4602 - val_loss: 0.6409\n",
            "Epoch 939/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4357 - val_loss: 0.6440\n",
            "Epoch 940/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4437 - val_loss: 0.6518\n",
            "Epoch 941/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4360 - val_loss: 0.6457\n",
            "Epoch 942/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.4577 - val_loss: 0.6487\n",
            "Epoch 943/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4397 - val_loss: 0.6482\n",
            "Epoch 944/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4377 - val_loss: 0.6518\n",
            "Epoch 945/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4569 - val_loss: 0.6446\n",
            "Epoch 946/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4702 - val_loss: 0.6470\n",
            "Epoch 947/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4508 - val_loss: 0.6156\n",
            "Epoch 948/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4551 - val_loss: 0.6352\n",
            "Epoch 949/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.4420 - val_loss: 0.6383\n",
            "Epoch 950/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4554 - val_loss: 0.6366\n",
            "Epoch 951/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4717 - val_loss: 0.6423\n",
            "Epoch 952/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4586 - val_loss: 0.6368\n",
            "Epoch 953/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4570 - val_loss: 0.6344\n",
            "Epoch 954/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4572 - val_loss: 0.6363\n",
            "Epoch 955/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4645 - val_loss: 0.6379\n",
            "Epoch 956/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4517 - val_loss: 0.6370\n",
            "Epoch 957/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4282 - val_loss: 0.6329\n",
            "Epoch 958/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4564 - val_loss: 0.6294\n",
            "Epoch 959/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4521 - val_loss: 0.6326\n",
            "Epoch 960/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4375 - val_loss: 0.6596\n",
            "Epoch 961/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4375 - val_loss: 0.6524\n",
            "Epoch 962/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4563 - val_loss: 0.6351\n",
            "Epoch 963/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4459 - val_loss: 0.6435\n",
            "Epoch 964/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4401 - val_loss: 0.6318\n",
            "Epoch 965/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4411 - val_loss: 0.6589\n",
            "Epoch 966/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4459 - val_loss: 0.6636\n",
            "Epoch 967/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4598 - val_loss: 0.6424\n",
            "Epoch 968/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4382 - val_loss: 0.6606\n",
            "Epoch 969/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4421 - val_loss: 0.6408\n",
            "Epoch 970/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4681 - val_loss: 0.6523\n",
            "Epoch 971/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4437 - val_loss: 0.6511\n",
            "Epoch 972/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.4557 - val_loss: 0.6282\n",
            "Epoch 973/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4421 - val_loss: 0.6431\n",
            "Epoch 974/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4515 - val_loss: 0.6394\n",
            "Epoch 975/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4403 - val_loss: 0.6496\n",
            "Epoch 976/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4527 - val_loss: 0.6476\n",
            "Epoch 977/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4242 - val_loss: 0.6471\n",
            "Epoch 978/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4349 - val_loss: 0.6677\n",
            "Epoch 979/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4480 - val_loss: 0.6436\n",
            "Epoch 980/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4617 - val_loss: 0.6460\n",
            "Epoch 981/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4433 - val_loss: 0.6387\n",
            "Epoch 982/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4290 - val_loss: 0.6558\n",
            "Epoch 983/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4255 - val_loss: 0.6620\n",
            "Epoch 984/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4454 - val_loss: 0.6491\n",
            "Epoch 985/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4141 - val_loss: 0.6559\n",
            "Epoch 986/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4579 - val_loss: 0.6456\n",
            "Epoch 987/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4463 - val_loss: 0.6391\n",
            "Epoch 988/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.4576 - val_loss: 0.6558\n",
            "Epoch 989/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4493 - val_loss: 0.6468\n",
            "Epoch 990/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4529 - val_loss: 0.6329\n",
            "Epoch 991/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4442 - val_loss: 0.6430\n",
            "Epoch 992/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4366 - val_loss: 0.6257\n",
            "Epoch 993/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4416 - val_loss: 0.6311\n",
            "Epoch 994/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4405 - val_loss: 0.6442\n",
            "Epoch 995/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4284 - val_loss: 0.6277\n",
            "Epoch 996/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4583 - val_loss: 0.6484\n",
            "Epoch 997/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.4454 - val_loss: 0.6361\n",
            "Epoch 998/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4474 - val_loss: 0.6357\n",
            "Epoch 999/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4341 - val_loss: 0.6329\n",
            "Epoch 1000/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4181 - val_loss: 0.6518\n"
          ]
        }
      ],
      "source": [
        "#run call the transformer model\n",
        "#df_read = df.copy()\n",
        "#df_read=(df_read-df_read.mean())/df_read.std()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(df_read, test_size=0.2)\n",
        "\n",
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "test = df5.copy()\n",
        "\n",
        "Y_train = np.array(train['class'])\n",
        "X_train= np.array(train.drop(['class'],axis=1))\n",
        "X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
        "\n",
        "Y_val=np.array(test['class'])\n",
        "X_val = np.array(test.drop(['class'],axis=1))\n",
        "X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])\n",
        "NUM_LAYERS =  4\n",
        "\n",
        "D_MODEL = X_train.shape[2]\n",
        "NUM_HEADS =  4\n",
        "UNITS =  2048\n",
        "DROPOUT = 0.1 #0.1\n",
        "TIME_STEPS= X_train.shape[1]\n",
        "OUTPUT_SIZE=1\n",
        "batch_size=64\n",
        "\n",
        "model1 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=3,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "model2 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=5,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "model3 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=6,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "\n",
        "#run\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='mae') #org\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss='mse')\n",
        "# model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mae')\n",
        "#\n",
        "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=80, restore_best_weights=True)\n",
        "# history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "history = model2.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UxMyaAwpcqdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7894b6f-ef9d-4708-edc8-96be641ff68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00022308223218802945\n",
            "(1660, 1, 180)\n",
            "(415, 1, 180)\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "import time\n",
        "st = time.time()\n",
        "p1 = np.array(model2(X_val)).flatten()\n",
        "end = time.time()\n",
        "# print(end, st, len(p1))\n",
        "print((end-st)/len(p1))\n",
        "p2 = np.array(model2(X_train)).flatten()\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Say3gnZgi331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b3697a-d004-4f2a-d8ae-cc0bbe2d2475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.90139745]\n",
            " [0.90139745 1.        ]] SignificanceResult(statistic=0.8962028226775357, pvalue=0.0) 0.7171195900112904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8007729604471366, 0.7914683627015778, 0.5974075595240654)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import kendalltau\n",
        "print(np.corrcoef(p2, Y_train), stats.spearmanr(p2, Y_train), kendalltau(p2,Y_train).correlation)\n",
        "np.corrcoef(p1, Y_val)[1][0], stats.spearmanr(p1, Y_val).correlation, kendalltau(p1,Y_val).correlation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ST5WiH4KoeOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b707ed8a-2d4a-4801-81d2-ef9c7672f7e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1660, 1, 180)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ksXNLzMScqg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "5e1ca23a-8c99-4196-d5fb-3cf25aa1a8cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"audio_dm_3_featu_.jpg\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}