{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jP5uwXtZay_",
        "outputId": "db663b7e-d374-46af-a470-a364369e1af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRPF7T0b-QZ",
        "outputId": "cce8e77e-bf5f-48ef-a355-0703770a8e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spafe in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from spafe) (1.11.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from spafe) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "!pip install -U spafe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "s2K5aSHYb-Sm"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lfcc import lfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-xNIFhPtb-U5"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QgbvNKYAb-YM"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet50,ResNet101\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.model_selection import StratifiedKFold , KFold ,RepeatedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LXsQV7ivcIcc"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  return tf.matmul(attention_weights, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7WB2_09_e-qt"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# This allows to the transformer to know where there is real data and where it is padded\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PjEQIR8TcIe3"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8hF4xYdLcSQA"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "j7ewC3iBcSSo"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_8IFloblcSWJ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def encoder(time_steps,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            projection,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  if projection=='linear':\n",
        "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
        "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
        "    print('linear')\n",
        "\n",
        "  else:\n",
        "    projection=tf.identity(inputs)\n",
        "    print('none')\n",
        "\n",
        "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NXgJh2PpcIiK"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "def transformer(time_steps,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                output_size,\n",
        "                projection,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(tf.dtypes.cast(\n",
        "\n",
        "      #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
        "      # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked\n",
        "      tf.math.reduce_sum(\n",
        "      inputs,\n",
        "      axis=2,\n",
        "      keepdims=False,\n",
        "      name=None\n",
        "  ), tf.int32))\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      time_steps=time_steps,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      projection=projection,\n",
        "      name='encoder'\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  #We reshape for feeding our FC in the next step\n",
        "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
        "\n",
        "  #We predict our class\n",
        "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True, name=\"outputs\")(outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9H-kkyuLcbPZ"
      },
      "outputs": [],
      "source": [
        "#run\n",
        "# num_batch_size = 32\n",
        "# num_epochs = 500\n",
        "# N_SPLIT = 10\n",
        "# num_labels=5\n",
        "# num_classes=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Iw3KzecbRt",
        "outputId": "1191d42f-a028-44a7-dbb3-7340b4088de6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2075, 181)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "#ajit\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate5_spectral2.csv')\n",
        "# dm\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/2075_concatenate_dm_4featu.csv')\n",
        "\n",
        "\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/melspectogram_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/chroma_cqt_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/mfcc_simple_mean_newdm_2075.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/DatabaseDistorted/final_tii_all_finaldata/features/spectral_centroid_meandm_2075_200.csv')\n",
        "\n",
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HrbtbLXcbTx",
        "outputId": "c52fb7d2-b499-4ae5-9ba2-f8db4ba7d669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n",
            "(415, 181)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "siz=415\n",
        "df_read = df.copy()\n",
        "df1 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df1.index)\n",
        "df2 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df2.index)\n",
        "df3 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df3.index)\n",
        "df4 = df_read.sample(siz)\n",
        "df_read = df_read.drop(df4.index)\n",
        "df5 = df_read.copy()\n",
        "\n",
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)\n",
        "print(df4.shape)\n",
        "print(df5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j19bgqpcbWT",
        "outputId": "9cedb235-a17e-4f6f-c7fe-447ee77e5970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int64Index([ 881,  453, 2004, 1353,  281,  941, 1185, 1159, 1138,  599,\n",
            "            ...\n",
            "             834, 1730,  353, 1345, 1190, 1375,  185,  701, 1671, 1982],\n",
            "           dtype='int64', length=415)\n"
          ]
        }
      ],
      "source": [
        "q = list(df1.index)+list(df2.index)+list(df3.index)+list(df4.index)+list(df5.index)\n",
        "print(df1.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "uT0Jrfa5cbZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e9edb3-40e8-4e89-d229-a79c06fe08a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear\n",
            "linear\n",
            "linear\n",
            "Epoch 1/1000\n",
            "52/52 [==============================] - 39s 60ms/step - loss: 4.2899 - val_loss: 2.5788\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 2.2022 - val_loss: 1.9331\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 2.0613 - val_loss: 1.6841\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.9514 - val_loss: 1.7535\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.7933 - val_loss: 1.7957\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.7568 - val_loss: 1.6254\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.7501 - val_loss: 1.5773\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.7583 - val_loss: 1.6182\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.7618 - val_loss: 1.5535\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 1.6987 - val_loss: 1.4097\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.6469 - val_loss: 1.3821\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.6659 - val_loss: 1.5695\n",
            "Epoch 13/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.6107 - val_loss: 1.4476\n",
            "Epoch 14/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.5321 - val_loss: 1.5385\n",
            "Epoch 15/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 1.5780 - val_loss: 1.4859\n",
            "Epoch 16/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.5488 - val_loss: 1.3583\n",
            "Epoch 17/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.5800 - val_loss: 1.2901\n",
            "Epoch 18/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.5339 - val_loss: 1.3296\n",
            "Epoch 19/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.5089 - val_loss: 1.3896\n",
            "Epoch 20/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.5453 - val_loss: 1.3227\n",
            "Epoch 21/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.5092 - val_loss: 1.3459\n",
            "Epoch 22/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.5191 - val_loss: 1.2925\n",
            "Epoch 23/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 1.4809 - val_loss: 1.3688\n",
            "Epoch 24/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.5126 - val_loss: 1.3245\n",
            "Epoch 25/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.4374 - val_loss: 1.3786\n",
            "Epoch 26/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.4672 - val_loss: 1.2622\n",
            "Epoch 27/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.4033 - val_loss: 1.3285\n",
            "Epoch 28/1000\n",
            "52/52 [==============================] - 4s 69ms/step - loss: 1.4135 - val_loss: 1.3466\n",
            "Epoch 29/1000\n",
            "52/52 [==============================] - 3s 56ms/step - loss: 1.4166 - val_loss: 1.3428\n",
            "Epoch 30/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.4411 - val_loss: 1.3230\n",
            "Epoch 31/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.3854 - val_loss: 1.3193\n",
            "Epoch 32/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.3691 - val_loss: 1.2927\n",
            "Epoch 33/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.3818 - val_loss: 1.4035\n",
            "Epoch 34/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.4146 - val_loss: 1.3679\n",
            "Epoch 35/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.3593 - val_loss: 1.2899\n",
            "Epoch 36/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 1.3742 - val_loss: 1.2906\n",
            "Epoch 37/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 1.3309 - val_loss: 1.3101\n",
            "Epoch 38/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.3403 - val_loss: 1.3556\n",
            "Epoch 39/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.3542 - val_loss: 1.2345\n",
            "Epoch 40/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.3198 - val_loss: 1.3137\n",
            "Epoch 41/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.3211 - val_loss: 1.3345\n",
            "Epoch 42/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.2815 - val_loss: 1.2656\n",
            "Epoch 43/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 1.3659 - val_loss: 1.3179\n",
            "Epoch 44/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.3045 - val_loss: 1.3705\n",
            "Epoch 45/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.2880 - val_loss: 1.4134\n",
            "Epoch 46/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.2948 - val_loss: 1.2819\n",
            "Epoch 47/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.3307 - val_loss: 1.2853\n",
            "Epoch 48/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.2255 - val_loss: 1.2339\n",
            "Epoch 49/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 1.2465 - val_loss: 1.1667\n",
            "Epoch 50/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 1.2698 - val_loss: 1.2775\n",
            "Epoch 51/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.2355 - val_loss: 1.3733\n",
            "Epoch 52/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.2178 - val_loss: 1.4124\n",
            "Epoch 53/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.2681 - val_loss: 1.2869\n",
            "Epoch 54/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.2502 - val_loss: 1.2039\n",
            "Epoch 55/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.2216 - val_loss: 1.2423\n",
            "Epoch 56/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.2136 - val_loss: 1.2929\n",
            "Epoch 57/1000\n",
            "52/52 [==============================] - 3s 57ms/step - loss: 1.2205 - val_loss: 1.1930\n",
            "Epoch 58/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 1.2196 - val_loss: 1.1239\n",
            "Epoch 59/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1867 - val_loss: 1.2787\n",
            "Epoch 60/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1982 - val_loss: 1.3140\n",
            "Epoch 61/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.2093 - val_loss: 1.1622\n",
            "Epoch 62/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1770 - val_loss: 1.1464\n",
            "Epoch 63/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.1615 - val_loss: 1.2171\n",
            "Epoch 64/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 1.1784 - val_loss: 1.1459\n",
            "Epoch 65/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.1138 - val_loss: 1.1139\n",
            "Epoch 66/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1797 - val_loss: 1.0901\n",
            "Epoch 67/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1536 - val_loss: 1.0933\n",
            "Epoch 68/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1871 - val_loss: 1.1251\n",
            "Epoch 69/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.1598 - val_loss: 1.2795\n",
            "Epoch 70/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 1.1292 - val_loss: 1.1641\n",
            "Epoch 71/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 1.1619 - val_loss: 1.0732\n",
            "Epoch 72/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1280 - val_loss: 1.0929\n",
            "Epoch 73/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1184 - val_loss: 0.9811\n",
            "Epoch 74/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1048 - val_loss: 1.1504\n",
            "Epoch 75/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.1472 - val_loss: 1.0689\n",
            "Epoch 76/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.1202 - val_loss: 1.1031\n",
            "Epoch 77/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 1.0852 - val_loss: 1.0619\n",
            "Epoch 78/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 1.0959 - val_loss: 1.0583\n",
            "Epoch 79/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.1214 - val_loss: 1.1135\n",
            "Epoch 80/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 1.0806 - val_loss: 1.0066\n",
            "Epoch 81/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0504 - val_loss: 1.1466\n",
            "Epoch 82/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0504 - val_loss: 0.9866\n",
            "Epoch 83/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 1.0722 - val_loss: 1.0159\n",
            "Epoch 84/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 1.0701 - val_loss: 1.1160\n",
            "Epoch 85/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 1.0660 - val_loss: 0.9931\n",
            "Epoch 86/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.0186 - val_loss: 1.0849\n",
            "Epoch 87/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0614 - val_loss: 1.0111\n",
            "Epoch 88/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.0629 - val_loss: 1.0308\n",
            "Epoch 89/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0228 - val_loss: 1.0183\n",
            "Epoch 90/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0613 - val_loss: 1.0936\n",
            "Epoch 91/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 1.0293 - val_loss: 1.0365\n",
            "Epoch 92/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 1.0310 - val_loss: 0.9840\n",
            "Epoch 93/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 1.0168 - val_loss: 0.9951\n",
            "Epoch 94/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0599 - val_loss: 1.0347\n",
            "Epoch 95/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0279 - val_loss: 0.9928\n",
            "Epoch 96/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0388 - val_loss: 0.9746\n",
            "Epoch 97/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0255 - val_loss: 0.9162\n",
            "Epoch 98/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.0025 - val_loss: 1.0463\n",
            "Epoch 99/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.9991 - val_loss: 0.9699\n",
            "Epoch 100/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0065 - val_loss: 0.9299\n",
            "Epoch 101/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0042 - val_loss: 0.9469\n",
            "Epoch 102/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9991 - val_loss: 0.9605\n",
            "Epoch 103/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9680 - val_loss: 1.0758\n",
            "Epoch 104/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9873 - val_loss: 0.9453\n",
            "Epoch 105/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 1.0049 - val_loss: 0.9408\n",
            "Epoch 106/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 0.9817 - val_loss: 0.9985\n",
            "Epoch 107/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9754 - val_loss: 0.9817\n",
            "Epoch 108/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 1.0003 - val_loss: 0.9213\n",
            "Epoch 109/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9853 - val_loss: 0.9151\n",
            "Epoch 110/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9685 - val_loss: 0.9154\n",
            "Epoch 111/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9551 - val_loss: 0.9076\n",
            "Epoch 112/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.9667 - val_loss: 0.9997\n",
            "Epoch 113/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.9885 - val_loss: 0.8829\n",
            "Epoch 114/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9547 - val_loss: 0.8348\n",
            "Epoch 115/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.9804 - val_loss: 0.9180\n",
            "Epoch 116/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9497 - val_loss: 0.9072\n",
            "Epoch 117/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.9712 - val_loss: 0.9082\n",
            "Epoch 118/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.9706 - val_loss: 0.8817\n",
            "Epoch 119/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.9442 - val_loss: 0.9647\n",
            "Epoch 120/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.9188 - val_loss: 0.8634\n",
            "Epoch 121/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9476 - val_loss: 0.8822\n",
            "Epoch 122/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.9721 - val_loss: 0.8707\n",
            "Epoch 123/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9427 - val_loss: 0.8249\n",
            "Epoch 124/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9263 - val_loss: 0.8584\n",
            "Epoch 125/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9383 - val_loss: 0.8329\n",
            "Epoch 126/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.9813 - val_loss: 0.8836\n",
            "Epoch 127/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.9518 - val_loss: 0.8133\n",
            "Epoch 128/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9396 - val_loss: 0.9296\n",
            "Epoch 129/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9088 - val_loss: 0.8357\n",
            "Epoch 130/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9366 - val_loss: 0.9076\n",
            "Epoch 131/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9515 - val_loss: 0.8620\n",
            "Epoch 132/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9073 - val_loss: 0.8405\n",
            "Epoch 133/1000\n",
            "52/52 [==============================] - 3s 58ms/step - loss: 0.8947 - val_loss: 0.8369\n",
            "Epoch 134/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.8961 - val_loss: 0.8457\n",
            "Epoch 135/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9226 - val_loss: 0.8680\n",
            "Epoch 136/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9032 - val_loss: 0.8113\n",
            "Epoch 137/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.9030 - val_loss: 0.8630\n",
            "Epoch 138/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9362 - val_loss: 0.8035\n",
            "Epoch 139/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.9102 - val_loss: 0.8361\n",
            "Epoch 140/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.9086 - val_loss: 0.8459\n",
            "Epoch 141/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.8920 - val_loss: 0.8005\n",
            "Epoch 142/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8826 - val_loss: 0.8230\n",
            "Epoch 143/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9136 - val_loss: 0.7975\n",
            "Epoch 144/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9156 - val_loss: 0.8306\n",
            "Epoch 145/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.9105 - val_loss: 0.8425\n",
            "Epoch 146/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.9378 - val_loss: 0.8331\n",
            "Epoch 147/1000\n",
            "52/52 [==============================] - 3s 59ms/step - loss: 0.9181 - val_loss: 0.8522\n",
            "Epoch 148/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8841 - val_loss: 0.8405\n",
            "Epoch 149/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8792 - val_loss: 0.7677\n",
            "Epoch 150/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8806 - val_loss: 0.8372\n",
            "Epoch 151/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8710 - val_loss: 0.8146\n",
            "Epoch 152/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8788 - val_loss: 0.8580\n",
            "Epoch 153/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.8532 - val_loss: 0.8469\n",
            "Epoch 154/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.8770 - val_loss: 0.8011\n",
            "Epoch 155/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8789 - val_loss: 0.8030\n",
            "Epoch 156/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8689 - val_loss: 0.8109\n",
            "Epoch 157/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8576 - val_loss: 0.7966\n",
            "Epoch 158/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8925 - val_loss: 0.8365\n",
            "Epoch 159/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8864 - val_loss: 0.7732\n",
            "Epoch 160/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.8764 - val_loss: 0.7919\n",
            "Epoch 161/1000\n",
            "52/52 [==============================] - 3s 57ms/step - loss: 0.8537 - val_loss: 0.8088\n",
            "Epoch 162/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.8819 - val_loss: 0.8170\n",
            "Epoch 163/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8606 - val_loss: 0.7786\n",
            "Epoch 164/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8844 - val_loss: 0.8149\n",
            "Epoch 165/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8625 - val_loss: 0.8503\n",
            "Epoch 166/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8610 - val_loss: 0.7979\n",
            "Epoch 167/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.8784 - val_loss: 0.7865\n",
            "Epoch 168/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.8523 - val_loss: 0.8798\n",
            "Epoch 169/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8954 - val_loss: 0.7961\n",
            "Epoch 170/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8604 - val_loss: 0.8088\n",
            "Epoch 171/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8389 - val_loss: 0.8514\n",
            "Epoch 172/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8543 - val_loss: 0.8177\n",
            "Epoch 173/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.8427 - val_loss: 0.7783\n",
            "Epoch 174/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.8237 - val_loss: 0.8523\n",
            "Epoch 175/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.8574 - val_loss: 0.7793\n",
            "Epoch 176/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8550 - val_loss: 0.8406\n",
            "Epoch 177/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8323 - val_loss: 0.8968\n",
            "Epoch 178/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8611 - val_loss: 0.7959\n",
            "Epoch 179/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8663 - val_loss: 0.7702\n",
            "Epoch 180/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8520 - val_loss: 0.7803\n",
            "Epoch 181/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.8597 - val_loss: 0.7847\n",
            "Epoch 182/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.8239 - val_loss: 0.7894\n",
            "Epoch 183/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8456 - val_loss: 0.7780\n",
            "Epoch 184/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8464 - val_loss: 0.7810\n",
            "Epoch 185/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8477 - val_loss: 0.7920\n",
            "Epoch 186/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8267 - val_loss: 0.7796\n",
            "Epoch 187/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.8522 - val_loss: 0.7786\n",
            "Epoch 188/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.8522 - val_loss: 0.8005\n",
            "Epoch 189/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.8363 - val_loss: 0.7488\n",
            "Epoch 190/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8383 - val_loss: 0.7816\n",
            "Epoch 191/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8562 - val_loss: 0.7933\n",
            "Epoch 192/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8340 - val_loss: 0.7525\n",
            "Epoch 193/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8345 - val_loss: 0.7658\n",
            "Epoch 194/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.8266 - val_loss: 0.7859\n",
            "Epoch 195/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.8362 - val_loss: 0.7657\n",
            "Epoch 196/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.8349 - val_loss: 0.7618\n",
            "Epoch 197/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8176 - val_loss: 0.8129\n",
            "Epoch 198/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8311 - val_loss: 0.8112\n",
            "Epoch 199/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8119 - val_loss: 0.7897\n",
            "Epoch 200/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.8156 - val_loss: 0.7656\n",
            "Epoch 201/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7998 - val_loss: 0.7930\n",
            "Epoch 202/1000\n",
            "52/52 [==============================] - 3s 57ms/step - loss: 0.8361 - val_loss: 0.7658\n",
            "Epoch 203/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.7900 - val_loss: 0.8465\n",
            "Epoch 204/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8417 - val_loss: 0.7792\n",
            "Epoch 205/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8247 - val_loss: 0.7485\n",
            "Epoch 206/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8245 - val_loss: 0.7935\n",
            "Epoch 207/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8090 - val_loss: 0.7674\n",
            "Epoch 208/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.8040 - val_loss: 0.8462\n",
            "Epoch 209/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.7977 - val_loss: 0.7685\n",
            "Epoch 210/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.8340 - val_loss: 0.7577\n",
            "Epoch 211/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8216 - val_loss: 0.7721\n",
            "Epoch 212/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8095 - val_loss: 0.7423\n",
            "Epoch 213/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7910 - val_loss: 0.7981\n",
            "Epoch 214/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7842 - val_loss: 0.8179\n",
            "Epoch 215/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.8325 - val_loss: 0.7525\n",
            "Epoch 216/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.8064 - val_loss: 0.7530\n",
            "Epoch 217/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7937 - val_loss: 0.7953\n",
            "Epoch 218/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7731 - val_loss: 0.7439\n",
            "Epoch 219/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8120 - val_loss: 0.7758\n",
            "Epoch 220/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7909 - val_loss: 0.7868\n",
            "Epoch 221/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.8130 - val_loss: 0.8112\n",
            "Epoch 222/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.8072 - val_loss: 0.7986\n",
            "Epoch 223/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.8208 - val_loss: 0.7664\n",
            "Epoch 224/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8152 - val_loss: 0.7424\n",
            "Epoch 225/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8017 - val_loss: 0.7578\n",
            "Epoch 226/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7576 - val_loss: 0.7608\n",
            "Epoch 227/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8013 - val_loss: 0.7385\n",
            "Epoch 228/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.8047 - val_loss: 0.7771\n",
            "Epoch 229/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.7621 - val_loss: 0.7859\n",
            "Epoch 230/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 0.7841 - val_loss: 0.7584\n",
            "Epoch 231/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7859 - val_loss: 0.7706\n",
            "Epoch 232/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.8034 - val_loss: 0.7410\n",
            "Epoch 233/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7566 - val_loss: 0.7705\n",
            "Epoch 234/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7864 - val_loss: 0.7421\n",
            "Epoch 235/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7999 - val_loss: 0.7425\n",
            "Epoch 236/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.7989 - val_loss: 0.7769\n",
            "Epoch 237/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.8021 - val_loss: 0.7242\n",
            "Epoch 238/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7938 - val_loss: 0.7553\n",
            "Epoch 239/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7486 - val_loss: 0.7773\n",
            "Epoch 240/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7587 - val_loss: 0.7377\n",
            "Epoch 241/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7771 - val_loss: 0.7393\n",
            "Epoch 242/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7628 - val_loss: 0.7436\n",
            "Epoch 243/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.7598 - val_loss: 0.7763\n",
            "Epoch 244/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.7858 - val_loss: 0.7876\n",
            "Epoch 245/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7841 - val_loss: 0.7887\n",
            "Epoch 246/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7670 - val_loss: 0.7295\n",
            "Epoch 247/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7468 - val_loss: 0.7199\n",
            "Epoch 248/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7629 - val_loss: 0.7505\n",
            "Epoch 249/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7385 - val_loss: 0.8039\n",
            "Epoch 250/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.7909 - val_loss: 0.7413\n",
            "Epoch 251/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.7796 - val_loss: 0.7332\n",
            "Epoch 252/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7562 - val_loss: 0.7633\n",
            "Epoch 253/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7575 - val_loss: 0.7261\n",
            "Epoch 254/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7638 - val_loss: 0.7479\n",
            "Epoch 255/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7634 - val_loss: 0.7462\n",
            "Epoch 256/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.7669 - val_loss: 0.7614\n",
            "Epoch 257/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.7717 - val_loss: 0.7163\n",
            "Epoch 258/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.7455 - val_loss: 0.7379\n",
            "Epoch 259/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7312 - val_loss: 0.7111\n",
            "Epoch 260/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7753 - val_loss: 0.7874\n",
            "Epoch 261/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7627 - val_loss: 0.7690\n",
            "Epoch 262/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7673 - val_loss: 0.7245\n",
            "Epoch 263/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.7722 - val_loss: 0.7337\n",
            "Epoch 264/1000\n",
            "52/52 [==============================] - 3s 56ms/step - loss: 0.7467 - val_loss: 0.7488\n",
            "Epoch 265/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7323 - val_loss: 0.7702\n",
            "Epoch 266/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7417 - val_loss: 0.7532\n",
            "Epoch 267/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7312 - val_loss: 0.7193\n",
            "Epoch 268/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7728 - val_loss: 0.7517\n",
            "Epoch 269/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.7664 - val_loss: 0.7604\n",
            "Epoch 270/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.7561 - val_loss: 0.7540\n",
            "Epoch 271/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.7332 - val_loss: 0.7469\n",
            "Epoch 272/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7394 - val_loss: 0.7794\n",
            "Epoch 273/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7415 - val_loss: 0.7718\n",
            "Epoch 274/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7235 - val_loss: 0.7465\n",
            "Epoch 275/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7339 - val_loss: 0.7538\n",
            "Epoch 276/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7660 - val_loss: 0.7337\n",
            "Epoch 277/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.7302 - val_loss: 0.7505\n",
            "Epoch 278/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.7362 - val_loss: 0.7738\n",
            "Epoch 279/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7409 - val_loss: 0.7309\n",
            "Epoch 280/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7277 - val_loss: 0.7477\n",
            "Epoch 281/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7759 - val_loss: 0.7260\n",
            "Epoch 282/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7287 - val_loss: 0.7287\n",
            "Epoch 283/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7160 - val_loss: 0.7618\n",
            "Epoch 284/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.7229 - val_loss: 0.7166\n",
            "Epoch 285/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.7450 - val_loss: 0.7199\n",
            "Epoch 286/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7120 - val_loss: 0.7184\n",
            "Epoch 287/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6975 - val_loss: 0.7381\n",
            "Epoch 288/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7498 - val_loss: 0.7386\n",
            "Epoch 289/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7521 - val_loss: 0.7241\n",
            "Epoch 290/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7390 - val_loss: 0.7298\n",
            "Epoch 291/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.7260 - val_loss: 0.7720\n",
            "Epoch 292/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.7184 - val_loss: 0.7409\n",
            "Epoch 293/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7289 - val_loss: 0.7068\n",
            "Epoch 294/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.7176 - val_loss: 0.7372\n",
            "Epoch 295/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7127 - val_loss: 0.7399\n",
            "Epoch 296/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7219 - val_loss: 0.7158\n",
            "Epoch 297/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.7304 - val_loss: 0.7177\n",
            "Epoch 298/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.7181 - val_loss: 0.7186\n",
            "Epoch 299/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7129 - val_loss: 0.7520\n",
            "Epoch 300/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7214 - val_loss: 0.7128\n",
            "Epoch 301/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7604 - val_loss: 0.7107\n",
            "Epoch 302/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7370 - val_loss: 0.7183\n",
            "Epoch 303/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7185 - val_loss: 0.7420\n",
            "Epoch 304/1000\n",
            "52/52 [==============================] - 3s 59ms/step - loss: 0.7152 - val_loss: 0.7289\n",
            "Epoch 305/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.7313 - val_loss: 0.7476\n",
            "Epoch 306/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7294 - val_loss: 0.7414\n",
            "Epoch 307/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7162 - val_loss: 0.7626\n",
            "Epoch 308/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7225 - val_loss: 0.7052\n",
            "Epoch 309/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7144 - val_loss: 0.7527\n",
            "Epoch 310/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.7267 - val_loss: 0.7180\n",
            "Epoch 311/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.7512 - val_loss: 0.7126\n",
            "Epoch 312/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7087 - val_loss: 0.7306\n",
            "Epoch 313/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.7185 - val_loss: 0.7608\n",
            "Epoch 314/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6995 - val_loss: 0.6988\n",
            "Epoch 315/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.7196 - val_loss: 0.7454\n",
            "Epoch 316/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.7382 - val_loss: 0.7234\n",
            "Epoch 317/1000\n",
            "52/52 [==============================] - 3s 62ms/step - loss: 0.7075 - val_loss: 0.7407\n",
            "Epoch 318/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6984 - val_loss: 0.7568\n",
            "Epoch 319/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7223 - val_loss: 0.7217\n",
            "Epoch 320/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7028 - val_loss: 0.7391\n",
            "Epoch 321/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7020 - val_loss: 0.7116\n",
            "Epoch 322/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7245 - val_loss: 0.7115\n",
            "Epoch 323/1000\n",
            "52/52 [==============================] - 3s 59ms/step - loss: 0.6918 - val_loss: 0.6980\n",
            "Epoch 324/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.7111 - val_loss: 0.6931\n",
            "Epoch 325/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7071 - val_loss: 0.7260\n",
            "Epoch 326/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7150 - val_loss: 0.7204\n",
            "Epoch 327/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.7263 - val_loss: 0.7158\n",
            "Epoch 328/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6850 - val_loss: 0.6992\n",
            "Epoch 329/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.7207 - val_loss: 0.7133\n",
            "Epoch 330/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.6680 - val_loss: 0.7294\n",
            "Epoch 331/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7163 - val_loss: 0.7239\n",
            "Epoch 332/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6911 - val_loss: 0.7406\n",
            "Epoch 333/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6789 - val_loss: 0.7683\n",
            "Epoch 334/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6853 - val_loss: 0.7026\n",
            "Epoch 335/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.7223 - val_loss: 0.7167\n",
            "Epoch 336/1000\n",
            "52/52 [==============================] - 3s 61ms/step - loss: 0.6914 - val_loss: 0.7342\n",
            "Epoch 337/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6614 - val_loss: 0.7237\n",
            "Epoch 338/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6942 - val_loss: 0.7305\n",
            "Epoch 339/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6848 - val_loss: 0.7350\n",
            "Epoch 340/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6851 - val_loss: 0.7223\n",
            "Epoch 341/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7035 - val_loss: 0.7498\n",
            "Epoch 342/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.6859 - val_loss: 0.7113\n",
            "Epoch 343/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.6993 - val_loss: 0.7264\n",
            "Epoch 344/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7103 - val_loss: 0.7507\n",
            "Epoch 345/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6716 - val_loss: 0.7319\n",
            "Epoch 346/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6827 - val_loss: 0.7055\n",
            "Epoch 347/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6867 - val_loss: 0.7331\n",
            "Epoch 348/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6835 - val_loss: 0.6910\n",
            "Epoch 349/1000\n",
            "52/52 [==============================] - 3s 57ms/step - loss: 0.7031 - val_loss: 0.7040\n",
            "Epoch 350/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6760 - val_loss: 0.6858\n",
            "Epoch 351/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6765 - val_loss: 0.7249\n",
            "Epoch 352/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6586 - val_loss: 0.6910\n",
            "Epoch 353/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6767 - val_loss: 0.7125\n",
            "Epoch 354/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6676 - val_loss: 0.6930\n",
            "Epoch 355/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.6773 - val_loss: 0.6915\n",
            "Epoch 356/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.7124 - val_loss: 0.7208\n",
            "Epoch 357/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6703 - val_loss: 0.7085\n",
            "Epoch 358/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.7006 - val_loss: 0.7378\n",
            "Epoch 359/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6967 - val_loss: 0.6865\n",
            "Epoch 360/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6824 - val_loss: 0.6880\n",
            "Epoch 361/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.7111 - val_loss: 0.7175\n",
            "Epoch 362/1000\n",
            "52/52 [==============================] - 3s 56ms/step - loss: 0.6769 - val_loss: 0.6929\n",
            "Epoch 363/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.6745 - val_loss: 0.7189\n",
            "Epoch 364/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6748 - val_loss: 0.7194\n",
            "Epoch 365/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6834 - val_loss: 0.7075\n",
            "Epoch 366/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6923 - val_loss: 0.7319\n",
            "Epoch 367/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6797 - val_loss: 0.7055\n",
            "Epoch 368/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.6786 - val_loss: 0.7156\n",
            "Epoch 369/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.6769 - val_loss: 0.6927\n",
            "Epoch 370/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6793 - val_loss: 0.6983\n",
            "Epoch 371/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6883 - val_loss: 0.7044\n",
            "Epoch 372/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6918 - val_loss: 0.6933\n",
            "Epoch 373/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6781 - val_loss: 0.7228\n",
            "Epoch 374/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6467 - val_loss: 0.7060\n",
            "Epoch 375/1000\n",
            "52/52 [==============================] - 3s 58ms/step - loss: 0.6640 - val_loss: 0.7352\n",
            "Epoch 376/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6672 - val_loss: 0.6997\n",
            "Epoch 377/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6703 - val_loss: 0.6999\n",
            "Epoch 378/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6737 - val_loss: 0.7224\n",
            "Epoch 379/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6674 - val_loss: 0.7162\n",
            "Epoch 380/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6711 - val_loss: 0.7023\n",
            "Epoch 381/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.6849 - val_loss: 0.7294\n",
            "Epoch 382/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.6694 - val_loss: 0.7207\n",
            "Epoch 383/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6856 - val_loss: 0.7127\n",
            "Epoch 384/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6508 - val_loss: 0.7089\n",
            "Epoch 385/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6641 - val_loss: 0.7256\n",
            "Epoch 386/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6529 - val_loss: 0.7077\n",
            "Epoch 387/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6561 - val_loss: 0.7488\n",
            "Epoch 388/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.6665 - val_loss: 0.7254\n",
            "Epoch 389/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.6691 - val_loss: 0.7148\n",
            "Epoch 390/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6640 - val_loss: 0.7214\n",
            "Epoch 391/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6521 - val_loss: 0.6964\n",
            "Epoch 392/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6671 - val_loss: 0.7668\n",
            "Epoch 393/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6656 - val_loss: 0.7104\n",
            "Epoch 394/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6691 - val_loss: 0.7076\n",
            "Epoch 395/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.6199 - val_loss: 0.7093\n",
            "Epoch 396/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.6613 - val_loss: 0.7105\n",
            "Epoch 397/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6607 - val_loss: 0.6922\n",
            "Epoch 398/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6603 - val_loss: 0.6910\n",
            "Epoch 399/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6832 - val_loss: 0.7472\n",
            "Epoch 400/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6875 - val_loss: 0.7174\n",
            "Epoch 401/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6709 - val_loss: 0.6901\n",
            "Epoch 402/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.6669 - val_loss: 0.7332\n",
            "Epoch 403/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.6498 - val_loss: 0.7173\n",
            "Epoch 404/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6413 - val_loss: 0.7119\n",
            "Epoch 405/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6663 - val_loss: 0.7095\n",
            "Epoch 406/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6619 - val_loss: 0.6838\n",
            "Epoch 407/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6758 - val_loss: 0.7112\n",
            "Epoch 408/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6247 - val_loss: 0.7270\n",
            "Epoch 409/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.6508 - val_loss: 0.7092\n",
            "Epoch 410/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.6460 - val_loss: 0.7373\n",
            "Epoch 411/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6426 - val_loss: 0.7013\n",
            "Epoch 412/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6468 - val_loss: 0.7394\n",
            "Epoch 413/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6726 - val_loss: 0.7115\n",
            "Epoch 414/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6495 - val_loss: 0.7202\n",
            "Epoch 415/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6323 - val_loss: 0.7587\n",
            "Epoch 416/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.6510 - val_loss: 0.7255\n",
            "Epoch 417/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6446 - val_loss: 0.7196\n",
            "Epoch 418/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6371 - val_loss: 0.7297\n",
            "Epoch 419/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6665 - val_loss: 0.7094\n",
            "Epoch 420/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6652 - val_loss: 0.7230\n",
            "Epoch 421/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6328 - val_loss: 0.7221\n",
            "Epoch 422/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6351 - val_loss: 0.6994\n",
            "Epoch 423/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.6473 - val_loss: 0.6945\n",
            "Epoch 424/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6508 - val_loss: 0.6927\n",
            "Epoch 425/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6538 - val_loss: 0.7219\n",
            "Epoch 426/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6469 - val_loss: 0.6974\n",
            "Epoch 427/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6420 - val_loss: 0.7007\n",
            "Epoch 428/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6546 - val_loss: 0.6972\n",
            "Epoch 429/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.6342 - val_loss: 0.7242\n",
            "Epoch 430/1000\n",
            "52/52 [==============================] - 3s 48ms/step - loss: 0.6345 - val_loss: 0.6858\n",
            "Epoch 431/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6473 - val_loss: 0.7247\n",
            "Epoch 432/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6516 - val_loss: 0.7252\n",
            "Epoch 433/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6210 - val_loss: 0.6969\n",
            "Epoch 434/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6287 - val_loss: 0.7257\n",
            "Epoch 435/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6248 - val_loss: 0.6879\n",
            "Epoch 436/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.6332 - val_loss: 0.7168\n",
            "Epoch 437/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6390 - val_loss: 0.7237\n",
            "Epoch 438/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6504 - val_loss: 0.7052\n",
            "Epoch 439/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6245 - val_loss: 0.6999\n",
            "Epoch 440/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6683 - val_loss: 0.7045\n",
            "Epoch 441/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6288 - val_loss: 0.6983\n",
            "Epoch 442/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6487 - val_loss: 0.7008\n",
            "Epoch 443/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.6219 - val_loss: 0.7199\n",
            "Epoch 444/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.6458 - val_loss: 0.7233\n",
            "Epoch 445/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6370 - val_loss: 0.7180\n",
            "Epoch 446/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6250 - val_loss: 0.7151\n",
            "Epoch 447/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6503 - val_loss: 0.7282\n",
            "Epoch 448/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6546 - val_loss: 0.6864\n",
            "Epoch 449/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6360 - val_loss: 0.7038\n",
            "Epoch 450/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.6283 - val_loss: 0.7033\n",
            "Epoch 451/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.6251 - val_loss: 0.7099\n",
            "Epoch 452/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6383 - val_loss: 0.6920\n",
            "Epoch 453/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6411 - val_loss: 0.6972\n",
            "Epoch 454/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6258 - val_loss: 0.7074\n",
            "Epoch 455/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6306 - val_loss: 0.6917\n",
            "Epoch 456/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6053 - val_loss: 0.6959\n",
            "Epoch 457/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.6583 - val_loss: 0.7043\n",
            "Epoch 458/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.6357 - val_loss: 0.7089\n",
            "Epoch 459/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6341 - val_loss: 0.7003\n",
            "Epoch 460/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6510 - val_loss: 0.6891\n",
            "Epoch 461/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6318 - val_loss: 0.7015\n",
            "Epoch 462/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6143 - val_loss: 0.7176\n",
            "Epoch 463/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6080 - val_loss: 0.6976\n",
            "Epoch 464/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.6471 - val_loss: 0.7070\n",
            "Epoch 465/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.6414 - val_loss: 0.6729\n",
            "Epoch 466/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6439 - val_loss: 0.6592\n",
            "Epoch 467/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6454 - val_loss: 0.6769\n",
            "Epoch 468/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6261 - val_loss: 0.6878\n",
            "Epoch 469/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6235 - val_loss: 0.6843\n",
            "Epoch 470/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6140 - val_loss: 0.6968\n",
            "Epoch 471/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.6387 - val_loss: 0.7011\n",
            "Epoch 472/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.6456 - val_loss: 0.6948\n",
            "Epoch 473/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6252 - val_loss: 0.6817\n",
            "Epoch 474/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.6193 - val_loss: 0.7405\n",
            "Epoch 475/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6295 - val_loss: 0.6822\n",
            "Epoch 476/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6472 - val_loss: 0.6813\n",
            "Epoch 477/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6088 - val_loss: 0.7134\n",
            "Epoch 478/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.6415 - val_loss: 0.7083\n",
            "Epoch 479/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.6170 - val_loss: 0.6990\n",
            "Epoch 480/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6290 - val_loss: 0.6898\n",
            "Epoch 481/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6358 - val_loss: 0.6849\n",
            "Epoch 482/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6226 - val_loss: 0.7261\n",
            "Epoch 483/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6442 - val_loss: 0.7016\n",
            "Epoch 484/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6183 - val_loss: 0.6870\n",
            "Epoch 485/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.6041 - val_loss: 0.7260\n",
            "Epoch 486/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.6158 - val_loss: 0.7096\n",
            "Epoch 487/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6101 - val_loss: 0.7283\n",
            "Epoch 488/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6245 - val_loss: 0.7078\n",
            "Epoch 489/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6145 - val_loss: 0.6806\n",
            "Epoch 490/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6078 - val_loss: 0.7074\n",
            "Epoch 491/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5939 - val_loss: 0.6763\n",
            "Epoch 492/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.6372 - val_loss: 0.6698\n",
            "Epoch 493/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.6180 - val_loss: 0.6867\n",
            "Epoch 494/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5938 - val_loss: 0.7011\n",
            "Epoch 495/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6195 - val_loss: 0.6967\n",
            "Epoch 496/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6044 - val_loss: 0.7186\n",
            "Epoch 497/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6062 - val_loss: 0.6795\n",
            "Epoch 498/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5775 - val_loss: 0.7071\n",
            "Epoch 499/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.6130 - val_loss: 0.6809\n",
            "Epoch 500/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.6069 - val_loss: 0.6816\n",
            "Epoch 501/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6119 - val_loss: 0.6706\n",
            "Epoch 502/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6014 - val_loss: 0.6760\n",
            "Epoch 503/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5927 - val_loss: 0.7028\n",
            "Epoch 504/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6101 - val_loss: 0.7270\n",
            "Epoch 505/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6071 - val_loss: 0.6844\n",
            "Epoch 506/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.6089 - val_loss: 0.6779\n",
            "Epoch 507/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.6032 - val_loss: 0.6888\n",
            "Epoch 508/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6017 - val_loss: 0.6839\n",
            "Epoch 509/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6118 - val_loss: 0.6888\n",
            "Epoch 510/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5872 - val_loss: 0.7078\n",
            "Epoch 511/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6180 - val_loss: 0.6900\n",
            "Epoch 512/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6303 - val_loss: 0.6985\n",
            "Epoch 513/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5941 - val_loss: 0.6800\n",
            "Epoch 514/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.6237 - val_loss: 0.7161\n",
            "Epoch 515/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.6001 - val_loss: 0.6584\n",
            "Epoch 516/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5799 - val_loss: 0.6707\n",
            "Epoch 517/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5974 - val_loss: 0.6759\n",
            "Epoch 518/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5761 - val_loss: 0.6653\n",
            "Epoch 519/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5963 - val_loss: 0.6960\n",
            "Epoch 520/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5862 - val_loss: 0.7157\n",
            "Epoch 521/1000\n",
            "52/52 [==============================] - 3s 59ms/step - loss: 0.6039 - val_loss: 0.6677\n",
            "Epoch 522/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.6162 - val_loss: 0.6977\n",
            "Epoch 523/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5746 - val_loss: 0.7146\n",
            "Epoch 524/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6066 - val_loss: 0.6523\n",
            "Epoch 525/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6257 - val_loss: 0.6816\n",
            "Epoch 526/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6091 - val_loss: 0.6822\n",
            "Epoch 527/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.6283 - val_loss: 0.6842\n",
            "Epoch 528/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.5999 - val_loss: 0.6773\n",
            "Epoch 529/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.6089 - val_loss: 0.7034\n",
            "Epoch 530/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6069 - val_loss: 0.6653\n",
            "Epoch 531/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5942 - val_loss: 0.6689\n",
            "Epoch 532/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6087 - val_loss: 0.6738\n",
            "Epoch 533/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5579 - val_loss: 0.6805\n",
            "Epoch 534/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5827 - val_loss: 0.7230\n",
            "Epoch 535/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.5878 - val_loss: 0.6767\n",
            "Epoch 536/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.5759 - val_loss: 0.6821\n",
            "Epoch 537/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5747 - val_loss: 0.7094\n",
            "Epoch 538/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.6133 - val_loss: 0.6840\n",
            "Epoch 539/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5884 - val_loss: 0.6963\n",
            "Epoch 540/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5973 - val_loss: 0.6887\n",
            "Epoch 541/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5968 - val_loss: 0.6539\n",
            "Epoch 542/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.6094 - val_loss: 0.6704\n",
            "Epoch 543/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5886 - val_loss: 0.6937\n",
            "Epoch 544/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5882 - val_loss: 0.6866\n",
            "Epoch 545/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5869 - val_loss: 0.7180\n",
            "Epoch 546/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5840 - val_loss: 0.6858\n",
            "Epoch 547/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5962 - val_loss: 0.6917\n",
            "Epoch 548/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5822 - val_loss: 0.6880\n",
            "Epoch 549/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5933 - val_loss: 0.7142\n",
            "Epoch 550/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.5802 - val_loss: 0.6971\n",
            "Epoch 551/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5877 - val_loss: 0.6850\n",
            "Epoch 552/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5795 - val_loss: 0.6830\n",
            "Epoch 553/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5806 - val_loss: 0.6671\n",
            "Epoch 554/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5816 - val_loss: 0.6928\n",
            "Epoch 555/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5755 - val_loss: 0.6676\n",
            "Epoch 556/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.5993 - val_loss: 0.6678\n",
            "Epoch 557/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5882 - val_loss: 0.7085\n",
            "Epoch 558/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5842 - val_loss: 0.6644\n",
            "Epoch 559/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6026 - val_loss: 0.6759\n",
            "Epoch 560/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5759 - val_loss: 0.6796\n",
            "Epoch 561/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5859 - val_loss: 0.6643\n",
            "Epoch 562/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5836 - val_loss: 0.6651\n",
            "Epoch 563/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.6044 - val_loss: 0.6751\n",
            "Epoch 564/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5858 - val_loss: 0.6646\n",
            "Epoch 565/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5797 - val_loss: 0.6773\n",
            "Epoch 566/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6116 - val_loss: 0.6616\n",
            "Epoch 567/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5878 - val_loss: 0.6766\n",
            "Epoch 568/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5827 - val_loss: 0.6651\n",
            "Epoch 569/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5865 - val_loss: 0.6477\n",
            "Epoch 570/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5961 - val_loss: 0.7231\n",
            "Epoch 571/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5729 - val_loss: 0.6661\n",
            "Epoch 572/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6020 - val_loss: 0.6721\n",
            "Epoch 573/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5711 - val_loss: 0.6708\n",
            "Epoch 574/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5725 - val_loss: 0.6718\n",
            "Epoch 575/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5849 - val_loss: 0.6438\n",
            "Epoch 576/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5780 - val_loss: 0.6569\n",
            "Epoch 577/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.5670 - val_loss: 0.6690\n",
            "Epoch 578/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.5965 - val_loss: 0.6679\n",
            "Epoch 579/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5813 - val_loss: 0.6406\n",
            "Epoch 580/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.6110 - val_loss: 0.6865\n",
            "Epoch 581/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.6000 - val_loss: 0.6762\n",
            "Epoch 582/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5560 - val_loss: 0.7182\n",
            "Epoch 583/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5730 - val_loss: 0.6738\n",
            "Epoch 584/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5581 - val_loss: 0.6894\n",
            "Epoch 585/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5649 - val_loss: 0.6701\n",
            "Epoch 586/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5657 - val_loss: 0.6512\n",
            "Epoch 587/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5965 - val_loss: 0.6641\n",
            "Epoch 588/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5724 - val_loss: 0.6415\n",
            "Epoch 589/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5751 - val_loss: 0.6526\n",
            "Epoch 590/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5717 - val_loss: 0.6822\n",
            "Epoch 591/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5792 - val_loss: 0.6642\n",
            "Epoch 592/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5948 - val_loss: 0.6849\n",
            "Epoch 593/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.5781 - val_loss: 0.6696\n",
            "Epoch 594/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5635 - val_loss: 0.6697\n",
            "Epoch 595/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5685 - val_loss: 0.6841\n",
            "Epoch 596/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5651 - val_loss: 0.6766\n",
            "Epoch 597/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5827 - val_loss: 0.6582\n",
            "Epoch 598/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5850 - val_loss: 0.6872\n",
            "Epoch 599/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.5723 - val_loss: 0.6627\n",
            "Epoch 600/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5729 - val_loss: 0.6641\n",
            "Epoch 601/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5722 - val_loss: 0.6607\n",
            "Epoch 602/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5942 - val_loss: 0.6771\n",
            "Epoch 603/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5859 - val_loss: 0.6758\n",
            "Epoch 604/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5595 - val_loss: 0.6516\n",
            "Epoch 605/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5723 - val_loss: 0.6477\n",
            "Epoch 606/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5839 - val_loss: 0.6750\n",
            "Epoch 607/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.5667 - val_loss: 0.6862\n",
            "Epoch 608/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5670 - val_loss: 0.6518\n",
            "Epoch 609/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5444 - val_loss: 0.6541\n",
            "Epoch 610/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5692 - val_loss: 0.6680\n",
            "Epoch 611/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5717 - val_loss: 0.6497\n",
            "Epoch 612/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5701 - val_loss: 0.6688\n",
            "Epoch 613/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.5844 - val_loss: 0.6503\n",
            "Epoch 614/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5770 - val_loss: 0.6615\n",
            "Epoch 615/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5719 - val_loss: 0.6703\n",
            "Epoch 616/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5595 - val_loss: 0.6699\n",
            "Epoch 617/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5519 - val_loss: 0.6637\n",
            "Epoch 618/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5720 - val_loss: 0.6526\n",
            "Epoch 619/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5687 - val_loss: 0.6804\n",
            "Epoch 620/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.5829 - val_loss: 0.6572\n",
            "Epoch 621/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.5606 - val_loss: 0.6668\n",
            "Epoch 622/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5511 - val_loss: 0.6708\n",
            "Epoch 623/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5560 - val_loss: 0.6897\n",
            "Epoch 624/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5409 - val_loss: 0.6433\n",
            "Epoch 625/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5530 - val_loss: 0.6770\n",
            "Epoch 626/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5317 - val_loss: 0.6685\n",
            "Epoch 627/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5641 - val_loss: 0.6804\n",
            "Epoch 628/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5549 - val_loss: 0.6628\n",
            "Epoch 629/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5549 - val_loss: 0.6984\n",
            "Epoch 630/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5663 - val_loss: 0.6692\n",
            "Epoch 631/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5756 - val_loss: 0.6751\n",
            "Epoch 632/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5565 - val_loss: 0.6800\n",
            "Epoch 633/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5584 - val_loss: 0.6580\n",
            "Epoch 634/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.5365 - val_loss: 0.6665\n",
            "Epoch 635/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5506 - val_loss: 0.6811\n",
            "Epoch 636/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5843 - val_loss: 0.6339\n",
            "Epoch 637/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5517 - val_loss: 0.6547\n",
            "Epoch 638/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5623 - val_loss: 0.6540\n",
            "Epoch 639/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5513 - val_loss: 0.6670\n",
            "Epoch 640/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5634 - val_loss: 0.6571\n",
            "Epoch 641/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.5468 - val_loss: 0.6529\n",
            "Epoch 642/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5645 - val_loss: 0.6784\n",
            "Epoch 643/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5606 - val_loss: 0.6720\n",
            "Epoch 644/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5525 - val_loss: 0.6616\n",
            "Epoch 645/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5684 - val_loss: 0.6630\n",
            "Epoch 646/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5326 - val_loss: 0.6614\n",
            "Epoch 647/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.5753 - val_loss: 0.6577\n",
            "Epoch 648/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.5437 - val_loss: 0.6630\n",
            "Epoch 649/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5306 - val_loss: 0.6715\n",
            "Epoch 650/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5632 - val_loss: 0.6608\n",
            "Epoch 651/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5576 - val_loss: 0.6608\n",
            "Epoch 652/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5447 - val_loss: 0.6856\n",
            "Epoch 653/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5381 - val_loss: 0.6730\n",
            "Epoch 654/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5600 - val_loss: 0.7083\n",
            "Epoch 655/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.5315 - val_loss: 0.6586\n",
            "Epoch 656/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5357 - val_loss: 0.6799\n",
            "Epoch 657/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5488 - val_loss: 0.6566\n",
            "Epoch 658/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5515 - val_loss: 0.6541\n",
            "Epoch 659/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5661 - val_loss: 0.6618\n",
            "Epoch 660/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5437 - val_loss: 0.6568\n",
            "Epoch 661/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.5633 - val_loss: 0.6475\n",
            "Epoch 662/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5499 - val_loss: 0.6719\n",
            "Epoch 663/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5555 - val_loss: 0.6612\n",
            "Epoch 664/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5420 - val_loss: 0.6732\n",
            "Epoch 665/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5412 - val_loss: 0.6703\n",
            "Epoch 666/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5672 - val_loss: 0.6384\n",
            "Epoch 667/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5293 - val_loss: 0.6738\n",
            "Epoch 668/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5230 - val_loss: 0.6654\n",
            "Epoch 669/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5423 - val_loss: 0.6545\n",
            "Epoch 670/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5574 - val_loss: 0.6423\n",
            "Epoch 671/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5521 - val_loss: 0.6521\n",
            "Epoch 672/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5609 - val_loss: 0.6598\n",
            "Epoch 673/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5308 - val_loss: 0.6500\n",
            "Epoch 674/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5501 - val_loss: 0.6564\n",
            "Epoch 675/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5381 - val_loss: 0.6742\n",
            "Epoch 676/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5625 - val_loss: 0.6662\n",
            "Epoch 677/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5410 - val_loss: 0.6754\n",
            "Epoch 678/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5527 - val_loss: 0.6579\n",
            "Epoch 679/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5423 - val_loss: 0.6666\n",
            "Epoch 680/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5458 - val_loss: 0.7222\n",
            "Epoch 681/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.5475 - val_loss: 0.6670\n",
            "Epoch 682/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5486 - val_loss: 0.6642\n",
            "Epoch 683/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5354 - val_loss: 0.6539\n",
            "Epoch 684/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5529 - val_loss: 0.6637\n",
            "Epoch 685/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5348 - val_loss: 0.6584\n",
            "Epoch 686/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5400 - val_loss: 0.6800\n",
            "Epoch 687/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5376 - val_loss: 0.6638\n",
            "Epoch 688/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.5451 - val_loss: 0.6681\n",
            "Epoch 689/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5572 - val_loss: 0.6585\n",
            "Epoch 690/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5451 - val_loss: 0.6576\n",
            "Epoch 691/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5229 - val_loss: 0.6973\n",
            "Epoch 692/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5460 - val_loss: 0.6773\n",
            "Epoch 693/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5467 - val_loss: 0.6756\n",
            "Epoch 694/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5298 - val_loss: 0.6549\n",
            "Epoch 695/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5317 - val_loss: 0.6731\n",
            "Epoch 696/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5403 - val_loss: 0.6578\n",
            "Epoch 697/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5373 - val_loss: 0.6731\n",
            "Epoch 698/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5541 - val_loss: 0.6712\n",
            "Epoch 699/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5417 - val_loss: 0.6513\n",
            "Epoch 700/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5427 - val_loss: 0.6391\n",
            "Epoch 701/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5258 - val_loss: 0.6476\n",
            "Epoch 702/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.5257 - val_loss: 0.6818\n",
            "Epoch 703/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5544 - val_loss: 0.6374\n",
            "Epoch 704/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5421 - val_loss: 0.6463\n",
            "Epoch 705/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5147 - val_loss: 0.6397\n",
            "Epoch 706/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5367 - val_loss: 0.6432\n",
            "Epoch 707/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5335 - val_loss: 0.6584\n",
            "Epoch 708/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.5213 - val_loss: 0.6904\n",
            "Epoch 709/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.5237 - val_loss: 0.6816\n",
            "Epoch 710/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5158 - val_loss: 0.6660\n",
            "Epoch 711/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5254 - val_loss: 0.6576\n",
            "Epoch 712/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5321 - val_loss: 0.6694\n",
            "Epoch 713/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5214 - val_loss: 0.6493\n",
            "Epoch 714/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5299 - val_loss: 0.6471\n",
            "Epoch 715/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.5460 - val_loss: 0.6722\n",
            "Epoch 716/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5299 - val_loss: 0.6644\n",
            "Epoch 717/1000\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.5283 - val_loss: 0.6620\n",
            "Epoch 718/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5258 - val_loss: 0.6812\n",
            "Epoch 719/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5491 - val_loss: 0.6760\n",
            "Epoch 720/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5274 - val_loss: 0.6464\n",
            "Epoch 721/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5222 - val_loss: 0.6613\n",
            "Epoch 722/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.5101 - val_loss: 0.6603\n",
            "Epoch 723/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5143 - val_loss: 0.6851\n",
            "Epoch 724/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5203 - val_loss: 0.6704\n",
            "Epoch 725/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5396 - val_loss: 0.6959\n",
            "Epoch 726/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5235 - val_loss: 0.6648\n",
            "Epoch 727/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5245 - val_loss: 0.6834\n",
            "Epoch 728/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5314 - val_loss: 0.6894\n",
            "Epoch 729/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5143 - val_loss: 0.6770\n",
            "Epoch 730/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5301 - val_loss: 0.6614\n",
            "Epoch 731/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5093 - val_loss: 0.6541\n",
            "Epoch 732/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5171 - val_loss: 0.6666\n",
            "Epoch 733/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5268 - val_loss: 0.6684\n",
            "Epoch 734/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5182 - val_loss: 0.6803\n",
            "Epoch 735/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5049 - val_loss: 0.6790\n",
            "Epoch 736/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5087 - val_loss: 0.6887\n",
            "Epoch 737/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5179 - val_loss: 0.6645\n",
            "Epoch 738/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5153 - val_loss: 0.6779\n",
            "Epoch 739/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5339 - val_loss: 0.6599\n",
            "Epoch 740/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5322 - val_loss: 0.6736\n",
            "Epoch 741/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5267 - val_loss: 0.7133\n",
            "Epoch 742/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5342 - val_loss: 0.6983\n",
            "Epoch 743/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5306 - val_loss: 0.6725\n",
            "Epoch 744/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5399 - val_loss: 0.6538\n",
            "Epoch 745/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5447 - val_loss: 0.6661\n",
            "Epoch 746/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5063 - val_loss: 0.6603\n",
            "Epoch 747/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5470 - val_loss: 0.6699\n",
            "Epoch 748/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5236 - val_loss: 0.6704\n",
            "Epoch 749/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.5133 - val_loss: 0.6896\n",
            "Epoch 750/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.5556 - val_loss: 0.6820\n",
            "Epoch 751/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5245 - val_loss: 0.6803\n",
            "Epoch 752/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5308 - val_loss: 0.6599\n",
            "Epoch 753/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5062 - val_loss: 0.6731\n",
            "Epoch 754/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5053 - val_loss: 0.6572\n",
            "Epoch 755/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5266 - val_loss: 0.6631\n",
            "Epoch 756/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.4996 - val_loss: 0.6697\n",
            "Epoch 757/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5085 - val_loss: 0.6548\n",
            "Epoch 758/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5202 - val_loss: 0.6655\n",
            "Epoch 759/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5270 - val_loss: 0.6561\n",
            "Epoch 760/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5214 - val_loss: 0.6607\n",
            "Epoch 761/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5092 - val_loss: 0.6687\n",
            "Epoch 762/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5123 - val_loss: 0.6696\n",
            "Epoch 763/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.4989 - val_loss: 0.6715\n",
            "Epoch 764/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5150 - val_loss: 0.6535\n",
            "Epoch 765/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5219 - val_loss: 0.6470\n",
            "Epoch 766/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4937 - val_loss: 0.6505\n",
            "Epoch 767/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5076 - val_loss: 0.6546\n",
            "Epoch 768/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4925 - val_loss: 0.6739\n",
            "Epoch 769/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4902 - val_loss: 0.6588\n",
            "Epoch 770/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.4966 - val_loss: 0.6669\n",
            "Epoch 771/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5130 - val_loss: 0.6613\n",
            "Epoch 772/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5287 - val_loss: 0.6730\n",
            "Epoch 773/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5117 - val_loss: 0.6512\n",
            "Epoch 774/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4976 - val_loss: 0.6685\n",
            "Epoch 775/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4982 - val_loss: 0.6563\n",
            "Epoch 776/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.5334 - val_loss: 0.6645\n",
            "Epoch 777/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.5102 - val_loss: 0.6550\n",
            "Epoch 778/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4982 - val_loss: 0.6819\n",
            "Epoch 779/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5062 - val_loss: 0.6572\n",
            "Epoch 780/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4975 - val_loss: 0.6737\n",
            "Epoch 781/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5225 - val_loss: 0.6501\n",
            "Epoch 782/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.5245 - val_loss: 0.6661\n",
            "Epoch 783/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.5154 - val_loss: 0.6595\n",
            "Epoch 784/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4965 - val_loss: 0.6594\n",
            "Epoch 785/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5099 - val_loss: 0.6580\n",
            "Epoch 786/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4973 - val_loss: 0.6447\n",
            "Epoch 787/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4896 - val_loss: 0.6726\n",
            "Epoch 788/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4904 - val_loss: 0.6604\n",
            "Epoch 789/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.5107 - val_loss: 0.6746\n",
            "Epoch 790/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5213 - val_loss: 0.6755\n",
            "Epoch 791/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5196 - val_loss: 0.6452\n",
            "Epoch 792/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5108 - val_loss: 0.6527\n",
            "Epoch 793/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5329 - val_loss: 0.6532\n",
            "Epoch 794/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5094 - val_loss: 0.6799\n",
            "Epoch 795/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.5071 - val_loss: 0.6809\n",
            "Epoch 796/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4991 - val_loss: 0.6770\n",
            "Epoch 797/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5041 - val_loss: 0.6656\n",
            "Epoch 798/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4972 - val_loss: 0.6608\n",
            "Epoch 799/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5099 - val_loss: 0.6758\n",
            "Epoch 800/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4956 - val_loss: 0.6542\n",
            "Epoch 801/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4922 - val_loss: 0.6522\n",
            "Epoch 802/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.4787 - val_loss: 0.6748\n",
            "Epoch 803/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5034 - val_loss: 0.6502\n",
            "Epoch 804/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5093 - val_loss: 0.6605\n",
            "Epoch 805/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5107 - val_loss: 0.6463\n",
            "Epoch 806/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5061 - val_loss: 0.6526\n",
            "Epoch 807/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4978 - val_loss: 0.6452\n",
            "Epoch 808/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4712 - val_loss: 0.6551\n",
            "Epoch 809/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4986 - val_loss: 0.6532\n",
            "Epoch 810/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4807 - val_loss: 0.6461\n",
            "Epoch 811/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5176 - val_loss: 0.6487\n",
            "Epoch 812/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5012 - val_loss: 0.6646\n",
            "Epoch 813/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4865 - val_loss: 0.6813\n",
            "Epoch 814/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4974 - val_loss: 0.6557\n",
            "Epoch 815/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5238 - val_loss: 0.6573\n",
            "Epoch 816/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4925 - val_loss: 0.6613\n",
            "Epoch 817/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5133 - val_loss: 0.6449\n",
            "Epoch 818/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4973 - val_loss: 0.6739\n",
            "Epoch 819/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4859 - val_loss: 0.6740\n",
            "Epoch 820/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.5107 - val_loss: 0.6728\n",
            "Epoch 821/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5020 - val_loss: 0.6706\n",
            "Epoch 822/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.5019 - val_loss: 0.6637\n",
            "Epoch 823/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5245 - val_loss: 0.6413\n",
            "Epoch 824/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.4810 - val_loss: 0.6465\n",
            "Epoch 825/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4958 - val_loss: 0.6807\n",
            "Epoch 826/1000\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.5029 - val_loss: 0.6591\n",
            "Epoch 827/1000\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.4943 - val_loss: 0.6826\n",
            "Epoch 828/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.5085 - val_loss: 0.6549\n",
            "Epoch 829/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.5119 - val_loss: 0.6313\n",
            "Epoch 830/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4914 - val_loss: 0.6482\n",
            "Epoch 831/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.5173 - val_loss: 0.6495\n",
            "Epoch 832/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4962 - val_loss: 0.6395\n",
            "Epoch 833/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4808 - val_loss: 0.6696\n",
            "Epoch 834/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4938 - val_loss: 0.6500\n",
            "Epoch 835/1000\n",
            "52/52 [==============================] - 3s 54ms/step - loss: 0.4979 - val_loss: 0.6552\n",
            "Epoch 836/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4966 - val_loss: 0.6534\n",
            "Epoch 837/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5026 - val_loss: 0.6741\n",
            "Epoch 838/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4835 - val_loss: 0.6873\n",
            "Epoch 839/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4744 - val_loss: 0.6611\n",
            "Epoch 840/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.5065 - val_loss: 0.6493\n",
            "Epoch 841/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.4833 - val_loss: 0.6575\n",
            "Epoch 842/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4954 - val_loss: 0.6605\n",
            "Epoch 843/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4692 - val_loss: 0.6606\n",
            "Epoch 844/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4842 - val_loss: 0.6519\n",
            "Epoch 845/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4761 - val_loss: 0.6695\n",
            "Epoch 846/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4843 - val_loss: 0.6530\n",
            "Epoch 847/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4975 - val_loss: 0.7043\n",
            "Epoch 848/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.4765 - val_loss: 0.6747\n",
            "Epoch 849/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.4893 - val_loss: 0.6708\n",
            "Epoch 850/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4775 - val_loss: 0.6570\n",
            "Epoch 851/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4870 - val_loss: 0.6815\n",
            "Epoch 852/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.5064 - val_loss: 0.6696\n",
            "Epoch 853/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4818 - val_loss: 0.6469\n",
            "Epoch 854/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4945 - val_loss: 0.6575\n",
            "Epoch 855/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.4761 - val_loss: 0.6511\n",
            "Epoch 856/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4852 - val_loss: 0.6585\n",
            "Epoch 857/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4757 - val_loss: 0.6562\n",
            "Epoch 858/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4857 - val_loss: 0.6474\n",
            "Epoch 859/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4739 - val_loss: 0.6541\n",
            "Epoch 860/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4838 - val_loss: 0.6526\n",
            "Epoch 861/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4767 - val_loss: 0.6514\n",
            "Epoch 862/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4750 - val_loss: 0.6639\n",
            "Epoch 863/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4929 - val_loss: 0.6430\n",
            "Epoch 864/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4837 - val_loss: 0.6806\n",
            "Epoch 865/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4911 - val_loss: 0.6488\n",
            "Epoch 866/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4755 - val_loss: 0.6713\n",
            "Epoch 867/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4832 - val_loss: 0.6702\n",
            "Epoch 868/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.4864 - val_loss: 0.6601\n",
            "Epoch 869/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4741 - val_loss: 0.6955\n",
            "Epoch 870/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4863 - val_loss: 0.6466\n",
            "Epoch 871/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4761 - val_loss: 0.7026\n",
            "Epoch 872/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4929 - val_loss: 0.6770\n",
            "Epoch 873/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4843 - val_loss: 0.6693\n",
            "Epoch 874/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.4675 - val_loss: 0.6671\n",
            "Epoch 875/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4818 - val_loss: 0.6485\n",
            "Epoch 876/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4873 - val_loss: 0.6665\n",
            "Epoch 877/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4842 - val_loss: 0.6588\n",
            "Epoch 878/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4680 - val_loss: 0.6479\n",
            "Epoch 879/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4822 - val_loss: 0.6561\n",
            "Epoch 880/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4945 - val_loss: 0.6820\n",
            "Epoch 881/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4635 - val_loss: 0.6898\n",
            "Epoch 882/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.4734 - val_loss: 0.7011\n",
            "Epoch 883/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4792 - val_loss: 0.6751\n",
            "Epoch 884/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4877 - val_loss: 0.6658\n",
            "Epoch 885/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4710 - val_loss: 0.6624\n",
            "Epoch 886/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4724 - val_loss: 0.6603\n",
            "Epoch 887/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.4937 - val_loss: 0.6703\n",
            "Epoch 888/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.5002 - val_loss: 0.6797\n",
            "Epoch 889/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4609 - val_loss: 0.6773\n",
            "Epoch 890/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4802 - val_loss: 0.6619\n",
            "Epoch 891/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4732 - val_loss: 0.6712\n",
            "Epoch 892/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4968 - val_loss: 0.6526\n",
            "Epoch 893/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.4750 - val_loss: 0.6466\n",
            "Epoch 894/1000\n",
            "52/52 [==============================] - 3s 58ms/step - loss: 0.4705 - val_loss: 0.6470\n",
            "Epoch 895/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4658 - val_loss: 0.6823\n",
            "Epoch 896/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4807 - val_loss: 0.6639\n",
            "Epoch 897/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4544 - val_loss: 0.6671\n",
            "Epoch 898/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4927 - val_loss: 0.6772\n",
            "Epoch 899/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4748 - val_loss: 0.6670\n",
            "Epoch 900/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.4982 - val_loss: 0.6490\n",
            "Epoch 901/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4831 - val_loss: 0.6462\n",
            "Epoch 902/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4708 - val_loss: 0.6659\n",
            "Epoch 903/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4592 - val_loss: 0.6702\n",
            "Epoch 904/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4905 - val_loss: 0.6564\n",
            "Epoch 905/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4610 - val_loss: 0.6639\n",
            "Epoch 906/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.4926 - val_loss: 0.6584\n",
            "Epoch 907/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.5001 - val_loss: 0.6565\n",
            "Epoch 908/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4756 - val_loss: 0.6463\n",
            "Epoch 909/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4799 - val_loss: 0.6593\n",
            "Epoch 910/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4865 - val_loss: 0.6595\n",
            "Epoch 911/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4818 - val_loss: 0.6482\n",
            "Epoch 912/1000\n",
            "52/52 [==============================] - 2s 43ms/step - loss: 0.4717 - val_loss: 0.6649\n",
            "Epoch 913/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.4859 - val_loss: 0.6823\n",
            "Epoch 914/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4840 - val_loss: 0.6660\n",
            "Epoch 915/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4689 - val_loss: 0.6612\n",
            "Epoch 916/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4537 - val_loss: 0.6652\n",
            "Epoch 917/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4688 - val_loss: 0.6507\n",
            "Epoch 918/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4901 - val_loss: 0.6676\n",
            "Epoch 919/1000\n",
            "52/52 [==============================] - 3s 57ms/step - loss: 0.4736 - val_loss: 0.6579\n",
            "Epoch 920/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.4802 - val_loss: 0.6791\n",
            "Epoch 921/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4651 - val_loss: 0.6585\n",
            "Epoch 922/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4626 - val_loss: 0.6491\n",
            "Epoch 923/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4651 - val_loss: 0.6539\n",
            "Epoch 924/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4667 - val_loss: 0.6762\n",
            "Epoch 925/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.4607 - val_loss: 0.6601\n",
            "Epoch 926/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4639 - val_loss: 0.6543\n",
            "Epoch 927/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.5000 - val_loss: 0.6485\n",
            "Epoch 928/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4803 - val_loss: 0.6542\n",
            "Epoch 929/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4501 - val_loss: 0.6470\n",
            "Epoch 930/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4665 - val_loss: 0.6555\n",
            "Epoch 931/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4774 - val_loss: 0.6612\n",
            "Epoch 932/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.4619 - val_loss: 0.6683\n",
            "Epoch 933/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4584 - val_loss: 0.6618\n",
            "Epoch 934/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4650 - val_loss: 0.6583\n",
            "Epoch 935/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4717 - val_loss: 0.6743\n",
            "Epoch 936/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4933 - val_loss: 0.6415\n",
            "Epoch 937/1000\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.4576 - val_loss: 0.6718\n",
            "Epoch 938/1000\n",
            "52/52 [==============================] - 3s 55ms/step - loss: 0.4608 - val_loss: 0.6676\n",
            "Epoch 939/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4702 - val_loss: 0.6630\n",
            "Epoch 940/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4622 - val_loss: 0.6667\n",
            "Epoch 941/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4503 - val_loss: 0.6505\n",
            "Epoch 942/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4570 - val_loss: 0.6704\n",
            "Epoch 943/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4799 - val_loss: 0.6746\n",
            "Epoch 944/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.4576 - val_loss: 0.6677\n",
            "Epoch 945/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4839 - val_loss: 0.6625\n",
            "Epoch 946/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4471 - val_loss: 0.6687\n",
            "Epoch 947/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4387 - val_loss: 0.6890\n",
            "Epoch 948/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4545 - val_loss: 0.6751\n",
            "Epoch 949/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4655 - val_loss: 0.6571\n",
            "Epoch 950/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4731 - val_loss: 0.6533\n",
            "Epoch 951/1000\n",
            "52/52 [==============================] - 3s 56ms/step - loss: 0.4738 - val_loss: 0.6602\n",
            "Epoch 952/1000\n",
            "52/52 [==============================] - 2s 42ms/step - loss: 0.4630 - val_loss: 0.6540\n",
            "Epoch 953/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4604 - val_loss: 0.6691\n",
            "Epoch 954/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4709 - val_loss: 0.6568\n",
            "Epoch 955/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4708 - val_loss: 0.6509\n",
            "Epoch 956/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4670 - val_loss: 0.6466\n",
            "Epoch 957/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4508 - val_loss: 0.6684\n",
            "Epoch 958/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.4495 - val_loss: 0.6534\n",
            "Epoch 959/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4527 - val_loss: 0.6845\n",
            "Epoch 960/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4509 - val_loss: 0.6365\n",
            "Epoch 961/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4494 - val_loss: 0.6748\n",
            "Epoch 962/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4834 - val_loss: 0.6739\n",
            "Epoch 963/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4745 - val_loss: 0.6622\n",
            "Epoch 964/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 0.4571 - val_loss: 0.6402\n",
            "Epoch 965/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.4630 - val_loss: 0.6655\n",
            "Epoch 966/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4699 - val_loss: 0.6718\n",
            "Epoch 967/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4697 - val_loss: 0.6544\n",
            "Epoch 968/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4507 - val_loss: 0.6500\n",
            "Epoch 969/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4508 - val_loss: 0.6516\n",
            "Epoch 970/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4609 - val_loss: 0.6486\n",
            "Epoch 971/1000\n",
            "52/52 [==============================] - 3s 49ms/step - loss: 0.4633 - val_loss: 0.6548\n",
            "Epoch 972/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4662 - val_loss: 0.6519\n",
            "Epoch 973/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4620 - val_loss: 0.6525\n",
            "Epoch 974/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4547 - val_loss: 0.6689\n",
            "Epoch 975/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4529 - val_loss: 0.6598\n",
            "Epoch 976/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4430 - val_loss: 0.6640\n",
            "Epoch 977/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.4439 - val_loss: 0.6661\n",
            "Epoch 978/1000\n",
            "52/52 [==============================] - 2s 44ms/step - loss: 0.4560 - val_loss: 0.6495\n",
            "Epoch 979/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4736 - val_loss: 0.6598\n",
            "Epoch 980/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4573 - val_loss: 0.6669\n",
            "Epoch 981/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4522 - val_loss: 0.6716\n",
            "Epoch 982/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4596 - val_loss: 0.6555\n",
            "Epoch 983/1000\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 0.4476 - val_loss: 0.6821\n",
            "Epoch 984/1000\n",
            "52/52 [==============================] - 3s 52ms/step - loss: 0.4619 - val_loss: 0.6666\n",
            "Epoch 985/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4634 - val_loss: 0.6571\n",
            "Epoch 986/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4424 - val_loss: 0.6792\n",
            "Epoch 987/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4425 - val_loss: 0.6608\n",
            "Epoch 988/1000\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4651 - val_loss: 0.6588\n",
            "Epoch 989/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4505 - val_loss: 0.6656\n",
            "Epoch 990/1000\n",
            "52/52 [==============================] - 2s 48ms/step - loss: 0.4605 - val_loss: 0.6542\n",
            "Epoch 991/1000\n",
            "52/52 [==============================] - 3s 53ms/step - loss: 0.4659 - val_loss: 0.6506\n",
            "Epoch 992/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4502 - val_loss: 0.6605\n",
            "Epoch 993/1000\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4576 - val_loss: 0.6477\n",
            "Epoch 994/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4452 - val_loss: 0.6457\n",
            "Epoch 995/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4566 - val_loss: 0.6656\n",
            "Epoch 996/1000\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.4441 - val_loss: 0.6742\n",
            "Epoch 997/1000\n",
            "52/52 [==============================] - 3s 51ms/step - loss: 0.4443 - val_loss: 0.6585\n",
            "Epoch 998/1000\n",
            "52/52 [==============================] - 2s 45ms/step - loss: 0.4445 - val_loss: 0.6554\n",
            "Epoch 999/1000\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.4342 - val_loss: 0.6698\n",
            "Epoch 1000/1000\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 0.4525 - val_loss: 0.6715\n"
          ]
        }
      ],
      "source": [
        "#run call the transformer model\n",
        "#df_read = df.copy()\n",
        "#df_read=(df_read-df_read.mean())/df_read.std()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train, test = train_test_split(df_read, test_size=0.2)\n",
        "\n",
        "train = pd.concat([df1,df2,df3,df4], axis=0)\n",
        "test = df5.copy()\n",
        "\n",
        "Y_train = np.array(train['class'])\n",
        "X_train= np.array(train.drop(['class'],axis=1))\n",
        "X_train=X_train.reshape(X_train.shape[0], 1 , X_train.shape[1])\n",
        "\n",
        "Y_val=np.array(test['class'])\n",
        "X_val = np.array(test.drop(['class'],axis=1))\n",
        "X_val=X_val.reshape(X_val.shape[0], 1 , X_val.shape[1])\n",
        "NUM_LAYERS =  4\n",
        "\n",
        "D_MODEL = X_train.shape[2]\n",
        "NUM_HEADS =  4\n",
        "UNITS =  2048\n",
        "DROPOUT = 0.1 #0.1\n",
        "TIME_STEPS= X_train.shape[1]\n",
        "OUTPUT_SIZE=1\n",
        "batch_size=64\n",
        "\n",
        "model1 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=3,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "model2 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=5,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "model3 = transformer(time_steps=TIME_STEPS,\n",
        "  num_layers=6,\n",
        "  units=UNITS,\n",
        "  d_model=D_MODEL,\n",
        "  num_heads=NUM_HEADS,\n",
        "  dropout=DROPOUT,\n",
        "  output_size=OUTPUT_SIZE,\n",
        "  projection='linear')\n",
        "\n",
        "#run\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='mae') #org\n",
        "model3.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss='mse')\n",
        "# model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mae')\n",
        "#\n",
        "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=80, restore_best_weights=True)\n",
        "# history = model.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "history = model3.fit(X_train,Y_train, epochs=1000, validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "UxMyaAwpcqdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ce8ad9-e92d-4e7b-e9dc-7d5ea5ec662f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00025239105684211454\n",
            "(1660, 1, 180)\n",
            "(415, 1, 180)\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "import time\n",
        "st = time.time()\n",
        "p1 = np.array(model3(X_val)).flatten()\n",
        "end = time.time()\n",
        "# print(end, st, len(p1))\n",
        "print((end-st)/len(p1))\n",
        "p2 = np.array(model3(X_train)).flatten()\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Say3gnZgi331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a0c87b-b502-4968-bfff-089bd55d7bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.89645854]\n",
            " [0.89645854 1.        ]] SignificanceResult(statistic=0.8928502932800825, pvalue=0.0) 0.711677173988813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7906476384468626, 0.7820366544950054, 0.587941779840286)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import kendalltau\n",
        "print(np.corrcoef(p2, Y_train), stats.spearmanr(p2, Y_train), kendalltau(p2,Y_train).correlation)\n",
        "np.corrcoef(p1, Y_val)[1][0], stats.spearmanr(p1, Y_val).correlation, kendalltau(p1,Y_val).correlation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ST5WiH4KoeOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916b20e6-035e-47bb-e4b7-5a828555e1ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1660, 1, 180)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ksXNLzMScqg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "5f67466b-e958-4a86-d098-c2ce1b0e7513"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#run\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"audio_dm_3_featu_.jpg\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}